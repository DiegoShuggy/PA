# response_generator.py
from typing import Dict, List, Optional
import logging
from datetime import datetime
import re
import random

# Importar el sistema de mejoras
try:
    from app.response_enhancer import enhance_response
    RESPONSE_ENHANCER_AVAILABLE = True
    logging.info("‚úÖ Mejoras de respuesta disponibles en ResponseGenerator")
except ImportError as e:
    RESPONSE_ENHANCER_AVAILABLE = False
    logging.warning(f"‚ö†Ô∏è Mejoras de respuesta no disponibles: {e}")

logger = logging.getLogger(__name__)

class ResponseGenerator:
    def __init__(self, rag_engine):
        self.rag_engine = rag_engine
        self.response_history = {}
    
    def _enhance_response_if_available(self, response_text: str, query: str, category: str = "") -> str:
        """Aplicar mejoras a la respuesta si est√° disponible el sistema de mejoras"""
        if RESPONSE_ENHANCER_AVAILABLE and response_text and len(response_text.strip()) > 0:
            try:
                enhanced = enhance_response(response_text, query, category)
                if enhanced != response_text:
                    logger.info(f"‚úÖ Respuesta mejorada: {len(response_text)} -> {len(enhanced)} chars")
                return enhanced
            except Exception as e:
                logger.warning(f"‚ùå Error aplicando mejoras: {e}")
                return response_text
        return response_text
    
    def detect_opinion_question(self, query: str) -> bool:
        """
        Detecta si la consulta solicita una opini√≥n personal.
        Returns: True si es una consulta de opini√≥n, False si no.
        """
        query_lower = query.lower().strip()
        
        # Patrones que indican solicitud de opini√≥n
        opinion_patterns = [
            # Patrones directos de opini√≥n
            r'\b(que opinas|que piensas|cu√°l es tu opini√≥n|cu√°l es tu punto de vista)\b',
            r'\b(te parece|crees que|consideras que|piensas que)\b',
            r'\b(me recomiendas|me aconsejas|que me sugieres)\b',
            r'\b(cu√°l prefieres|cu√°l te gusta m√°s|cu√°l es mejor)\b',
            r'\b(estar√≠as de acuerdo|estar√≠as a favor|estar√≠as en contra)\b',
            r'\b(ser√≠a bueno|ser√≠a malo|ser√≠a mejor)\b',
            
            # Patrones de preferencia personal
            r'\b(cu√°l es tu favorito|cu√°l te gusta|cu√°l prefieres)\b',
            r'\b(te gusta|te agrada|te disgusta|te molesta)\b',
            
            # Patrones de juicio personal
            r'\b(est√° bien|est√° mal|es correcto|es incorrecto)\b',
            r'\b(deber√≠a|no deber√≠a|tengo que|no tengo que)\b',
            
            # Patrones de evaluaci√≥n subjetiva
            r'\b(es bueno|es malo|es mejor|es peor)\b',
            r'\b(vale la pena|no vale la pena|merece la pena)\b',
            
            # Patrones de consejo personalizado
            r'\b(qu√© har√≠as t√∫|qu√© har√≠as en mi lugar|c√≥mo lo har√≠as t√∫)\b',
            r'\b(qu√© me recomiendas hacer|qu√© me sugieres que haga)\b',
            
            # Patrones de gustos y preferencias
            r'\b(te gustar√≠a|te encantar√≠a|te interesar√≠a)\b',
            r'\b(prefiero|me gusta m√°s|me agrada m√°s)\b'
        ]
        
        # Verificar patrones de opini√≥n
        for pattern in opinion_patterns:
            if re.search(pattern, query_lower):
                logger.info(f"OPINI√ìN DETECTADA: '{query}' -> Patr√≥n: {pattern}")
                return True
        
        # Palabras clave que suelen indicar solicitud de opini√≥n
        opinion_keywords = [
            'opinas', 'piensas', 'opini√≥n', 'parece', 'crees', 'consideras',
            'recomiendas', 'aconsejas', 'sugieres', 'prefieres', 'gusta',
            'favorito', 'deber√≠a', 'correcto', 'incorrecto', 'mejor', 'peor'
        ]
        
        # Contar palabras clave de opini√≥n
        opinion_word_count = sum(1 for keyword in opinion_keywords if keyword in query_lower)
        
        # Si tiene 2 o m√°s palabras clave de opini√≥n, probablemente es una consulta de opini√≥n
        if opinion_word_count >= 2:
            logger.info(f"OPINI√ìN DETECTADA por m√∫ltiples keywords: '{query}'")
            return True
        
        return False
    
    def _get_opinion_rejection_response(self) -> Dict:
        """
        Devuelve una respuesta est√°ndar para consultas de opini√≥n
        """
        rejection_responses = [
            {
                'response': "Como asistente virtual de Duoc UC, mi funci√≥n es proporcionar informaci√≥n objetiva y factual sobre los servicios y programas estudiantiles. No puedo ofrecer opiniones personales o consejos subjetivos. ¬øPuedo ayudarte con informaci√≥n espec√≠fica sobre alg√∫n servicio o programa?",
                'sources': [],
                'cache_type': 'opinion_rejection'
            },
            {
                'response': "Mi prop√≥sito es brindarte informaci√≥n precisa sobre los servicios de Duoc UC. Para consultas que requieran opiniones personales o consejos subjetivos, te recomiendo contactar directamente con las √°reas especializadas correspondientes. ¬øEn qu√© informaci√≥n objetiva puedo asistirte?",
                'sources': [],
                'cache_type': 'opinion_rejection'
            },
            {
                'response': "Entiendo que buscas una perspectiva personal, pero como sistema de informaci√≥n de Duoc UC, debo limitarme a proporcionar datos objetivos sobre nuestros servicios estudiantiles. Puedo ayudarte con informaci√≥n factual sobre programas, horarios, requisitos y procedimientos.",
                'sources': [],
                'cache_type': 'opinion_rejection'
            },
            {
                'response': "Puedo ayudarte con informaci√≥n factual sobre programas, servicios y procedimientos de Duoc UC. Para orientaci√≥n personalizada que requiera opiniones subjetivas, te sugiero consultar con los profesionales correspondientes en cada √°rea. ¬øQu√© informaci√≥n espec√≠fica necesitas?",
                'sources': [],
                'cache_type': 'opinion_rejection'
            }
        ]
        
        return random.choice(rejection_responses)
    
    def generate_response(self, query: str, session_id: str, processing_info: Dict) -> Dict:
        try:
            # 1. VERIFICAR SI ES CONSULTA DE OPINI√ìN (NUEVO)
            if self.detect_opinion_question(query):
                logger.info(f"CONSULTA DE OPINI√ìN BLOQUEADA: '{query}'")
                return self._get_opinion_rejection_response()
            
            # 2. Verificar memoria primero
            memory_response = self._check_memory(query, session_id)
            if memory_response:
                return memory_response
            
            # 3. Procesar seg√∫n estrategia
            strategy = processing_info.get('processing_strategy', 'default')
            response_data = self._process_by_strategy(query, strategy, processing_info)
            
            # 4. MEJORAR LA RESPUESTA CON INFORMACI√ìN ESPEC√çFICA
            if 'response' in response_data:
                category = processing_info.get('category', strategy)
                enhanced_response = self._enhance_response_if_available(
                    response_data['response'], 
                    query, 
                    category
                )
                response_data['response'] = enhanced_response
                logger.info(f"üîß Respuesta mejorada aplicada para categor√≠a: {category}")
            
            # 5. Guardar en memoria
            self._store_in_memory(query, response_data, session_id)
            
            return response_data
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return self._generate_error_response()
    
    def _check_memory(self, query: str, session_id: str) -> Optional[Dict]:
        similar_queries = self.rag_engine.memory_manager.find_similar_queries(query)
        if similar_queries:
            best_match = similar_queries[0]
            if best_match['similarity'] > 0.85:
                logger.info(f"Using cached response with similarity: {best_match['similarity']:.3f}")
                return {
                    'response': best_match['response'],
                    'sources': best_match.get('metadata', {}).get('sources', []),
                    'cache_type': 'memory',
                    'similarity_score': best_match['similarity'],
                    'conversation_context': self.rag_engine.memory_manager.get_conversation_context(session_id)
                }
        return None
    
    def _process_by_strategy(self, query: str, strategy: str, processing_info: Dict) -> Dict:
        if strategy == 'template':
            return self._generate_template_response(processing_info)
        elif strategy == 'greeting':
            return self._generate_greeting_response()
        elif strategy == 'emergency':
            return self._generate_emergency_response()
        elif strategy == 'derivation':
            return self._generate_derivation_response(processing_info)
        else:
            return self._generate_rag_response(query, processing_info)
    
    def _generate_template_response(self, processing_info: Dict) -> Dict:
        template_id = processing_info.get('template_id')
        # Implementar l√≥gica de templates aqu√≠
        return {
            'response': f"Respuesta de template {template_id}",
            'sources': [],
            'cache_type': 'template'
        }
    
    def _generate_greeting_response(self) -> Dict:
        return {
            'response': "¬°Hola! Soy InA, tu asistente virtual del Punto Estudiantil de Duoc UC Plaza Norte. ¬øEn qu√© puedo ayudarte hoy?",
            'sources': [],
            'cache_type': 'greeting'
        }
    
    def _generate_emergency_response(self) -> Dict:
        return {
            'response': "Detecto que podr√≠as necesitar ayuda urgente. Te recomiendo contactar inmediatamente con el equipo de Bienestar Estudiantil o la L√≠nea OPS. ¬øNecesitas que te proporcione los contactos de emergencia?",
            'sources': [],
            'cache_type': 'emergency'
        }
    
    def _generate_derivation_response(self, processing_info: Dict) -> Dict:
        derivation_suggestion = processing_info.get('derivation_suggestion', '')
        return {
            'response': f"Para ayudarte mejor con esta consulta, te sugiero: {derivation_suggestion}",
            'sources': [],
            'cache_type': 'derivation'
        }
    
    def _generate_rag_response(self, query: str, processing_info: Dict) -> Dict:
        # Realizar b√∫squeda h√≠brida
        search_results = self.rag_engine.hybrid_search(query)
        
        if not search_results:
            return self._generate_fallback_response()
        
        # Construir respuesta basada en resultados
        sources = []
        response_parts = []
        
        for result in search_results:
            response_parts.append(result['document'])
            if result['metadata']:
                sources.append({
                    'content': result['document'],
                    'metadata': result['metadata'],
                    'similarity': result['similarity']
                })
        
        final_response = ' '.join(response_parts)
        
        return {
            'response': final_response,
            'sources': sources,
            'cache_type': 'rag',
            'processing_info': processing_info
        }
    
    def _generate_fallback_response(self) -> Dict:
        return {
            'response': "Lo siento, no encontr√© informaci√≥n espec√≠fica para tu consulta. ¬øPodr√≠as reformularla o ser m√°s espec√≠fico?",
            'sources': [],
            'cache_type': 'fallback'
        }
    
    def _generate_error_response(self) -> Dict:
        return {
            'response': "Lo siento, hubo un error procesando tu consulta. Por favor, int√©ntalo de nuevo.",
            'sources': [],
            'cache_type': 'error'
        }
    
    def _store_in_memory(self, query: str, response_data: Dict, session_id: str) -> None:
        try:
            # Guardar en memoria a corto plazo
            self.rag_engine.memory_manager.add_to_memory(
                query=query,
                response=response_data['response'],
                metadata={
                    'sources': response_data.get('sources', []),
                    'cache_type': response_data.get('cache_type', 'unknown'),
                    'timestamp': datetime.now().isoformat()
                }
            )
            
            # Actualizar historial de conversaci√≥n
            self.rag_engine.memory_manager.add_to_conversation_history(
                session_id=session_id,
                query=query,
                response=response_data['response']
            )
            
        except Exception as e:
            logger.error(f"Error storing in memory: {e}")