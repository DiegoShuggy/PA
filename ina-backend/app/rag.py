# rag.py - VERSI√ìN COMPLETA ACTUALIZADA CON SISTEMA H√çBRIDO
# IMPORTS SIN chromadb (para evitar activar telemetr√≠a)
import ollama
from typing import List, Dict, Optional
import logging
import json
from app.qr_generator import qr_generator
import traceback
import hashlib
from datetime import datetime, timedelta
from collections import defaultdict
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# IMPORTACIONES EXISTENTES
from app.cache_manager import rag_cache, response_cache, normalize_question
from app.topic_classifier import TopicClassifier
from app.classifier import classifier  # IMPORTAR CLASIFICADOR

# NUEVO: Importar sistema h√≠brido
# ‚ùå ELIMINADO EN LIMPIEZA - hybrid_response_system.py no se usaba
# try:
#     from app.hybrid_response_system import HybridResponseSystem
#     HYBRID_SYSTEM_AVAILABLE = True
#     logging.info("‚úÖ Sistema h√≠brido cargado correctamente")
# except ImportError as e:
HYBRID_SYSTEM_AVAILABLE = False
#     logging.warning(f"‚ö†Ô∏è Sistema h√≠brido no disponible: {e}")
# except Exception as e:
#     HYBRID_SYSTEM_AVAILABLE = False
#     logging.error(f"‚ùå Error cargando sistema h√≠brido: {e}")

# NUEVO: Importar sistema de mejora de respuestas
try:
    from app.response_enhancer import enhance_response
    RESPONSE_ENHANCER_AVAILABLE = True
    logging.info("‚úÖ Mejoras de respuesta cargadas correctamente")
except ImportError as e:
    RESPONSE_ENHANCER_AVAILABLE = False
    logging.warning(f"‚ö†Ô∏è Mejoras de respuesta no disponibles: {e}")
except Exception as e:
    RESPONSE_ENHANCER_AVAILABLE = False
    logging.error(f"‚ùå Error cargando mejoras de respuesta: {e}")

# NUEVO: Importar optimizador inteligente de respuestas
try:
    from app.intelligent_response_optimizer import intelligent_optimizer, optimize_rag_response
    INTELLIGENT_OPTIMIZER_AVAILABLE = True
    logging.info("‚úÖ Optimizador inteligente cargado correctamente")
except ImportError as e:
    INTELLIGENT_OPTIMIZER_AVAILABLE = False
    logging.warning(f"‚ö†Ô∏è Optimizador inteligente no disponible: {e}")
except Exception as e:
    INTELLIGENT_OPTIMIZER_AVAILABLE = False
    logging.error(f"‚ùå Error cargando optimizador inteligente: {e}")

logger = logging.getLogger(__name__)

# FUNCI√ìN AUXILIAR PARA MEJORAR RESPUESTAS
def enhance_final_response(response_text: str, query: str, category: str = "") -> str:
    """Aplicar mejoras CONSERVADORAS a la respuesta - NO eliminar contenido √∫til"""
    if not response_text or len(response_text.strip()) < 20:
        logger.warning(f"‚ö†Ô∏è Respuesta muy corta, no se mejorar√°: {len(response_text)} chars")
        return response_text
    
    if RESPONSE_ENHANCER_AVAILABLE:
        try:
            # Solo mejorar si la respuesta ya tiene contenido sustancial
            if len(response_text) >= 50:
                enhanced = enhance_response(response_text, query, category)
                # Verificar que la mejora no elimin√≥ contenido importante
                if len(enhanced) >= len(response_text) * 0.7:  # Al menos 70% del original
                    logger.info(f"‚úÖ Respuesta mejorada: {len(response_text)} ‚Üí {len(enhanced)} chars")
                    return enhanced
                else:
                    logger.warning(f"‚ö†Ô∏è Mejora rechazada (perdi√≥ contenido): {len(enhanced)} < {len(response_text)}")
                    return response_text
            else:
                logger.debug(f"Respuesta corta, no se mejora: {len(response_text)} chars")
                return response_text
        except Exception as e:
            logger.warning(f"‚ùå Error mejorando respuesta: {e}")
            return response_text
    else:
        return response_text


class SemanticCache:
    def __init__(self, similarity_threshold: float = 0.65):
        try:
            self.model = SentenceTransformer(
                'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            self.cache = {}
            self.threshold = similarity_threshold
            logger.info(f"Cache sem√°ntico inicializado (umbral: {similarity_threshold})")
        except Exception as e:
            logger.error(f"Error inicializando cache sem√°ntico: {e}")
            self.model = None
            self.cache = {}

    def get_embedding(self, text: str) -> Optional[np.ndarray]:
        if self.model is None:
            return None
        try:
            return self.model.encode([text])[0]
        except Exception as e:
            logger.error(f"Error generando embedding: {e}")
            return None

    def _embedding_to_key(self, embedding: np.ndarray) -> tuple:
        return tuple(embedding.tolist())

    def find_similar(self, query_embedding: np.ndarray) -> Optional[Dict]:
        if not self.cache or query_embedding is None:
            return None

        best_similarity = 0
        best_response = None

        for cached_embedding_key, response_data in self.cache.items():
            try:
                cached_embedding = np.array(cached_embedding_key)
                similarity = cosine_similarity(
                    [query_embedding], [cached_embedding])[0][0]

                if similarity > self.threshold and similarity > best_similarity:
                    best_similarity = similarity
                    best_response = response_data

            except Exception as e:
                logger.error(f"Error calculando similitud: {e}")
                continue

        if best_response:
            logger.info(f"Semantic similarity found: {best_similarity:.3f}")
            best_response['semantic_similarity'] = best_similarity
            return best_response

        return None

    def add_to_cache(self, query: str, response_data: Dict):
        embedding = self.get_embedding(query)
        if embedding is not None:
            embedding_key = self._embedding_to_key(embedding)
            self.cache[embedding_key] = response_data
            logger.info(f"Added to semantic cache: '{query[:50]}...'")


class EnhancedTopicClassifier:
    """CLASIFICADOR MEJORADO CON DETECCI√ìN INTELIGENTE"""
    
    def __init__(self):
        self.topic_classifier = TopicClassifier()
        
        # PALABRAS CLAVE CR√çTICAS PARA DETECCI√ìN MEJORADA
        self.critical_keywords = {
            'tne': ['tne', 'tarjeta nacional estudiantil', 'pase escolar', 'validar tne', 'renovar tne'],
            'deporte': ['deporte', 'taller deportivo', 'gimnasio', 'entrenamiento', 'f√∫tbol', 'basquetbol'],
            'certificado': ['certificado', 'alumno regular', 'constancia', 'record acad√©mico'],
            'bienestar': ['psicol√≥gico', 'salud mental', 'bienestar', 'crisis', 'urgencia'],
            'practicas': ['pr√°ctica', 'empleo', 'curriculum', 'entrevista', 'duoclaboral'],
            'contrase√±a': ['contrase√±a', 'password', 'mi duoc', 'plataforma', 'correo institucional']
        }

    def classify_topic(self, query: str) -> Dict:
        """CLASIFICACI√ìN MEJORADA"""
        return self.topic_classifier.classify_topic(query)

    def should_derive(self, query: str) -> bool:
        """DETECCI√ìN MEJORADA DE CONSULTAS PARA DERIVAR"""
        topic_info = self.classify_topic(query)
        
        # Consultas que SIEMPRE deben derivarse
        derivation_keywords = [
            # ESPA√ëOL
            'contrase√±a', 'password', 'mi duoc', 'plataforma', 'correo institucional',
            'wifi', 'acceso denegado', 'bloqueado', 'login', 'portal', 'olvid√© contrase√±a',
            'recuperar contrase√±a', 'no puedo entrar', 'error acceso',
            # INGL√âS 
            'password', 'my duoc', 'platform', 'institutional email',
            'wifi', 'access denied', 'blocked', 'login', 'portal', 'forgot password',
            'recover password', 'cannot enter', 'access error',
            # FRANC√âS
            'mot de passe', 'mon duoc', 'plateforme', 'email institutionnel',
            'wifi', 'acc√®s refus√©', 'bloqu√©', 'connexion', 'portail', 'oubli√© mot de passe',
            'r√©cup√©rer mot de passe', 'ne peux pas entrer', 'erreur acc√®s',
            'courrier √©lectronique institutionnel', 'e-mail institutionnel'
        ]
        
        query_lower = query.lower()
        if any(keyword in query_lower for keyword in derivation_keywords):
            return True
        
        return not topic_info.get('is_institutional', True)

    def detect_multiple_queries(self, query: str) -> List[str]:
        """DETECCI√ìN INTELIGENTE MEJORADA DE CONSULTAS M√öLTIPLES"""
        query_lower = query.lower().strip()
        
        # EVITAR DIVIDIR CONSULTAS DE DERIVACI√ìN
        if self.should_derive(query):
            return [query]
        
        # EVITAR DIVIDIR CONSULTAS FRANCESAS V√ÅLIDAS
        french_indicators = [
            "j'ai essay√©", "mais je ne", "que dois-je faire", "comment savoir",
            "ai-je une", "existe-t-il", "puis-je", "ne trouve pas",
            "cours d'ambassadeurs", "responsabilit√© suppl√©mentaire"
        ]
        
        for indicator in french_indicators:
            if indicator in query_lower:
                return [query]  # No dividir consultas francesas
        
        # PATRONES M√ÅS RESTRICTIVOS PARA DIVISI√ìN
        split_patterns = [
            r'\s+y\s+adem√°s\s+',     # " y adem√°s "
            r'\s+tambi√©n\s+quiero\s+', # " tambi√©n quiero "
            r'\s+por otro lado\s+',  # " por otro lado "
            r'\s+asimismo\s+',       # " asimismo "
            r';\s*',                 # Puntos y coma
        ]
        
        # Intentar dividir por patrones M√ÅS RESTRICTIVOS
        for pattern in split_patterns:
            parts = re.split(pattern, query_lower)
            if len(parts) > 1:
                # VERIFICAR QUE LAS PARTES TIENEN SENTIDO
                valid_parts = []
                for part in parts:
                    part_clean = part.strip()
                    # CRITERIOS M√ÅS ESTRICTOS
                    words = part_clean.split()
                    if len(words) >= 4:  # M√≠nimo 4 palabras
                        valid_parts.append(part_clean)
                
                if len(valid_parts) > 1:
                    logger.info(f"Consulta m√∫ltiple detectada: {valid_parts}")
                    return valid_parts
        
        return [query]
    
    def get_derivation_suggestion(self, topic_type: str) -> str:
        """SUGERENCIAS ESPEC√çFICAS PARA DERIVACI√ìN"""
        return self.topic_classifier.get_redirection_message(topic_type)


class RAGEngine:
    def extract_keywords(self, text: str) -> list:
        """Extrae palabras clave simples del texto (puedes mejorar con NLP si lo deseas)"""
        # Simple: palabras √∫nicas con longitud > 4
        words = re.findall(r'\b\w{5,}\b', text.lower())
        return list(set(words))

    def __init__(self):
        from app.memory_manager import MemoryManager
        from app.derivation_manager import derivation_manager
        # from app.stationary_ai_filter import stationary_filter  # ‚ùå ELIMINADO EN LIMPIEZA
        # Inicializar el gestor de memoria
        self.memory_manager = MemoryManager()
        # Inicializar el gestor de derivaci√≥n estacionaria
        self.derivation_manager = derivation_manager
        # Inicializar filtro espec√≠fico para IA estacionaria
        self.stationary_filter = None  # stationary_filter  # ‚ùå M√≥dulo eliminado
        # Expansiones de sin√≥nimos mejoradas
        self.synonym_expansions = {
            "tne": ["tarjeta nacional estudiantil", "pase escolar", "tne duoc", "beneficio tne", "tarjeta estudiante", "validaci√≥n tne", "activaci√≥n tne"],
            "deporte": ["deportes", "actividad f√≠sica", "taller deportivo", "entrenamiento", "gimnasio", "maiclub", "entretiempo", "acquatiempo", "deporte duoc", "selecci√≥n deportiva"],
            "certificado": ["certificados", "alumno regular", "constancia", "record acad√©mico", "concentraci√≥n de notas", "documentos acad√©micos", "solicitud certificado"],
            "bienestar": ["salud mental", "psicol√≥gico", "apoyo emocional", "consejer√≠a", "urgencia", "crisis", "l√≠nea ops"],
            "pr√°ctica": ["pr√°cticas profesionales", "empleo", "duoclaboral", "bolsa de trabajo", "curriculum", "cv", "entrevista"],
            "matr√≠cula": ["matricular", "arancel", "pago", "postulaci√≥n", "admisi√≥n"],
            "beneficio": ["beca", "ayuda econ√≥mica", "programa emergencia", "subsidio"],
            "embajadores": ["curso embajadores", "embajadores salud mental", "m√≥dulo embajadores", "85% embajadores"]
        }
        # IMPORTAR chromadb AL FINAL, DESPU√âS DE QUE chroma_config.py LO HAYA DESACTIVADO
        # INICIALIZACI√ìN SEGURA DE CHROMADB CON AUTO-REPARACI√ìN
        try:
            from app.chromadb_autofix import safe_chromadb_init
            self.client = safe_chromadb_init()
            if self.client is None:
                raise Exception("No se pudo inicializar ChromaDB")
            logger.info("‚úÖ ChromaDB inicializado de forma segura")
        except Exception as e:
            logger.error(f"‚ùå Error con ChromaDB seguro, usando fallback b√°sico: {e}")
            # Fallback: usar cliente en memoria
            import chromadb
            self.client = chromadb.Client()
            logger.warning("‚ö†Ô∏è Usando ChromaDB en memoria como fallback")
        try:
            self.collection = self.client.get_or_create_collection(
                name="duoc_knowledge"
            )
            # Verificar que la colecci√≥n se cre√≥ correctamente
            if not hasattr(self.collection, 'count'):
                raise Exception("Colecci√≥n inv√°lida - no tiene m√©todo count()")
            logger.info(f"‚úÖ Colecci√≥n 'duoc_knowledge' inicializada correctamente")
        except Exception as e:
            logger.error(f"‚ùå Error creando colecci√≥n: {e}")
            # Reintentar con cliente nuevo en memoria
            import chromadb
            self.client = chromadb.Client()
            self.collection = self.client.get_or_create_collection(name="duoc_knowledge")
            logger.warning("‚ö†Ô∏è Usando colecci√≥n en memoria como √∫ltimo recurso")
        # CLASIFICADOR DE TEMAS MEJORADO
        self.topic_classifier = EnhancedTopicClassifier()
        # CONFIGURACI√ìN ESPEC√çFICA DUOC UC
        self.duoc_context = {
            "sede": "Plaza Norte",
            "direccion": "Santa Elena de Huechuraba 1660, Huechuraba",
            "horario_punto_estudiantil": "Lunes a Viernes 8:30-19:00",
            "telefono": "+56 2 2360 6400",
            "email": "Puntoestudiantil_pnorte@duoc.cl"
        }
        # CACHE SEM√ÅNTICO MEJORADO
        self.semantic_cache = SemanticCache(similarity_threshold=0.65)
        self.text_cache = {}
        # CONFIGURACI√ìN DE MODELOS OLLAMA OPTIMIZADA
        # llama3.2:1b-instruct-q4_K_M es m√°s liviano (807MB) y optimizado para instrucciones
        # mistral:7b requiere 4.5GB y causa errores de memoria
        self.ollama_models = ['llama3.2:1b-instruct-q4_K_M', 'llama3.2:3b', 'gemma3:4b']  
        self.current_model = self._select_best_model()
        logger.info("RAG Engine DUOC UC inicializado")
        logger.info(f"ü§ñ Modelo Ollama: {self.current_model}")
        self.metrics = {
            'total_queries': 0,
            'successful_responses': 0,
            'cache_hits': 0,
            'semantic_cache_hits': 0,
            'text_cache_hits': 0,
            'documents_added': 0,
            'errors': 0,
            'categories_used': defaultdict(int),
            'response_times': [],
            'derivations': 0,
            'multiple_queries': 0,
            'ambiguous_queries': 0,
            'greetings': 0,
            'emergencies': 0,
            'template_responses': 0  # M√âTRICA PARA TEMPLATES
        }
        from app.memory_manager import MemoryManager
        from app.derivation_manager import derivation_manager
        # from app.stationary_ai_filter import stationary_filter  # ‚ùå ELIMINADO EN LIMPIEZA
        
        # Inicializar el gestor de memoria
        self.memory_manager = MemoryManager()
        
        # Inicializar el gestor de derivaci√≥n estacionaria
        self.derivation_manager = derivation_manager
        
        # Inicializar filtro espec√≠fico para IA estacionaria
        self.stationary_filter = None  # stationary_filter  # ‚ùå M√≥dulo eliminado
        
        # Expansiones de sin√≥nimos mejoradas
        self.synonym_expansions = {
            "tne": ["tarjeta nacional estudiantil", "pase escolar", "tne duoc", "beneficio tne", "tarjeta estudiante", "validaci√≥n tne", "activaci√≥n tne"],
            "deporte": ["deportes", "actividad f√≠sica", "taller deportivo", "entrenamiento", "gimnasio", "maiclub", "entretiempo", "acquatiempo", "deporte duoc", "selecci√≥n deportiva"],
            "certificado": ["certificados", "alumno regular", "constancia", "record acad√©mico", "concentraci√≥n de notas", "documentos acad√©micos", "solicitud certificado"],
            "bienestar": ["salud mental", "psicol√≥gico", "apoyo emocional", "consejer√≠a", "urgencia", "crisis", "l√≠nea ops"],
            "pr√°ctica": ["pr√°cticas profesionales", "empleo", "duoclaboral", "bolsa de trabajo", "curriculum", "cv", "entrevista"],
            "matr√≠cula": ["matricular", "arancel", "pago", "postulaci√≥n", "admisi√≥n"],
            "beneficio": ["beca", "ayuda econ√≥mica", "programa emergencia", "subsidio"],
            "embajadores": ["curso embajadores", "embajadores salud mental", "m√≥dulo embajadores", "85% embajadores"]
        }

        # IMPORTAR chromadb AL FINAL, DESPU√âS DE QUE chroma_config.py LO HAYA DESACTIVADO
        # INICIALIZACI√ìN SEGURA DE CHROMADB CON AUTO-REPARACI√ìN
        try:
            from app.chromadb_autofix import safe_chromadb_init
            self.client = safe_chromadb_init()
            
            if self.client is None:
                raise Exception("No se pudo inicializar ChromaDB")
            
            logger.info("‚úÖ ChromaDB inicializado de forma segura")
        except Exception as e:
            logger.error(f"‚ùå Error con ChromaDB seguro, usando fallback b√°sico: {e}")
            # Fallback: usar cliente en memoria
            import chromadb
            self.client = chromadb.Client()
            logger.warning("‚ö†Ô∏è Usando ChromaDB en memoria como fallback")

        try:
            self.collection = self.client.get_or_create_collection(
                name="duoc_knowledge"
            )
            # Verificar que la colecci√≥n se cre√≥ correctamente
            if not hasattr(self.collection, 'count'):
                raise Exception("Colecci√≥n inv√°lida - no tiene m√©todo count()")
            logger.info(f"‚úÖ Colecci√≥n 'duoc_knowledge' inicializada correctamente")
        except Exception as e:
            logger.error(f"‚ùå Error creando colecci√≥n: {e}")
            # Reintentar con cliente nuevo en memoria
            import chromadb
            self.client = chromadb.Client()
            self.collection = self.client.get_or_create_collection(name="duoc_knowledge")
            logger.warning("‚ö†Ô∏è Usando colecci√≥n en memoria como √∫ltimo recurso")

        # CLASIFICADOR DE TEMAS MEJORADO
        self.topic_classifier = EnhancedTopicClassifier()

        # CONFIGURACI√ìN ESPEC√çFICA DUOC UC
        self.duoc_context = {
            "sede": "Plaza Norte",
            "direccion": "Santa Elena de Huechuraba 1660, Huechuraba",
            "horario_punto_estudiantil": "Lunes a Viernes 8:30-19:00",
            "telefono": "+56 2 2360 6400",
            "email": "Puntoestudiantil_pnorte@duoc.cl"
        }

        # CACHE SEM√ÅNTICO MEJORADO
        self.semantic_cache = SemanticCache(similarity_threshold=0.65)
        self.text_cache = {}
        
        # CONFIGURACI√ìN DE MODELOS OLLAMA OPTIMIZADA
        # llama3.2:1b-instruct-q4_K_M es m√°s liviano (807MB) y optimizado para instrucciones
        # mistral:7b requiere 4.5GB y causa errores de memoria
        self.ollama_models = ['llama3.2:1b-instruct-q4_K_M', 'llama3.2:3b', 'gemma3:4b']  
        self.current_model = self._select_best_model()

        logger.info("RAG Engine DUOC UC inicializado")
        logger.info(f"ü§ñ Modelo Ollama: {self.current_model}")
        self.metrics = {
            'total_queries': 0,
            'successful_responses': 0,
            'cache_hits': 0,
            'semantic_cache_hits': 0,
            'text_cache_hits': 0,
            'documents_added': 0,
            'errors': 0,
            'categories_used': defaultdict(int),
            'response_times': [],
            'derivations': 0,
            'multiple_queries': 0,
            'ambiguous_queries': 0,
            'greetings': 0,
            'emergencies': 0,
            'template_responses': 0  # M√âTRICA PARA TEMPLATES
        }
        
    def _select_best_model(self) -> str:
        """Selecciona el mejor modelo Ollama disponible"""
        try:
            import subprocess
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=5)
            available_models = result.stdout.lower()
            
            # Debug: mostrar modelos disponibles
            logger.info(f"üîç Modelos Ollama disponibles:\n{available_models}")
            
            for model in self.ollama_models:
                model_lower = model.lower()
                if model_lower in available_models:
                    logger.info(f"‚úÖ Modelo seleccionado: {model}")
                    return model
                else:
                    logger.info(f"‚ùå Modelo no encontrado: {model}")
            
            # Fallback: verificar si hay alg√∫n modelo disponible
            logger.warning("‚ö†Ô∏è No se encontraron modelos preferidos, buscando cualquier modelo disponible")
            
            # Extraer nombres de modelos de la salida de ollama list
            lines = result.stdout.strip().split('\n')[1:]  # Skip header
            if lines:
                first_available = lines[0].split()[0]  # Primer columna (NAME)
                logger.warning(f"üîÑ Usando primer modelo disponible: {first_available}")
                return first_available
            
            # √öltimo fallback
            logger.error("‚ùå No se encontraron modelos Ollama disponibles")
            return 'llama3.2:1b-instruct-q4_K_M'  # Default to our preferred lightweight model
        except Exception as e:
            logger.error(f"Error detectando modelos Ollama: {e}")
            return 'llama3.2:1b-instruct-q4_K_M'  # Default to our preferred lightweight model
    
    def _build_strict_prompt(self, sources: List[Dict], query: str) -> str:
        """Construye prompt estricto: HORARIOS ESPEC√çFICOS, SIN UBICACIONES"""
        if not sources:
            return f"Di brevemente que no tienes informaci√≥n sobre '{query}' y que pueden consultar en el Punto Estudiantil (est√°s al lado). Horario: lunes-viernes 08:30-22:30, s√°bados 08:30-14:00. Contacto: +56 2 2999 3075. NO agregues disculpas."
        
        # Construir contexto conciso
        context_parts = []
        for i, source in enumerate(sources[:3], 1):  # M√°ximo 3 fuentes
            content = source['document'][:300]  # 300 chars max por fuente
            category = source.get('metadata', {}).get('category', 'info')
            context_parts.append(f"[{i}] {content}")
        
        context = "\n".join(context_parts)
        
        # Prompt optimizado: √âNFASIS EN HORARIOS, SIN UBICACIONES
        prompt = f"""Eres InA, asistente al lado del Punto Estudiantil Plaza Norte. Responde en m√°ximo 150 palabras.

DATOS DISPONIBLES:
{context}

REGLAS ESTRICTAS:
1. Responde en 2-3 oraciones SIN emojis, negritas ni formato Markdown
2. Usa SOLO los datos de arriba - no inventes
3. PRIORIDAD M√ÅXIMA: Si pide horario, da d√≠as y horas EXACTOS del servicio espec√≠fico
4. NO indiques ubicaciones f√≠sicas (la IA est√° al lado del Punto Estudiantil)
5. Si pide requisitos/proceso: lista directo sin decorar
6. NUNCA menciones otras universidades que no sean Duoc UC
7. NO uses frases gen√©ricas como "¬°Hola!" o "Con gusto"
8. NO uses secciones formateadas como "üìç Ubicaci√≥n:" o "‚è∞ Horario:"
9. Escribe texto corrido natural

INFORMACI√ìN ESPEC√çFICA POR SERVICIO:
- Punto Estudiantil: Piso 2, lunes-viernes 08:30-22:30, s√°bados 08:30-14:00
- Biblioteca: Lunes-viernes 08:00-21:00, s√°bados 09:00-14:00
- Bienestar: Lunes-viernes 09:00-18:00
- Gimnasio: Lunes-viernes 07:00-22:00, s√°bados 09:00-14:00
- Contacto: Mesa Central +56 2 2999 3000, Punto Estudiantil +56 2 2999 3075

IMPORTANTE: NO indiques direcciones de calle (ej: Calle Nueva 1660), solo "Piso 2" si preguntan por ubicaci√≥n.

PREGUNTA: {query}

RESPUESTA (texto corrido, horarios exactos, sin direcciones de calle):"""
        
        return prompt

        # Si pregunta por beneficios, agregar instrucciones espec√≠ficas
        if is_beneficios:
            return base_prompt + """

üí° ESPECIAL: Lista solo los beneficios MENCIONADOS en el contexto.
Formato: vi√±etas cortas. NO inventes becas internacionales u otros no listados.

‚úçÔ∏è RESPUESTA:"""
        else:
            return base_prompt + """

‚úçÔ∏è RESPUESTA:"""
    
    def _expand_query(self, query: str) -> str:
        """Expande consulta con sin√≥nimos clave para mejorar recall - MEJORADO CON PRIORITY KEYWORDS"""
        from app.priority_keyword_system import priority_keyword_system
        
        query_lower = query.lower().strip()
        
        # üî• PASO 1: Verificar si hay keyword prioritaria que evite expansi√≥n gen√©rica
        priority_detection = priority_keyword_system.detect_absolute_keyword(query)
        
        if priority_detection:
            logger.info(f"üéØ Priority keyword detected: '{priority_detection['keyword']}' (priority: {priority_detection['priority']})")
            
            # Si la keyword NO debe ser expandida, retornar query original
            if priority_detection['avoid_expansion']:
                logger.info(f"üö´ Evitando expansi√≥n gen√©rica para: '{priority_detection['keyword']}'")
                
                # Solo agregar expansiones ESPEC√çFICAS para esta keyword
                specific_terms = priority_detection['specific_expansion']
                if specific_terms:
                    expanded_query = query + " " + " ".join(specific_terms)
                    logger.info(f"‚úÖ Expansi√≥n espec√≠fica: '{query}' ‚Üí +{len(specific_terms)} t√©rminos espec√≠ficos")
                    return expanded_query
                else:
                    logger.info(f"‚úÖ Query sin expansi√≥n (keyword absoluta): '{query}'")
                    return query
            
            # Si permite expansi√≥n, usar solo t√©rminos espec√≠ficos
            expanded_terms = list(set(priority_detection['specific_expansion']))
            if expanded_terms:
                expanded_query = query + " " + " ".join(expanded_terms)
                logger.info(f"‚úÖ Expansi√≥n espec√≠fica permitida: '{query}' ‚Üí +{len(expanded_terms)} t√©rminos")
                return expanded_query
        
        # üî• PASO 2: Expansi√≥n gen√©rica solo si NO hay keyword prioritaria
        expanded_terms = []
        is_short_query = len(query_lower.split()) <= 2
        
        for base, synonyms in self.synonym_expansions.items():
            if base in query_lower:
                if is_short_query:
                    # Para queries cortas, usar todos los sin√≥nimos
                    expanded_terms.extend(synonyms)
                else:
                    # Para queries largas, solo los primeros 2 sin√≥nimos
                    expanded_terms.extend(synonyms[:2])
            
        if expanded_terms:
            # Eliminar duplicados
            expanded_terms = list(set(expanded_terms))
            expanded_query = query + " " + " ".join(expanded_terms)
            logger.info(f"üîç Query Expansion gen√©rica: '{query}' ‚Üí +{len(expanded_terms)} t√©rminos")
            return expanded_query
        
        logger.debug(f"Query sin expansi√≥n: '{query}'")
        return query

    def enhanced_normalize_text(self, text: str) -> str:
        
        """NORMALIZACI√ìN SUPER MEJORADA PARA DUOC UC"""
        text = text.lower().strip()
        
        # EXPANDIR SIN√ìNIMOS Y VARIANTES ESPEC√çFICAS DUOC - MEJORADO
        synonym_expansions = {
            'tne': ['tarjeta nacional estudiantil', 'pase escolar', 'tne duoc', 'beneficio tne', 'credencial estudiantil', 'transporte p√∫blico'],
            'tarjeta nacional estudiantil': ['tne', 'pase escolar', 'credencial estudiante', 'tarjeta transporte'],
            'tarjeta nacional': ['tne', 'tarjeta estudiantil', 'pase escolar'],
            'tarjeta estudiantil': ['tne', 'tarjeta nacional', 'pase escolar', 'credencial'],
            
            # DEPORTES Y ACTIVIDADES
            'deporte': ['deportes', 'actividad f√≠sica', 'entrenamiento', 'ejercicio', 'taller deportivo', 'recreaci√≥n'],
            'deportes': ['deporte', 'actividades f√≠sicas', 'entrenamiento', 'recreaci√≥n', 'talleres deportivos'],
            'taller': ['talleres', 'clase', 'actividad deportiva', 'entrenamiento grupal', 'curso'],
            'gimnasio': ['gimnasio duoc', 'complejo deportivo', 'instalaciones deportivas', 'maiclub', 'caf', 'fitness'],
            'natacion': ['nataci√≥n', 'piscina', 'acquatiempo', 'nadar', 'clases acu√°ticas'],
            
            # ACAD√âMICO
            'certificado': ['certificados', 'constancia', 'documento oficial', 'record acad√©mico', 'papeles'],
            'notas': ['calificaciones', 'promedio', 'evaluaciones', 'record acad√©mico', 'concentraci√≥n notas'],
            'carrera': ['programa', 'especialidad', 'ingenier√≠a', 't√©cnico', 'profesional'],
            
            # BIENESTAR
            'psicol√≥gico': ['psic√≥logo', 'salud mental', 'bienestar', 'apoyo emocional', 'consejer√≠a', 'psicolog√≠a'],
            'bienestar': ['bienestar estudiantil', 'apoyo', 'salud mental', 'psicol√≥gico', 'asistencia'],
            'salud': ['bienestar', 'm√©dico', 'atenci√≥n m√©dica', 'salud mental', 'enfermer√≠a'],
            
            # FINANCIERO
            'beca': ['becas', 'ayuda econ√≥mica', 'beneficio estudiantil', 'subsidio', 'financiamiento'],
            'beneficio': ['beneficios', 'becas', 'ayuda econ√≥mica', 'subsidio estudiantil', 'apoyo financiero'],
            'financiamiento': ['finanzas', 'financiero', 'econ√≥mico', 'beca', 'ayuda', 'cr√©dito'],
            'pago': ['pagos', 'arancel', 'cuota', 'financiero', 'deuda', 'cancelar'],
            'arancel': ['aranceles', 'pago', 'cuota', 'matr√≠cula', 'mensualidad'],
            
            # SERVICIOS
            'biblioteca': ['libros', 'pr√©stamo', 'estudio', 'recursos acad√©micos', 'salas estudio'],
            'contacto': ['tel√©fono', 'correo', 'email', 'comunicaci√≥n', 'informaci√≥n'],
            'horario': ['horarios', 'atenci√≥n', 'funcionamiento', 'disponibilidad'],
            'ubicaci√≥n': ['direcci√≥n', 'lugar', 'd√≥nde', 'encuentro', 'localizaci√≥n'],
            'estacionamiento': ['parking', 'estacionar', 'veh√≠culo', 'auto', 'aparcamiento'],
            
            # DESARROLLO LABORAL
            'trabajo': ['empleo', 'laboral', 'pr√°ctica', 'duoclaboral', 'profesional'],
            'practica': ['pr√°ctica profesional', 'pasant√≠a', 'trabajo', 'empresa', 'experiencia'],
            'curriculum': ['cv', 'hoja vida', 'curr√≠culum vitae', 'perfil profesional'],
            'entrevista': ['entrevistas', 'entrevista laboral', 'trabajo', 'empleo'],
            
            # SERVICIOS DIGITALES
            'digital': ['digitales', 'online', 'virtual', 'internet', 'plataforma'],
            'servicios': ['servicio', 'atenci√≥n', 'apoyo', 'asistencia'],
            'plataforma': ['portal', 'sistema', 'acceso', 'digital', 'online'],
            'correo': ['email', 'mail', 'electr√≥nico', 'comunicaci√≥n'],
            'contrase√±a': ['password', 'clave', 'acceso', 'login'],
            'pr√°ctica': ['practica profesional', 'empleo', 'trabajo', 'duoclaboral', 'bolsa trabajo'],
            'contrase√±a': ['password', 'acceso', 'login', 'plataforma', 'mi duoc'],
        }
        
        # Aplicar expansiones
        expanded_terms = []
        for base, variants in synonym_expansions.items():
            if base in text:
                expanded_terms.extend(variants)
        
        if expanded_terms:
            text += " " + " ".join(expanded_terms)
    
        # PATRONES ESPEC√çFICOS DUOC
        duoc_patterns = {
            r'plaza norte': 'sede plaza norte ubicaci√≥n',
            r'mi duoc': 'plataforma mi duoc portal duoc acceso digital',
            r'punto estudiantil': 'punto estudiantil duoc uc atenci√≥n estudiante',
            r'claudia cort√©s': 'desarrollo laboral claudia cortes empleabilidad',
            r'elizabeth dom√≠nguez': 'inclusi√≥n paedis elizabeth dominguez discapacidad',
            r'adriana v√°squez': 'bienestar estudiantil adriana vasquez salud mental',
            r'complejo maiclub': 'complejo deportivo maiclub gimnasio instalaciones',
            r'gimnasio entretiempo': 'gimnasio entretiempo centro acondicionamiento f√≠sico',
            # Modismos y variaciones coloquiales chilenas
            r'd[o√≥]nde\s+(est[a√°]|queda|se\s+encuentra|anda)': 'ubicaci√≥n d√≥nde',
            r'(donde|d[o√≥]nde)\s+(puedo|se\s+puede|hago)': 'd√≥nde',
            r'(horario|hora|cuando|cu[a√°]ndo)\s+(atiend|abre|funciona|est[a√°]\s+abierto)': 'horario',
            r'(plata|dinero|lucas?)\b': 'costo dinero',
            r'(comida|almuerzo|almorzar|comer)': 'casino alimentaci√≥n',
        }
        
        for pattern, replacement in duoc_patterns.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        # Limpieza final - EVITAR DUPLICADOS Y OPTIMIZAR
        text = re.sub(r'[^\w\s√°√©√≠√≥√∫√±√º]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # Eliminar palabras duplicadas
        words = text.split()
        unique_words = []
        seen = set()
        for word in words:
            if word not in seen:
                seen.add(word)
                unique_words.append(word)
        return ' '.join(unique_words)

    def process_user_query(self, user_message: str, session_id: str = None,
                          conversational_context: str = None, user_profile: dict = None) -> Dict:
        """PROCESAMIENTO INTELIGENTE MEJORADO CON SMART KEYWORD DETECTION + PRIORITY KEYWORDS"""
        from app.smart_keyword_detector import smart_keyword_detector
        from app.priority_keyword_system import priority_keyword_system
        
        self.metrics['total_queries'] += 1
        
        query_lower = user_message.lower().strip()
        
        # 0A. DETECCI√ìN DE KEYWORDS ABSOLUTAS (M√ÅXIMA PRIORIDAD)
        priority_detection = priority_keyword_system.detect_absolute_keyword(user_message)
        if priority_detection:
            print(f"üî• KEYWORD ABSOLUTA DETECTADA: '{priority_detection['keyword']}' "
                  f"(priority: {priority_detection['priority']}, category: {priority_detection['category']})")
            logger.info(f"üî• Priority keyword: {priority_detection['keyword']} ‚Üí "
                       f"{priority_detection['category']}/{priority_detection['topic']} "
                       f"(avoid_expansion: {priority_detection['avoid_expansion']})")
        
        # 0B. DETECCI√ìN INTELIGENTE DE KEYWORDS (SEGUNDA PRIORIDAD)
        keyword_analysis = smart_keyword_detector.detect_keywords(user_message)
        
        # Si hay keyword de alta confianza, usarla para orientar la b√∫squeda
        if keyword_analysis['confidence'] >= 80 and keyword_analysis['primary_keyword']:
            print(f"üéØ KEYWORD SMART: {keyword_analysis['primary_keyword']} "
                  f"({keyword_analysis['match_type']}, {keyword_analysis['confidence']}%)")
            logger.info(f"üéØ Smart detection: {keyword_analysis['primary_keyword']} ‚Üí "
                       f"{keyword_analysis['category']}/{keyword_analysis['topic']}")
        
        # 1. DETECCI√ìN DE IDIOMA Y CATEGOR√çA (UNA SOLA VEZ)
        try:
            classification_info = classifier.get_classification_info(user_message)
            detected_language = classification_info.get('language', 'es')
            
            # üéØ USAR PRIORITY KEYWORD PRIMERO, luego SMART DETECTOR
            if priority_detection:
                category = priority_detection['category']
                confidence = priority_detection['confidence']
                print(f"üî• Categor√≠a: {category} (priority, conf: {confidence:.2f})")
                logger.info(f"üî• Category: {category} from priority keyword")
            elif keyword_analysis['confidence'] >= 80 and keyword_analysis['category']:
                category = keyword_analysis['category']
                confidence = keyword_analysis['confidence'] / 100.0
                print(f"‚ú® Categor√≠a: {category} (smart, conf: {confidence:.2f})")
            else:
                category = classification_info.get('category', 'otros')
                confidence = classification_info.get('confidence', 0.5)
            
            print(f"üåç Idioma: {detected_language} | Categor√≠a: {category} ({confidence:.2f})")
            logger.info(f"üîç '{user_message}' -> {category} ({detected_language}) {confidence:.2f}")
        except Exception as e:
            logger.warning(f"Error en clasificaci√≥n, usando fallback: {e}")
            detected_language = self.detect_language(user_message)
            
            if priority_detection:
                category = priority_detection['category']
                confidence = priority_detection['confidence']
            elif keyword_analysis['confidence'] >= 80 and keyword_analysis['category']:
                category = keyword_analysis['category']
                confidence = keyword_analysis['confidence'] / 100.0
            else:
                category = classifier.classify_question(user_message)
                confidence = 0.6
        
        # 2. VERIFICAR TEMPLATES (M√ÅXIMA PRIORIDAD)
        template_match = classifier.detect_template_match(user_message)
        if template_match:
            print(f"üìã Template: {template_match} ({detected_language})")
            logger.info(f"‚úÖ Template '{template_match}' detectado")
            return {
                'processing_strategy': 'template',
                'original_query': user_message,
                'detected_language': detected_language,  # üî• CACHEAR IDIOMA
                'template_id': template_match,
                'detected_language': detected_language,
                'category': category,
                'query_parts': [user_message]
            }
        
        # 2. SI NO HAY TEMPLATE, BUSCAR EN MEMORIA (SEGUNDA PRIORIDAD)
        similar_queries = self.memory_manager.find_similar_queries(user_message)
        if similar_queries:
            best_match = similar_queries[0]
            if best_match['similarity'] > 0.85:  # Alta confianza en la similitud
                print(f"üíæ Memoria: {best_match['similarity']:.1%}")
                logger.info(f"üíæ Memoria: {best_match['similarity']:.3f}")
                return {
                    'processing_strategy': 'memory',
                    'original_query': user_message,
                    'detected_language': detected_language,  # üî• CACHEAR IDIOMA
                    'cached_response': best_match['response'],
                    'similarity_score': best_match['similarity'],
                    'metadata': best_match['metadata']
                }
        
        # 3. DETECCI√ìN PRIORITARIA DE SALUDOS
        greeting_keywords = [
            'hola', 'holi', 'holis', 'holaa', 'buenos d√≠as', 'buenas tardes', 
            'buenas noches', 'saludos', 'qui√©n eres', 'presentate', 'presentaci√≥n',
            'qu√© eres', 'tu nombre', 'hola ina', 'hola in√°', 'ina hola'
        ]
        
        if any(greeting in query_lower for greeting in greeting_keywords):
            logger.info(f"SALUDO DETECTADO: {user_message}")
            self.metrics['greetings'] += 1
            return {
                'processing_strategy': 'greeting',
                'original_query': user_message,
                'topic_classification': {'topic': 'greeting', 'type': 'allowed', 'confidence': 0.95},
                'is_greeting': True,
                'query_parts': [user_message]
            }
        
        # 4. DETECCI√ìN PRIORITARIA DE URGENCIAS/CRISIS
        emergency_keywords = [
            'crisis', 'urgencia', 'emergencia', 'l√≠nea ops', 
            'me siento mal', 'ayuda urgente', 'necesito ayuda ahora',
            'estoy desesperado', 'no puedo m√°s', 'pensamientos suicidas',
            'ataque de p√°nico', 'ansiedad extrema', 'angustia severa'
        ]
        
        if any(keyword in query_lower for keyword in emergency_keywords):
            logger.warning(f"URGENCIA DETECTADA: {user_message}")
            self.metrics['emergencies'] += 1
            return {
                'processing_strategy': 'emergency',
                'original_query': user_message,
                'topic_classification': {
                    'topic': 'bienestar_estudiantil', 
                    'type': 'allowed',
                    'confidence': 0.95
                },
                'is_emergency': True,
                'query_parts': [user_message]
            }
        
        # 5. BUSCAR EN CHROMADB PRIMERO antes de decidir derivar
        topic_info = self.topic_classifier.classify_topic(user_message)
        
        # üî• NUEVO: Intentar b√∫squeda en ChromaDB ANTES de derivar
        chromadb_has_info = False
        try:
            logger.info(f"üîç Pre-b√∫squeda en ChromaDB para: '{user_message}'")
            test_search = self.hybrid_search(user_message, n_results=10)  # Buscar m√°s resultados
            
            # Verificar si hay resultados con relevancia razonable
            if test_search and len(test_search) > 0:
                best_score = test_search[0].get('similarity', 0.0)
                if best_score >= 0.20:  # Umbral M√ÅS bajo para capturar nuevos documentos
                    chromadb_has_info = True
                    logger.info(f"‚úÖ ChromaDB tiene informaci√≥n: {len(test_search)} docs, mejor score: {best_score:.3f}")
                else:
                    logger.info(f"‚ö†Ô∏è ChromaDB: relevancia baja (mejor: {best_score:.3f})")
            else:
                logger.info(f"‚ö†Ô∏è ChromaDB: sin resultados")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error en pre-b√∫squeda ChromaDB: {e}")
        
        # 5b. DERIVAR SOLO SI ChromaDB NO TIENE INFORMACI√ìN
        should_derive = self.topic_classifier.should_derive(user_message)
        if should_derive and not chromadb_has_info:
            logger.info(f"DERIVACI√ìN ACTIVADA: ChromaDB sin info + should_derive=True")
            self.metrics['derivations'] += 1
            return {
                'processing_strategy': 'derivation',
                'original_query': user_message,
                'topic_classification': topic_info,
                'derivation_suggestion': self.topic_classifier.get_derivation_suggestion(topic_info.get('category', 'unknown')),
                'multiple_queries_detected': False,
                'query_parts': [user_message]
            }
        elif should_derive and chromadb_has_info:
            logger.info(f"üéØ ANULANDO DERIVACI√ìN: ChromaDB tiene informaci√≥n relevante")
        
        # 6. Detectar consultas m√∫ltiples SOLO para temas institucionales
        query_parts = self.topic_classifier.detect_multiple_queries(user_message)
        
        response_info = {
            'original_query': user_message,
            'detected_language': detected_language,  # üî• CACHEAR IDIOMA
            'topic_classification': topic_info,
            'multiple_queries_detected': len(query_parts) > 1,
            'query_parts': query_parts,
            'processing_strategy': 'standard'
        }
        
        # ESTRATEGIAS DIFERENCIADAS MEJORADAS
        if topic_info.get('category') == 'unknown':
            response_info['processing_strategy'] = 'clarification'
            self.metrics['ambiguous_queries'] += 1
            
        elif len(query_parts) > 1:
            response_info['processing_strategy'] = 'multiple_queries'
            self.metrics['multiple_queries'] += 1
            
        else:
            response_info['processing_strategy'] = 'standard_rag'
            
        logger.info(f"Procesamiento: '{user_message}' -> Estrategia: {response_info['processing_strategy']}")
        
        return response_info

    def detect_language(self, query: str) -> str:
        """Detecta el idioma con prioridad correcta para espa√±ol"""
        query_lower = query.lower()
        
        # ================================================================
        # PASO 1: DETECCI√ìN DIRECTA DE CONSULTAS FRANCESAS INEQU√çVOCAS
        # ================================================================
        ultra_specific_french_queries = [
            'comment fonctionne l\'assurance',
            'comment fonctionne assurance',
            'comment renouveler ma tne',
            'comment obtenir ma tne',
            'quelles sont les cat√©gories',
            'programme d\'urgence',
            'quand puis-je postuler',
            'informations sur les programmes',
            'conditions pour postuler',
            'elle est perdue ou endommag√©e',
            'programmes de soutien aux √©tudiants',
            'offrez-vous des simulations d\'entretiens d\'embauche',
            'offrez-vous des simulations d\'entretiens'
        ]
        
        # RETORNO INMEDIATO solo para consultas 100% francesas
        for direct_query in ultra_specific_french_queries:
            if direct_query in query_lower:
                print(f"   üî• ULTRA-SPECIFIC FRENCH MATCH: '{direct_query}' -> FORCING FRENCH")
                return 'fr'
        
        # PASO 1.1: DETECCI√ìN DIRECTA DE CONSULTAS INGLESAS INEQU√çVOCAS
        # ================================================================
        ultra_specific_english_queries = [
            'where can i access the duoc uc job bank',
            'duoc uc job bank access',
            'access duoc uc job bank'
        ]
        
        # RETORNO INMEDIATO solo para consultas 100% inglesas espec√≠ficas
        for direct_query in ultra_specific_english_queries:
            if direct_query in query_lower:
                print(f"   üî• ULTRA-SPECIFIC ENGLISH MATCH: '{direct_query}' -> FORCING ENGLISH")
                return 'en'
        
        # ================================================================
        # PASO 2: IDENTIFICADORES ESPA√ëOLES FUERTES (PRIORIDAD M√ÅXIMA)
        # ================================================================
        strong_spanish_indicators = {
            # Signos de puntuaci√≥n espa√±oles
            '¬ø': 50,    # Pregunta espa√±ola - INDICADOR M√ÅS FUERTE
            '¬°': 40,    # Exclamaci√≥n espa√±ola
            
            # Interrogativos espa√±oles espec√≠ficos
            'qu√©': 25,      # Con acento espa√±ol
            'c√≥mo': 25,     # Con acento espa√±ol
            'cu√°ndo': 25,   # Con acento espa√±ol
            'd√≥nde': 25,    # Con acento espa√±ol
            'cu√°les': 25,   # Con acento espa√±ol
            'cu√°ntos': 25,  # Con acento espa√±ol
            'cu√°ntas': 25,  # Con acento espa√±ol
            
            # Verbos espa√±oles comunes
            'puedo': 20,    # Primera persona singular
            'debo': 20,     # Primera persona singular
            'tengo': 20,    # Primera persona singular
            'necesito': 20, # Primera persona singular
            'quiero': 20,   # Primera persona singular
            's√©': 15,       # S√© con acento
            'est√°': 15,     # Est√° con acento
            'est√°s': 15,    # Est√°s con acento
            
            # Contexto institucional espa√±ol
            'duoc uc': 30,      # Nombre instituci√≥n
            'en duoc': 30,      # En la instituci√≥n
            'estudiante': 25,   # Sin s final (vs √©tudiants)
            'psic√≥logo': 25,    # T√©rmino acad√©mico espa√±ol
            'atenci√≥n': 20,     # Servicio espa√±ol
            'sesiones': 20,     # Plural espa√±ol
            'apoyo': 20,        # Servicio espa√±ol
            'curso': 15,        # Educativo espa√±ol
            'embajadores': 20,  # Programa espec√≠fico
            
            # Art√≠culos y conectores espa√±oles
            ' de la ': 15, ' del ': 15, ' con el ': 15,
            ' al ': 10, ' para ': 10, ' por ': 10,
        }
        
        # ================================================================
        # PASO 3: IDENTIFICADORES FRANCESES ESPEC√çFICOS
        # ================================================================
        specific_french_indicators = {
            # Interrogativos franceses √∫nicos
            'comment': 20,  # C√≥mo en franc√©s
            'quelles': 20,  # Plural femenino franc√©s
            'quand': 15,    # Cu√°ndo en franc√©s
            'puis-je': 30,  # Construcci√≥n √∫nica francesa
            
            # Verbos franceses espec√≠ficos
            'fonctionne': 25, # Funciona en franc√©s
            'renouveler': 25, # Renovar en franc√©s
            'obtenir': 20,    # Obtener en franc√©s
            'postuler': 20,   # Postular en franc√©s
            
            # Sustantivos franceses √∫nicos
            'assurance': 20,     # Seguro en franc√©s
            'programme': 15,     # Sin acento (vs programa)
            'urgence': 15,       # Urgencia en franc√©s
            'informations': 15,  # Plural franc√©s
            'soutien': 20,       # Apoyo en franc√©s
            '√©tudiants': 25,     # Con acento franc√©s y plural
            
            # Construcciones francesas espec√≠ficas
            'd\'urgence': 30,    # Ultra-espec√≠fico franc√©s
            'l\'assurance': 30,  # Ultra-espec√≠fico franc√©s
            'aux √©tudiants': 30, # A los estudiantes franc√©s
            
            # Art√≠culos y conectores franceses
            'pour': 8,  # Para en franc√©s (BAJO - puede confundirse)
            'sur': 8,   # Sobre en franc√©s (BAJO)
            # 'des': 10,  # ELIMINADO - aparece en palabras espa√±olas como "consejos para mejorar mi habilidades"
            'sont': 15, # Son/est√°n en franc√©s
        }
        
        # ================================================================
        # PASO 4: IDENTIFICADORES INGLESES (SIN FALSOS POSITIVOS)
        # ================================================================
        english_indicators = {
            'how': 15, 'what': 15, 'when': 15, 'where': 12, 'why': 12,
            'student': 15, 'insurance': 15, 'emergency': 15, 'support': 12,
            'programs': 12, 'information': 12, 'categories': 12,
            'apply': 12, 'obtain': 12, 'renew': 15, 'can': 8, 'should': 8,
            # REMOVIDO 'exist' - causa falsos positivos con 'existe' espa√±ol
        }
        
        # ================================================================
        # PASO 5: C√ÅLCULO DE SCORES CORREGIDO
        # ================================================================
        spanish_score = 0
        french_score = 0
        english_score = 0
        
        # Calcular puntuaci√≥n espa√±ola
        for indicator, weight in strong_spanish_indicators.items():
            if indicator in query_lower:
                spanish_score += weight
                print(f"   üá™üá∏ SPANISH KEYWORD: '{indicator}' +{weight} points")
        
        # Calcular puntuaci√≥n francesa
        for indicator, weight in specific_french_indicators.items():
            if indicator in query_lower:
                french_score += weight
                print(f"   üá´üá∑ FRENCH KEYWORD: '{indicator}' +{weight} points")
        
        # Calcular puntuaci√≥n inglesa
        for indicator, weight in english_indicators.items():
            if indicator in query_lower:
                english_score += weight
                print(f"   üá∫üá∏ ENGLISH KEYWORD: '{indicator}' +{weight} points")
        
        # ================================================================
        # PASO 6: MANEJO ESPECIAL DE ACENTOS (PROBLEMA PRINCIPAL)
        # ================================================================
        # Los acentos espa√±oles NO deben dar puntos al franc√©s
        spanish_accents = ['√≥', '√°', '√≠', '√∫', '√±']  # Acentos t√≠picamente espa√±oles
        french_accents = ['√®', '√™', '√†', '√π', '√ß', '√¥', '√Æ', '√Ø', '√´', '√º']  # Acentos t√≠picamente franceses
        
        # Solo contar acentos franceses si NO hay indicadores espa√±oles fuertes
        if spanish_score < 20:  # Solo si no hay indicadores espa√±oles claros
            french_accent_count = sum(1 for char in french_accents if char in query_lower)
            if french_accent_count > 0:
                accent_bonus = french_accent_count * 5  # REDUCIDO de 8 a 5
                french_score += accent_bonus
                print(f"   ‚ú® FRENCH ACCENTS: {french_accent_count} accents +{accent_bonus} points")
        
        # Bonus por acentos espa√±oles
        spanish_accent_count = sum(1 for char in spanish_accents if char in query_lower)
        if spanish_accent_count > 0:
            spanish_accent_bonus = spanish_accent_count * 10
            spanish_score += spanish_accent_bonus
            print(f"   üá™üá∏ SPANISH ACCENTS: {spanish_accent_count} accents +{spanish_accent_bonus} points")
        
        # ================================================================
        # PASO 7: PENALIZACIONES POR CONFUSI√ìN
        # ================================================================
        # Si detectamos "√©" en contexto espa√±ol, penalizar franc√©s
        if '√©' in query_lower and any(esp_word in query_lower for esp_word in ['qu√©', 'psic√≥log', 'm√©di']):
            french_penalty = 15
            french_score -= french_penalty
            print(f"   ‚õî FRENCH PENALTY FOR SPANISH CONTEXT: -{french_penalty} points")
        
        # Si detectamos "est" en contexto espa√±ol (como "existe"), penalizar franc√©s
        if 'est' in query_lower and any(esp_word in query_lower for esp_word in ['exist', 'cuest', 'contest', 'manifest', 'rest']):
            french_penalty = 10
            french_score -= french_penalty
            print(f"   ‚õî FRENCH 'EST' PENALTY IN SPANISH CONTEXT: -{french_penalty} points")
        
        # Si detectamos "les" en contexto espa√±ol (como "disponibles"), penalizar franc√©s
        if 'les' in query_lower and any(esp_word in query_lower for esp_word in ['disponib', 'posib', 'terrib']):
            french_penalty = 8
            french_score -= french_penalty
            print(f"   ‚õî FRENCH 'LES' PENALTY IN SPANISH CONTEXT: -{french_penalty} points")
        
        # ================================================================
        # PASO 8: LOGGING Y DECISI√ìN FINAL
        # ================================================================
        print(f"üîç Language detection: ES={spanish_score}, EN={english_score}, FR={french_score} para '{query_lower[:50]}...'")
        
        # REGLAS DE DECISI√ìN CORREGIDAS
        
        # 1. Si hay indicadores espa√±oles fuertes (¬ø, qu√©, puedo, etc.)
        if spanish_score >= 20:
            print(f"   üá™üá∏ DETECTED: SPANISH (STRONG INDICATORS: {spanish_score})")
            return 'es'
        
        # 2. Si hay indicadores franceses MUY espec√≠ficos sin confusi√≥n espa√±ola
        if french_score >= 35 and spanish_score < 10:
            print(f"   üá´üá∑ DETECTED: FRENCH (VERY SPECIFIC: {french_score} vs ES:{spanish_score})")
            return 'fr'
        
        # 3. Si espa√±ol domina claramente
        if spanish_score > french_score and spanish_score > english_score:
            print(f"   üá™üá∏ DETECTED: SPANISH (DOMINANT: {spanish_score} vs FR:{french_score} EN:{english_score})")
            return 'es'
        
        # 4. Si ingl√©s domina claramente
        if english_score >= 15 and english_score > spanish_score and english_score > french_score:
            print(f"   üá∫üá∏ DETECTED: ENGLISH (DOMINANT: {english_score} vs ES:{spanish_score} FR:{french_score})")
            return 'en'
        
        # 5. Si franc√©s tiene puntaje moderado SIN confusi√≥n
        if french_score >= 20 and spanish_score < 5 and english_score < french_score:
            print(f"   üá´üá∑ DETECTED: FRENCH (MODERATE CLEAN: {french_score} vs ES:{spanish_score} EN:{english_score})")
            return 'fr'
        
        # 6. Fallback: Priorizar espa√±ol por defecto
        if spanish_score > 0:
            print(f"   üá™üá∏ DETECTED: SPANISH (FALLBACK: {spanish_score})")
            return 'es'
        elif english_score > 0:
            print(f"   üá∫üá∏ DETECTED: ENGLISH (FALLBACK: {english_score})")
            return 'en'
        else:
            print(f"   üá™üá∏ DETECTED: SPANISH (DEFAULT)")
            return 'es'
    
    def generate_template_response(self, processing_info: Dict) -> Dict:
        """GENERAR RESPUESTA DESDE TEMPLATE CON QR CODES CORREGIDO CON SOPORTE MULTIIDIOMA"""
        import time
        start_time = time.time()
        
        template_id = processing_info['template_id']
        original_query = processing_info.get('original_query', '')
        
        # üî• USAR IDIOMA CACHEADO (ya detectado en process_user_query)
        detected_language = processing_info.get('detected_language', 'es')
        print(f"üåç Idioma: {detected_language}")
        logger.info(f"üåç Idioma: {detected_language}")
        
        # CARGAR TEMPLATES - PRIORIDAD AL SISTEMA MULTIIDIOMA
        try:
            template_response = None
            template_category = processing_info.get('category', 'asuntos_estudiantiles')
            
            # PRIMERO: Intentar con nuevo template_manager (RECOMENDADO)
            try:
                from app.template_manager.templates_manager import template_manager, detect_area_from_query
                
                # Detectar √°rea desde la query para tener el √°rea correcta
                detected_area_tuple = detect_area_from_query(original_query)
                detected_area = detected_area_tuple[0] if isinstance(detected_area_tuple, tuple) else detected_area_tuple
                
                # Usar template_manager directamente
                template_response = template_manager.get_template(detected_area, template_id, detected_language)
                template_category = detected_area
                
                if template_response:
                    print(f"\nüìÑ GENERANDO RESPUESTA DESDE TEMPLATE:")
                    print(f"   ‚úÖ Template encontrado: {template_id}")
                    print(f"   üìÇ √Årea: {template_category}")
                    print(f"   üåç Idioma: {detected_language}")
                    logger.info(f"‚úÖ Template multiidioma '{template_id}' encontrado en '{template_category}' idioma '{detected_language}'")
                else:
                    print(f"\n‚ö†Ô∏è  Template no encontrado en √°rea principal")
                    print(f"   üîç Buscando en otras √°reas...")
                    logger.warning(f"‚ùå Template multiidioma '{template_id}' NO encontrado en '{template_category}' idioma '{detected_language}'")
                    
                    # B√öSQUEDA AGRESIVA: Si no se encuentra en el √°rea detectada, buscar en TODAS las √°reas
                    print(f"üîç B√öSQUEDA AGRESIVA: Buscando template '{template_id}' en todas las √°reas...")
                    all_areas = ['asuntos_estudiantiles', 'bienestar_estudiantil', 'desarrollo_laboral', 'deportes', 'pastoral']
                    
                    for search_area in all_areas:
                        if search_area != detected_area:  # No buscar en el √°rea ya probada
                            try:
                                aggressive_template = template_manager.get_template(search_area, template_id, detected_language)
                                if aggressive_template:
                                    template_response = aggressive_template
                                    template_category = search_area
                                    print(f"‚úÖ Template encontrado en b√∫squeda agresiva: {template_id} en {search_area} ({detected_language})")
                                    logger.info(f"‚úÖ Template agresivo '{template_id}' encontrado en '{search_area}' idioma '{detected_language}'")
                                    break
                            except Exception as search_error:
                                continue  # Continuar b√∫squeda en otras √°reas
                
            except Exception as tm_error:
                logger.warning(f"Error en template_manager: {tm_error}")
                
                # SEGUNDO: Fallback a sistema MULTILINGUAL_TEMPLATES
                from app.templates import MULTILINGUAL_TEMPLATES, get_multilingual_template
                
                multilingua_response = get_multilingual_template(template_id, detected_language)
                if multilingua_response:
                    template_response = multilingua_response
                    print(f"‚úÖ Template multiidioma (fallback) encontrado: {template_id} en {template_category} ({detected_language})")
                    logger.info(f"‚úÖ Template multiidioma fallback '{template_id}' encontrado en '{template_category}' idioma '{detected_language}'")
                else:
                    print(f"‚ùå Template multiidioma (fallback) NO encontrado: {template_id} en {template_category} ({detected_language})")
                    logger.warning(f"‚ùå Template multiidioma fallback '{template_id}' NO encontrado en '{template_category}' idioma '{detected_language}'")
            
            # SEGUNDO: Si no se encontr√≥, usar sistema anterior
            if not template_response:
                from app.templates import TEMPLATES
                
                # Buscar template en todas las categor√≠as del sistema espa√±ol
                for category, templates in TEMPLATES.items():
                    if template_id in templates:
                        template_response = templates[template_id]
                        template_category = category
                        if detected_language != 'es':
                            print(f"‚ö†Ô∏è Usando template espa√±ol como fallback para idioma {detected_language}")
                        else:
                            print(f"üìã Template espa√±ol usado: {template_id} en {category}")
                        logger.info(f"‚úÖ Template espa√±ol '{template_id}' encontrado en categor√≠a '{template_category}'")
                        break
            
            # TERCERO: Si a√∫n no se encuentra, intentar multiidioma en espa√±ol
            if not template_response:
                try:
                    from app.template_manager.templates_manager import template_manager, detect_area_from_query
                    
                    detected_area_tuple = detect_area_from_query(original_query)
                    detected_area = detected_area_tuple[0]  # Solo tomar el √°rea, no la tupla completa
                    
                    # Override espec√≠fico para desinscripci√≥n deportiva
                    if template_id == "desinscripcion_optativos" and category == "deportes":
                        detected_area = "deportes"
                        logger.info(f"üîß Override: Forzando √°rea 'deportes' para template 'desinscripcion_optativos'")
                    
                    # Buscar template en nuevo sistema (espa√±ol como fallback)
                    template_response = template_manager.get_template(detected_area, template_id, 'es')
                    template_category = detected_area
                    
                    if template_response:
                        logger.info(f"‚úÖ Template fallback '{template_id}' encontrado en √°rea '{detected_area}' idioma 'es'")
                
                except Exception as e:
                    logger.warning(f"Error en sistema multiidioma fallback: {e}")
            
            # LOGGING DE RESULTADOS FINAL
            if template_response:
                logger.info(f"üéØ Template FINAL: '{template_id}' en idioma '{detected_language}' categor√≠a '{template_category}'")
            else:
                logger.warning(f"‚ùå Template '{template_id}' NO encontrado en ning√∫n sistema")
                
        except Exception as e:
            logger.error(f"Error cargando templates: {e}")
            template_response = None
            template_category = None
        
        # PROCESAR RESPUESTA SI SE ENCONTR√ì TEMPLATE
        if template_response:
                # AGREGAR GENERACI√ìN DE QR CODES PARA TEMPLATES (ESTRUCTURA CORREGIDA)
                original_query = processing_info['original_query']
                
                # MEJORAR LA RESPUESTA CON INFORMACI√ìN ESPEC√çFICA
                enhanced_response = enhance_final_response(template_response, original_query, template_category)
                
                qr_processed_response = qr_generator.process_response(enhanced_response, original_query)
                
                response_time = time.time() - start_time
                self.metrics['template_responses'] += 1
                self.metrics['categories_used'][template_category] += 1
                
                logger.info(f"TEMPLATE RESPONSE: {template_id} en {response_time:.3f}s")
                if qr_processed_response['has_qr']:
                    logger.info(f"QR generados desde template: {qr_processed_response['total_qr_generated']} c√≥digos")
                
                # ESTRUCTURA CORREGIDA - qr_codes como dict simple
                return {
                    'response': enhanced_response.strip(),
                    'sources': [],
                    'category': template_category,
                    'response_time': response_time,
                    'cache_type': 'template',
                    'processing_info': processing_info,
                    'template_used': template_id,
                    'qr_codes': qr_processed_response['qr_codes'],  # Dict simple {url: qr_image}
                    'has_qr': qr_processed_response['has_qr']       # Boolean
                }
        else:
            logger.warning(f"Template no encontrado: {template_id}")
            # Fallback si no se encuentra el template
            return self.generate_clarification_response(processing_info)

    def generate_greeting_response(self, processing_info: Dict) -> Dict:
        """RESPUESTA CORTA Y AMIGABLE PARA SALUDOS CON QR"""
        import random
        import time
        start_time = time.time()
        
        greeting_options = [
            "¬°Hola! Soy InA, tu asistente del Punto Estudiantil Duoc UC. ¬øEn qu√© puedo ayudarte hoy?",
            "¬°Hola! Soy InA, estoy aqu√≠ para ayudarte con informaci√≥n del Punto Estudiantil.",
            "¬°Hola! Soy InA, tu asistente de Duoc UC. ¬øQu√© necesitas saber?",
            "¬°Hola! Soy InA, del Punto Estudiantil. ¬øEn qu√© te puedo ayudar?",
        ]
        
        greeting = random.choice(greeting_options)
        
        # SUGERENCIAS DE CONSULTAS COMUNES
        suggestions = """
        
Puedo ayudarte con:*
‚Ä¢ TNE, certificados, programas de apoyo
‚Ä¢ Salud mental, bienestar estudiantil  
‚Ä¢ Deportes, talleres, gimnasio
‚Ä¢ CV, pr√°cticas, empleabilidad

¬øQu√© necesitas? 
"""
        
        response = greeting + suggestions
        
        # AGREGAR QR CODES PARA GREETING (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(response, processing_info['original_query'])
        
        # ESTRUCTURA CORREGIDA
        return {
            'response': response.strip(),
            'sources': [],
            'category': 'greeting',
            'response_time': time.time() - start_time,
            'cache_type': 'greeting',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # Dict simple
            'has_qr': qr_processed_response['has_qr']       # Boolean
        }

    def generate_emergency_response(self, processing_info: Dict) -> Dict:
        """RESPUESTA DE EMERGENCIA PRIORITARIA CON QR"""
        import time
        start_time = time.time()
        
        response = """
    **URGENCIA - APOYO INMEDIATO DISPONIBLE**

    *L√≠neas de ayuda 24/7:*
    ‚Ä¢ **L√≠nea OPS Duoc UC**: +56 2 2820 3450
    ‚Ä¢ **Salud Responde**: 600 360 7777
    ‚Ä¢ **Fono Mayor**: 800 4000 35

    *Atenci√≥n en sede:*
    ‚Ä¢ **Sala primeros auxilios**: Piso 2, Sede Plaza Norte
    ‚Ä¢ **Tel√©fono interno**: +56 2 2999 3005

    *Recuerda: No est√°s solo/a - hay ayuda disponible*

    *Si es emergencia m√©dica vital, llama al 131*
    """
        
        # AGREGAR QR CODES PARA EMERGENCIA (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(response, processing_info['original_query'])
        
        # ESTRUCTURA CORREGIDA
        return {
            'response': response.strip(),
            'sources': [],
            'category': 'emergency',
            'response_time': time.time() - start_time,
            'cache_type': 'emergency',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # Dict simple
            'has_qr': qr_processed_response['has_qr']       # Boolean
        }

    def generate_derivation_response(self, processing_info: Dict) -> Dict:
        """DERIVACI√ìN MEJORADA CON INFORMACI√ìN ESPEC√çFICA Y QR - FORMATO ESTRUCTURADO"""
        import time
        start_time = time.time()
        
        # Generar respuesta estructurada similar a las respuestas autom√°ticas
        response = (
            "Para esta consulta espec√≠fica:\n\n"
            "üè¢ **Punto Estudiantil Plaza Norte**\n"
            "üìç Ubicaci√≥n: Piso 2, Sede Plaza Norte\n"
            "üìû Tel: +56 2 2999 3075\n"
            "üïí Horario: Lunes a Viernes 08:30-22:30, S√°bados 08:30-14:00\n\n"
            "El personal puede orientarte seg√∫n tu consulta espec√≠fica.\n\n"
            "üí° **Tambi√©n puedo ayudarte con**: TNE, bienestar, deportes o desarrollo laboral"
        )
        
        # AGREGAR QR CODES PARA DERIVACI√ìN (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(response, processing_info['original_query'])
        
        # ESTRUCTURA CORREGIDA
        return {
            'response': response,
            'sources': [],
            'category': 'derivation',
            'response_time': time.time() - start_time,
            'cache_type': 'derivation',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # Dict simple
            'has_qr': qr_processed_response['has_qr']       # Boolean
        }

    def generate_multiple_queries_response(self, processing_info: Dict) -> Dict:
        """RESPUESTA OPTIMIZADA PARA CONSULTAS M√öLTIPLES CON QR"""
        import time
        start_time = time.time()
        
        query_parts = processing_info['query_parts']
        original_query = processing_info['original_query']
        
        logger.info(f"Procesando {len(query_parts)} consultas m√∫ltiples: {query_parts}")
        
        # ESTRATEGIA MEJORADA
        detailed_responses = []
        all_sources = []
        
        for i, part in enumerate(query_parts):
            logger.info(f"  Procesando parte {i+1}: '{part}'")
            
            # BUSCAR CON T√âRMINOS EXPANDIDOS
            expanded_query = self._expand_query_with_context(part, original_query)
            sources = self.hybrid_search(expanded_query, n_results=2)
            
            if sources:
                part_response = self._process_with_ollama_optimized(expanded_query, sources)
                response_text = part_response['response']
                
                # MEJORAR CALIDAD DE RESPUESTA
                if "no hay informaci√≥n" in response_text.lower() or "consulta en punto estudiantil" in response_text.lower():
                    # Intentar con b√∫squeda m√°s amplia
                    broader_sources = self.hybrid_search(part, n_results=3)
                    if broader_sources:
                        part_response = self._process_with_ollama_optimized(part, broader_sources)
                
                detailed_responses.append(f"**{i+1}. {part}:**\n{part_response['response']}")
                all_sources.extend(part_response['sources'])
            else:
                # RESPUESTA M√ÅS √öTIL CON INFORMACI√ìN GEN√âRICA
                generic_info = self._get_generic_topic_info(part)
                detailed_responses.append(f"**{i+1}. {part}:**\n{generic_info}")
        
        # CONSTRUIR RESPUESTA M√ÅS COHERENTE
        if detailed_responses:
            response = "**Varias consultas detectadas:**\n\n" + "\n\n".join(detailed_responses)
            response += "\n\n¬øNecesitas m√°s detalles de alguna consulta?*"
        else:
            response = "No pude procesar todas las consultas. ¬øPodr√≠as reformularlas por separado?"
        
        processing_time = time.time() - start_time
        logger.info(f"Consultas m√∫ltiples procesadas en {processing_time:.2f}s")
        
        # AGREGAR QR CODES PARA M√öLTIPLES CONSULTAS (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(response, original_query)
        
        # ESTRUCTURA CORREGIDA
        return {
            'response': response,
            'sources': all_sources[:3],
            'category': 'multiple_queries',
            'response_time': processing_time,
            'cache_type': 'multiple_queries',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # Dict simple
            'has_qr': qr_processed_response['has_qr']       # Boolean
        }

    def _expand_query_with_context(self, partial_query: str, full_query: str) -> str:
        """EXPANDIR CONSULTA PARCIAL CON CONTEXTO COMPLETO"""
        important_keywords = ['tne', 'deporte', 'taller', 'certificado', 'beca', 'psicol√≥gico', 'pr√°ctica']
        
        expanded = partial_query
        
        for keyword in important_keywords:
            if keyword in full_query and keyword not in partial_query:
                expanded += f" {keyword}"
        
        return expanded

    def _get_generic_topic_info(self, query: str) -> str:
        """INFORMACI√ìN GEN√âRICA POR TEMA CUANDO NO HAY FUENTES"""
        query_lower = query.lower()
        
        generic_responses = {
            'tne': "**TNE**: Para tr√°mites de Tarjeta Nacional Estudiantil, acude a Punto Estudiantil con tu c√©dula de identidad. Horario: L-V 8:30-19:00",
            'deporte': "**Deportes**: Duoc UC ofrece talleres deportivos, gimnasio y selecciones. Informaci√≥n en Complejo Deportivo Maiclub.",
            'taller': "**Talleres**: Hay talleres deportivos, culturales y de desarrollo. Consulta programaci√≥n en Punto Estudiantil.",
            'certificado': "**Certificados**: Solicita certificados de alumno regular en Punto Estudiantil o portal Mi Duoc.",
            'gimnasio': "**Gimnasio**: El Complejo Deportivo Maiclub tiene gimnasio, piscina y canchas. Horario: L-V 8:00-21:00.",
            'psicol√≥gico': "**Apoyo Psicol√≥gico**: Sesiones de apoyo psicol√≥gico disponibles. Contacta a Bienestar Estudiantil.",
            'pr√°ctica': "**Pr√°cticas**: Asesor√≠a para pr√°cticas profesionales con Claudia Cort√©s. Desarrollo Laboral, edificio central.",
        }
        
        for topic, response in generic_responses.items():
            if topic in query_lower:
                return response
        
        return "Consulta en Punto Estudiantil para informaci√≥n espec√≠fica sobre este tema."

    def _process_with_ollama_optimized(self, query: str, sources: List[Dict]) -> Dict:
        """VERSI√ìN OPTIMIZADA PARA EQUIPO FINAL CON OPTIMIZADOR INTELIGENTE"""
        try:
            limited_sources = sources[:2]
            
            if not limited_sources:
                return {
                    'response': "Consulta en Punto Estudiantil para m√°s informaci√≥n.",
                    'sources': []
                }
            
            # Usar el prompt estricto mejorado
            system_message = self._build_strict_prompt(limited_sources, query)
            
            response = ollama.chat(
                model=self.current_model,
                messages=[
                    {'role': 'system', 'content': 'Responde estrictamente en espa√±ol (Chile). No uses ingl√©s.'},
                    {'role': 'user', 'content': system_message}  # Todo en user para mayor claridad
                ],
                options={
                    'temperature': 0.0,  # M√°ximo determinismo
                    'num_predict': 120,  # Respuestas concisas
                    'top_p': 0.8,        # M√°s enfocado
                    'repeat_penalty': 1.5  # Evitar repeticiones
                }
            )
            
            # PROCESAR RESPUESTA CON OPTIMIZADOR INTELIGENTE
            raw_response = response['message']['content'].strip()
            
            # APLICAR OPTIMIZACI√ìN INTELIGENTE si est√° disponible
            if INTELLIGENT_OPTIMIZER_AVAILABLE:
                try:
                    category = sources[0]['metadata'].get('category', 'general') if sources else 'general'
                    optimization_result = optimize_rag_response(
                        raw_response, 
                        query, 
                        category,
                        sources=limited_sources
                    )
                    
                    if optimization_result.get('success'):
                        optimized_response = optimization_result['optimized_response']
                        logger.info(f"‚úÖ Respuesta optimizada: {optimization_result['original_length']} ‚Üí "
                                  f"{optimization_result['optimized_length']} chars "
                                  f"(calidad: {optimization_result['quality_score']}/100)")
                        
                        return {
                            'response': optimized_response,
                            'sources': [{
                                'content': source['document'][:80] + '...',
                                'category': source['metadata'].get('category', 'general'),
                                'similarity': round(source.get('similarity', 0.5), 3)
                            } for source in limited_sources],
                            'optimization_applied': True,
                            'quality_score': optimization_result['quality_score']
                        }
                except Exception as opt_error:
                    logger.warning(f"‚ö†Ô∏è Error en optimizaci√≥n, usando respuesta original: {opt_error}")
                    # Fallback a respuesta original
                    pass
            
            # Fallback: retornar respuesta sin optimizaci√≥n
            return {
                'response': raw_response,
                'sources': [{
                    'content': source['document'][:80] + '...',
                    'category': source['metadata'].get('category', 'general'),
                    'similarity': round(source.get('similarity', 0.5), 3)
                } for source in limited_sources],
                'optimization_applied': False
            }
            
        except Exception as e:
            logger.error(f"Error procesando con Ollama: {e}")
            if sources:
                short_response = sources[0]['document'][:100] + "..." if len(sources[0]['document']) > 100 else sources[0]['document']
                return {
                    'response': short_response,
                    'sources': []
                }
            else:
                return {
                    'response': "Consulta en Punto Estudiantil para informaci√≥n espec√≠fica.",
                    'sources': []
                }

    def generate_clarification_response(self, processing_info: Dict) -> Dict:
        """GENERAR RESPUESTA PARA CONSULTAS AMBIGUAS CON QR"""
        import time
        start_time = time.time()
        
        original_query = processing_info['original_query']
        
        response = f"""
No entiendo completamente '{original_query}'.

¬øTe refieres a alguno de estos temas?*

‚Ä¢ TNE y certificados
‚Ä¢ Programas de apoyo econ√≥mico  
‚Ä¢ Salud mental y bienestar
‚Ä¢ Deportes y actividades
‚Ä¢ Desarrollo laboral y CV

*Ejemplo: "¬øC√≥mo saco mi TNE?"*
"""
        
        # MEJORAR LA RESPUESTA DE CLARIFICACI√ìN CON CONTACTOS
        enhanced_response = enhance_final_response(response, original_query, "clarification")
        
        # AGREGAR QR CODES PARA CLARIFICATION (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(enhanced_response, original_query)
        
        # ESTRUCTURA CORREGIDA
        return {
            'response': enhanced_response.strip(),
            'sources': [],
            'category': 'clarification',
            'response_time': time.time() - start_time,
            'cache_type': 'clarification',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # Dict simple
            'has_qr': qr_processed_response['has_qr']       # Boolean
        }

    def add_document(self, document: str, metadata: Dict = None) -> bool:
        """AGREGAR DOCUMENTO AL RAG - OPTIMIZADO PARA MD/JSON CON METADATA ENRIQUECIDA"""
        try:
            doc_id = f"doc_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{hash(document) % 10000}"

            # Preservar todo el metadata que venga del loader (section, is_structured, optimized, etc.)
            enhanced_metadata = {"timestamp": datetime.now().isoformat()}
            
            # üî• FASE 3: Logging de metadata enriquecida MD/JSON
            source_type = 'unknown'
            
            if isinstance(metadata, dict):
                # Detectar tipo de fuente
                if 'source_type' in metadata:
                    source_type = metadata['source_type']
                elif metadata.get('type') == 'json_faq':
                    source_type = 'json_faq'
                elif metadata.get('type') == 'markdown_chunk':
                    source_type = 'markdown'
                elif 'departamento' in metadata or 'tema_principal' in metadata:
                    source_type = 'markdown_frontmatter'
                
                # No sobrescribir timestamp si viene en metadata
                for k, v in metadata.items():
                    if k == 'timestamp':
                        continue
                    # Convertir listas a strings para ChromaDB
                    if isinstance(v, list):
                        enhanced_metadata[k] = ', '.join(str(item) for item in v) if v else ''
                    # Convertir diccionarios a strings JSON para ChromaDB
                    elif isinstance(v, dict):
                        enhanced_metadata[k] = json.dumps(v) if v else '{}'
                    else:
                        enhanced_metadata[k] = v
                
                # Asegurar claves m√≠nimas si faltan
                enhanced_metadata.setdefault('source', metadata.get('source', 'unknown'))
                enhanced_metadata.setdefault('category', metadata.get('category', 'general'))
                enhanced_metadata.setdefault('type', metadata.get('type', 'general'))
                
                # üî• FASE 3: Logging mejorado para debugging
                if source_type in ['markdown', 'markdown_frontmatter', 'json_faq']:
                    logger.debug(f"‚úÖ Agregando chunk {source_type}: "
                               f"cat={enhanced_metadata.get('category', 'N/A')}, "
                               f"dept={enhanced_metadata.get('departamento', 'N/A')}, "
                               f"keywords={enhanced_metadata.get('keywords', '')[:40]}...")
                
            else:
                enhanced_metadata.update({
                    'source': 'unknown',
                    'category': 'general',
                    'type': 'general'
                })
            
            # Asegurar que keywords y chunk_id est√©n presentes
            if 'keywords' not in enhanced_metadata or not enhanced_metadata['keywords']:
                enhanced_metadata['keywords'] = ', '.join(self.extract_keywords(document))
            if 'chunk_id' not in enhanced_metadata or not enhanced_metadata['chunk_id']:
                enhanced_metadata['chunk_id'] = hashlib.md5(document.encode('utf-8')).hexdigest()

            # Verificar que la colecci√≥n es v√°lida antes de agregar
            if not hasattr(self.collection, 'add'):
                logger.error("Colecci√≥n inv√°lida - no tiene m√©todo add()")
                self.metrics['errors'] += 1
                return False

            self.collection.add(
                documents=[document],
                metadatas=[enhanced_metadata],
                ids=[doc_id]
            )
            
            self.metrics['documents_added'] += 1
            return True
            
        except Exception as e:
            logger.error(f"Error a√±adiendo documento: {e}")
            logger.debug(f"Tipo de colecci√≥n: {type(self.collection)}, Tiene add: {hasattr(self.collection, 'add')}")
            self.metrics['errors'] += 1
            return False

    def query(self, query_text: str, n_results: int = 3) -> List[str]:
        """QUERY B√ÅSICA"""
        try:
            results = self.collection.query(
                query_texts=[query_text],
                n_results=n_results
            )
            return results['documents'][0] if results['documents'] else []
        except Exception as e:
            logger.error(f"Error en query RAG: {e}")
            return []

    def query_optimized(self, query_text: str, n_results: int = 3, score_threshold: float = 0.25, 
                        metadata_filters: Dict = None):
        """B√öSQUEDA OPTIMIZADA CON METADATA FILTERS (DeepSeek)"""
        try:
            processed_query = self.enhanced_normalize_text(query_text)

            # Construir where_document para filtrado por metadata
            where_filter = None
            if metadata_filters:
                where_filter = {}
                if 'departamento' in metadata_filters:
                    where_filter['departamento'] = metadata_filters['departamento']
                if 'tema' in metadata_filters:
                    where_filter['tema'] = metadata_filters['tema']
                if 'content_type' in metadata_filters:
                    where_filter['content_type'] = metadata_filters['content_type']

            # Query con filtros opcionales
            query_params = {
                'query_texts': [processed_query],
                'n_results': n_results * 4,
                'include': ['distances', 'documents', 'metadatas']
            }
            if where_filter:
                query_params['where'] = where_filter
                logger.info(f"üîç Aplicando filtros: {where_filter}")

            results = self.collection.query(**query_params)

            filtered_docs = []
            for i, distance in enumerate(results['distances'][0]):
                similarity = 1 - distance
                
                current_threshold = score_threshold
                if 'd√≥nde' in query_text.lower() or 'ubicaci√≥n' in query_text.lower():
                    current_threshold = 0.15  # M√°s permisivo
                elif 'biblioteca' in query_text.lower() or 'estacionamiento' in query_text.lower():
                    current_threshold = 0.15  # M√°s permisivo para temas comunes
                
                if similarity >= current_threshold:
                    doc_content = results['documents'][0][i]
                    doc_metadata = results['metadatas'][0][i]
                    
                    # Boost score si keywords coinciden
                    keyword_boost = self._calculate_keyword_boost(query_text, doc_metadata)
                    adjusted_similarity = min(1.0, similarity + keyword_boost)
                    
                    if self._is_relevant_document_improved(processed_query, doc_content):
                        filtered_docs.append({
                            'document': doc_content,
                            'metadata': doc_metadata,
                            'similarity': adjusted_similarity
                        })

            filtered_docs.sort(key=lambda x: x['similarity'], reverse=True)
            
            if not filtered_docs:
                logger.warning(f"‚ö†Ô∏è No documentos con threshold {score_threshold}, reintentando con threshold m√°s bajo...")
                # FALLBACK: Reducir threshold dr√°sticamente 
                fallback_threshold = 0.1
                for i, distance in enumerate(results['distances'][0]):
                    similarity = 1 - distance
                    if similarity >= fallback_threshold:
                        doc_content = results['documents'][0][i]
                        doc_metadata = results['metadatas'][0][i]
                        keyword_boost = self._calculate_keyword_boost(query_text, doc_metadata)
                        adjusted_similarity = min(1.0, similarity + keyword_boost)
                        
                        filtered_docs.append({
                            'document': doc_content,
                            'metadata': doc_metadata,
                            'similarity': adjusted_similarity
                        })
                        
                if not filtered_docs:
                    logger.info(f"‚ùå Sin resultados incluso con threshold {fallback_threshold} para: {query_text}")
                    return []
                else:
                    logger.info(f"‚úÖ Fallback exitoso: {len(filtered_docs)} docs con threshold {fallback_threshold}")
            
            return filtered_docs[:n_results]

        except Exception as e:
            print(f"\n{'='*80}")
            print(f"‚ùå ERROR EN B√öSQUEDA CHROMADB")
            print(f"{'='*80}")
            print(f"üî¥ Error: {str(e)[:200]}")
            print(f"üîß Tipo: {type(e).__name__}")
            print(f"üìù Query: '{query_text}'")
            print(f"üîÑ Intentando b√∫squeda simple como fallback...")
            print(f"{'='*80}\n")
            
            logger.error(f"Error en query optimizada: {e}")
            # En caso de error, retornar resultados simples sin recursi√≥n
            try:
                simple_results = self.query(query_text, n_results)
                if simple_results:
                    print(f"‚úÖ B√∫squeda simple exitosa: {len(simple_results)} resultados")
                return [{'document': doc, 'metadata': {}, 'similarity': 0.7} for doc in simple_results]
            except Exception as fallback_error:
                print(f"‚ùå B√∫squeda simple tambi√©n fall√≥: {str(fallback_error)[:100]}")
                return []

    def _calculate_keyword_boost(self, query: str, metadata: Dict) -> float:
        """Calcula boost de relevancia basado en keywords del metadata"""
        if not metadata or 'keywords' not in metadata:
            return 0.0
        
        query_lower = query.lower()
        keywords_str = metadata.get('keywords', '')
        if not keywords_str:
            return 0.0
        
        # Convertir keywords (pueden ser string separado por comas o lista)
        if isinstance(keywords_str, str):
            keywords = [k.strip() for k in keywords_str.split(',')]
        else:
            keywords = keywords_str
        
        # Contar coincidencias de keywords en la query
        matches = sum(1 for kw in keywords if kw.lower() in query_lower)
        
        # Boost proporcional (m√°ximo +0.15)
        boost = min(0.15, matches * 0.05)
        if boost > 0:
            logger.debug(f"üìà Keyword boost: +{boost:.2f} ({matches} matches)")
        return boost
    
    def _is_relevant_document_improved(self, query: str, document: str) -> bool:
        """VERIFICACI√ìN DE RELEVANCIA MEJORADA"""
        query_words = set(query.lower().split())
        doc_words = set(document.lower().split())

        critical_keywords = {
            'tne', 'deporte', 'taller', 'gimnasio', 'certificado', 'beca', 
            'psicol√≥gico', 'claudia', 'elizabeth', 'adriana', 'duoc', 'estudiantil',
            'pr√°ctica', 'empleo', 'curriculum', 'entrevista'
        }
        
        critical_matches = critical_keywords.intersection(query_words)
        if critical_matches:
            doc_has_critical = any(keyword in document.lower() for keyword in critical_matches)
            if doc_has_critical:
                return True

        stop_words = {'el', 'la', 'los', 'las', 'de', 'en', 'y', 'que', 'con', 'para', 'por'}
        query_words = query_words - stop_words
        doc_words = doc_words - stop_words

        if not query_words:
            return True

        overlap = len(query_words.intersection(doc_words))
        relevance_ratio = overlap / len(query_words)

        return relevance_ratio >= 0.15

    def query_with_sources(self, query_text: str, n_results: int = 3) -> List[Dict]:
        """B√öSQUEDA CON FUENTES"""
        try:
            results = self.query_optimized(query_text, n_results, score_threshold=0.35)

            sources = []
            for result in results:
                sources.append({
                    'content': result['document'],
                    'category': result['metadata'].get('category', 'general'),
                    'source': result['metadata'].get('source', 'unknown'),
                    'similarity': result['similarity']
                })

            return sources

        except Exception as e:
            logger.error(f"Error en query con fuentes: {e}")
            return []

    def _build_strict_system_prompt(self, sources: List[Dict], user_query: str) -> str:
        """Construye un prompt de sistema estricto basado en contexto"""
        # Formatear fuentes con metadatos enriquecidos
        sources_text = []
        for i, source in enumerate(sources, 1):
            metadata = source.get('metadata', {})
            section = metadata.get('section', 'Sin secci√≥n')
            source_name = metadata.get('source', 'Desconocido')
            keywords = metadata.get('keywords', '')
            
            formatted = f"""[FUENTE {i}]
Documento: {source_name}
Secci√≥n: {section}
Palabras clave: {keywords}
Contenido:
{source['document'][:500]}...
"""
            sources_text.append(formatted)
        
        context = "\n\n".join(sources_text)
        
        return f"""Eres un asistente especializado de Duoc UC Plaza Norte.

INSTRUCCIONES OBLIGATORIAS:
1. Responde √öNICAMENTE con informaci√≥n del CONTEXTO proporcionado abajo
2. Si la informaci√≥n NO est√° en el contexto, responde EXACTAMENTE:
    "No tengo informaci√≥n actualizada sobre eso. Te recomiendo contactar a Punto Estudiantil al +56 2 2999 3075 o visitar centroayuda.duoc.cl"
3. S√© CONCISO: M√°ximo 4-5 l√≠neas + datos de contacto
4. Incluye informaci√≥n pr√°ctica: horarios, ubicaciones, tel√©fonos, correos
5. Cita la secci√≥n del documento: "Seg√∫n [secci√≥n], ..."
6. NO inventes informaci√≥n que no est√© en el contexto
7. NO uses frases gen√©ricas como "estamos aqu√≠ para ayudarte"

CONTEXTO DISPONIBLE:
{context}

PREGUNTA DEL ESTUDIANTE:
{user_query}

RESPUESTA (basada SOLO en el contexto):"""
    
    def hybrid_search(self, query_text: str, n_results: int = 3) -> List[Dict]:
        """B√öSQUEDA H√çBRIDA MEJORADA CON MAYOR RECALL"""
        try:
            # Expandir query con sin√≥nimos y contexto
            expanded_query = self._expand_query(query_text)
            processed_query = self.enhanced_normalize_text(expanded_query)
            
            # üî• MEJORA: Buscar M√ÅS resultados (10x) con umbral M√ÅS BAJO para capturar documentos nuevos
            results = self.query_optimized(processed_query, n_results * 10, score_threshold=0.08)
            
            logger.info(f"üîç B√∫squeda h√≠brida: '{query_text[:50]}' ‚Üí {len(results)} resultados")

            # üî• MEJORA: Filtrar con umbral A√öN M√ÅS PERMISIVO para nuevos documentos
            filtered_docs = []
            for result in results:
                if result['similarity'] >= 0.12:  # Reducido de 0.15 a 0.12
                    filtered_docs.append(result)
                    logger.debug(f"  ‚úì Doc {result['metadata'].get('category', 'unknown')}: {result['similarity']:.3f}")
                    
            # Si a√∫n no hay resultados, tomar cualquier cosa por encima de 0.06 (reducido de 0.08)
            if not filtered_docs:
                for result in results:
                    if result['similarity'] >= 0.06:
                        filtered_docs.append(result)
                        logger.debug(f"  ‚ö° Fallback doc {result['metadata'].get('category', 'unknown')}: {result['similarity']:.3f}")

            # Ordenar por relevancia
            filtered_docs.sort(key=lambda x: x['similarity'], reverse=True)
            
            # Retornar top resultados
            final_results = filtered_docs[:n_results]
            if final_results:
                logger.info(f"‚úÖ Retornando {len(final_results)} documentos (mejor: {final_results[0]['similarity']:.3f})")
            else:
                logger.warning(f"‚ö†Ô∏è No se encontraron documentos relevantes para: '{query_text}'")
            
            return final_results

        except Exception as e:
            logger.error(f"‚ùå Error en hybrid search: {e}")
            return []

    def get_cache_stats(self) -> Dict:
        """ESTAD√çSTICAS MEJORADAS"""
        stats = {
            'text_cache_size': len(self.text_cache),
            'semantic_cache_size': len(self.semantic_cache.cache),
            'metrics': self.metrics,
            'semantic_cache_enabled': self.semantic_cache.model is not None,
            'total_documents': self.collection.count() if hasattr(self.collection, 'count') else 'N/A',
            'duoc_context': self.duoc_context,
            'processing_stats': {
                'total_derivations': self.metrics['derivations'],
                'total_multiple_queries': self.metrics['multiple_queries'],
                'total_ambiguous': self.metrics['ambiguous_queries'],
                'total_greetings': self.metrics['greetings'],
                'total_emergencies': self.metrics['emergencies'],
                'total_templates': self.metrics['template_responses']  # 
            }
        }

        if self.metrics['response_times']:
            avg_time = sum(self.metrics['response_times']) / len(self.metrics['response_times'])
            stats['average_response_time'] = round(avg_time, 3)
        else:
            stats['average_response_time'] = 0

        return stats


# OPTIMIZACI√ìN: Lazy loading del motor RAG
# No se inicializa hasta que se use por primera vez
_rag_engine_instance = None
_rag_engine_initializing = False

def _get_rag_engine():
    """Obtener instancia de RAG Engine con lazy loading"""
    global _rag_engine_instance, _rag_engine_initializing
    
    if _rag_engine_instance is None and not _rag_engine_initializing:
        _rag_engine_initializing = True
        import time
        start = time.time()
        print(f"‚è±Ô∏è  Inicializando RAG Engine bajo demanda...")
        _rag_engine_instance = RAGEngine()
        elapsed = time.time() - start
        print(f"‚è±Ô∏è  RAG Engine inicializado en {elapsed:.2f}s")
        _rag_engine_initializing = False
    
    return _rag_engine_instance

# Property que simula una instancia pero es lazy
class LazyRAGEngine:
    """Proxy lazy para RAG Engine - solo se inicializa cuando se accede a un atributo/m√©todo"""
    def __getattr__(self, name):
        # Solo inicializar cuando realmente se accede a un atributo
        engine = _get_rag_engine()
        if engine is None:
            raise RuntimeError("RAG Engine no inicializado todav√≠a")
        return getattr(engine, name)
    
    def __setattr__(self, name, value):
        engine = _get_rag_engine()
        if engine is None:
            raise RuntimeError("RAG Engine no inicializado todav√≠a")
        return setattr(engine, name, value)
    
    def __call__(self, *args, **kwargs):
        engine = _get_rag_engine()
        if engine is None:
            raise RuntimeError("RAG Engine no inicializado todav√≠a")
        return engine(*args, **kwargs)

# Instancia global del motor RAG (lazy)
rag_engine = LazyRAGEngine()


def get_ai_response(user_message: str, context: list = None, 
                   conversational_context: str = None, user_profile: dict = None) -> Dict:
    """VERSI√ìN MEJORADA - PROCESAMIENTO INTELIGENTE CON SMART KEYWORD DETECTION"""
    import time
    from app.smart_keyword_detector import smart_keyword_detector
    start_time = time.time()

    # üéØ BANNER INICIAL DE CONSULTA
    print(f"\n{'='*80}")
    print(f"üîç NUEVA CONSULTA RECIBIDA")
    print(f"{'='*80}")
    print(f"üìù Query: '{user_message}'")
    print(f"üìè Longitud: {len(user_message)} caracteres")
    print(f"‚è∞ Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}\n")
    logger.info(f"{'='*80}")
    logger.info(f"üîç NUEVA CONSULTA: '{user_message}' (len={len(user_message)})")
    logger.info(f"{'='*80}")

    # üîç PASO 0: Detecci√≥n inteligente de palabras clave con priorizaci√≥n
    print(f"üìå PASO 1: DETECCI√ìN INTELIGENTE DE KEYWORDS")
    keyword_analysis = smart_keyword_detector.detect_keywords(user_message)
    
    if keyword_analysis.get('primary_keyword'):
        print(f"   ‚úÖ Keyword detectada: '{keyword_analysis.get('primary_keyword')}'")
        print(f"   üìÇ Categor√≠a: {keyword_analysis.get('category', 'N/A')}")
        print(f"   üéØ Confianza: {keyword_analysis.get('confidence', 0)}%")
        print(f"   üîç Tipo match: {keyword_analysis.get('match_type', 'N/A')}")
        logger.info(f"üéØ Keyword: {keyword_analysis.get('primary_keyword')} | "
                   f"Cat: {keyword_analysis.get('category')} | "
                   f"Conf: {keyword_analysis.get('confidence')}%")
    else:
        print(f"   ‚ÑπÔ∏è  No se detect√≥ keyword espec√≠fica")
        logger.info(f"‚ÑπÔ∏è  No se detect√≥ keyword espec√≠fica")
    
    # Si hay una keyword clara, priorizar esa categor√≠a
    if keyword_analysis['confidence'] >= 80 and keyword_analysis['primary_keyword']:
        logger.info(f"‚ú® KEYWORD DE ALTA CONFIANZA detectada: {keyword_analysis['primary_keyword']} "
                   f"‚Üí Categor√≠a: {keyword_analysis['category']}")
        # No modificar la query original para consultas simples
        enhanced_query = user_message
    else:
        # Para consultas complejas, mantener mejora si existe
        enhanced_query = user_message

    # üî• PRIORIDAD ABSOLUTA: Procesar query con contexto inteligente PRIMERO (incluye detecci√≥n de templates)
    print(f"\nüìå PASO 2: PROCESAMIENTO INTELIGENTE DE QUERY")
    logger.info(f"üîÑ Llamando a process_user_query para: '{user_message}'")
    
    # Usar la consulta mejorada si es diferente
    query_to_process = enhanced_query if enhanced_query != user_message else user_message
    
    # Obtener instancia de RAG Engine (lazy loading)
    engine = _get_rag_engine()
    
    processing_info = engine.process_user_query(
        query_to_process, 
        conversational_context=conversational_context,
        user_profile=user_profile
    )
    strategy = processing_info['processing_strategy']
    
    # üéØ Agregar informaci√≥n inteligente de keywords al processing_info
    processing_info['keyword_analysis'] = keyword_analysis
    processing_info['smart_detection'] = {
        'primary_keyword': keyword_analysis.get('primary_keyword'),
        'category': keyword_analysis.get('category'),
        'topic': keyword_analysis.get('topic'),
        'confidence': keyword_analysis.get('confidence'),
        'match_type': keyword_analysis.get('match_type')
    }
    
    print(f"   ‚úÖ Estrategia determinada: {strategy.upper()}")
    print(f"   üìÇ Categor√≠a: {processing_info.get('category', 'N/A')}")
    print(f"   üåç Idioma: {processing_info.get('language', 'N/A')}")
    logger.info(f"üìã Estrategia: {strategy} | Cat: {processing_info.get('category')} | Lang: {processing_info.get('language')}")

    # üéØ SI ES TEMPLATE, PROCESARLO INMEDIATAMENTE (M√ÅXIMA PRIORIDAD)
    if strategy == 'template':
        print(f"\n‚ú® GENERANDO RESPUESTA DESDE TEMPLATE...")
        logger.info(f"‚ú® Generando respuesta desde template para: '{user_message}'")
        
        response_data = engine.generate_template_response(processing_info)
        
        # MEJORAR RESPUESTA DE TEMPLATE
        if 'response' in response_data:
            category = processing_info.get('category', 'template')
            enhanced_response = enhance_final_response(response_data['response'], user_message, category)
            response_data['response'] = enhanced_response
            print(f"‚úÖ Respuesta de template mejorada (categor√≠a: {category})")
            logger.info(f"‚úÖ Template response enhanced for category: {category}")
        
        response_data['response_time'] = time.time() - start_time
        response_data['intelligent_features_applied'] = True
        return response_data

    # üî• Inicializar sources para evitar error
    sources = []
    
    # üî• FALLBACK 1: Sistema h√≠brido DESACTIVADO para debugging del RAG mejorado
    print(f"\n‚ö†Ô∏è Sistema h√≠brido DESACTIVADO - forzando RAG mejorado con ChromaDB")
    if False and HYBRID_SYSTEM_AVAILABLE:
        try:
            hybrid_system = HybridResponseSystem()
            context_str = "\n".join(context) if context else ""
            
            hybrid_result = hybrid_system.generate_smart_response(user_message, context_str)
            
            if hybrid_result["success"]:
                # Generar QR codes para respuesta h√≠brida
                qr_processed_response = qr_generator.process_response(
                    hybrid_result["content"], user_message
                )
                
                return {
                    "response": hybrid_result["content"],
                    "qr_codes": qr_processed_response.get("qr_codes", []),
                    "response_type": f"hybrid_{hybrid_result['strategy']}",
                    "sources": hybrid_result["sources"],
                    "confidence": hybrid_result["confidence"],
                    "processing_time": hybrid_result["processing_time"],
                    "success": True
                }
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Sistema h√≠brido fall√≥, usando RAG tradicional: {e}")

    # üìö INTENTAR RAG PARA BIBLIOTECA ANTES DE DERIVAR
    if 'biblioteca' in user_message.lower() and len(sources) == 0:
        logger.info("üîç Detectada 'biblioteca' - intentando b√∫squeda RAG...")
        print(f"\nüîç Detectada consulta sobre biblioteca - buscando informaci√≥n...")
        try:
            sources_biblioteca = engine.query_optimized(
                query_text=user_message,
                n_results=5,
                score_threshold=0.25
            )
            if sources_biblioteca:
                sources = sources_biblioteca
                strategy = 'standard_rag'
                logger.info(f"‚úÖ Encontradas {len(sources_biblioteca)} fuentes para biblioteca")
                print(f"‚úÖ Fuentes encontradas: {len(sources_biblioteca)}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error buscando biblioteca: {e}")
    
    # üî• FALLBACK 2: An√°lisis de derivaci√≥n para IA estacionaria
    derivation_analysis = {"should_derive": False, "is_inappropriate": False, "is_emergency": False}
    if hasattr(engine, 'derivation_manager') and engine.derivation_manager:
        derivation_analysis = engine.derivation_manager.analyze_query(user_message)
        logger.info(f"üîç AN√ÅLISIS DERIVACI√ìN: {derivation_analysis}")
    
    # üî• FALLBACK: Filtro espec√≠fico para IA estacionaria
    stationary_analysis = {"has_auto_response": False}
    if hasattr(engine, 'stationary_filter') and engine.stationary_filter:
        stationary_analysis = engine.stationary_filter.analyze_query(user_message)
        logger.info(f"üõ°Ô∏è AN√ÅLISIS FILTRO ESTACIONARIO: {stationary_analysis}")
    
    # Manejar respuestas autom√°ticas para consultas fuera de alcance
    if stationary_analysis["has_auto_response"] and engine.stationary_filter:
        auto_response = engine.stationary_filter.get_auto_response(stationary_analysis["auto_response_key"])
        logger.info(f"ü§ñ RESPUESTA AUTOM√ÅTICA ACTIVADA: {stationary_analysis['auto_response_key']}")
        
        # Generar QR codes espec√≠ficos para respuestas autom√°ticas
        qr_processed_response = qr_generator.process_response(auto_response, user_message)
        
        return {
            "response": auto_response,
            "qr_codes": qr_processed_response.get('qr_codes', {}),
            "has_qr": qr_processed_response.get('has_qr', False),
            "response_time": time.time() - start_time,
            "stationary_filter_applied": True,
            "filter_reason": stationary_analysis["derivation_reason"]
        }
    
    # Manejar contenido inapropiado
    if derivation_analysis["is_inappropriate"]:
        return {
            "response": "No puedo proporcionar esa informaci√≥n. Para consultas espec√≠ficas, dir√≠gete al personal del Punto Estudiantil.",
            "qr_codes": {},
            "has_qr": False,
            "response_time": time.time() - start_time,
            "derivation_applied": True,
            "derivation_reason": "inappropriate_content"
        }
    
    # Manejar emergencias
    if derivation_analysis["is_emergency"] and engine.derivation_manager:
        emergency_response = engine.derivation_manager.generate_emergency_response()
        return {
            "response": emergency_response["response"],
            "qr_codes": {},
            "has_qr": False,
            "response_time": time.time() - start_time,
            "derivation_applied": True,
            "derivation_reason": "emergency"
        }

    # Agregar informaci√≥n de derivaci√≥n al processing_info
    processing_info['derivation_analysis'] = derivation_analysis
    
    # Agregar contexto conversacional al processing_info si est√° disponible
    if conversational_context:
        processing_info['conversational_context'] = conversational_context
        processing_info['has_conversation_history'] = True
        
    # Agregar perfil de usuario al processing_info si est√° disponible
    if user_profile:
        processing_info['user_profile'] = user_profile
        processing_info['user_preferences'] = user_profile.get('area_interes', [])

    if strategy == 'greeting' or processing_info.get('is_greeting', False):
        response_data = rag_engine.generate_greeting_response(processing_info)
        # MEJORAR RESPUESTA DE SALUDO
        if 'response' in response_data:
            enhanced_response = enhance_final_response(response_data['response'], user_message, 'greeting')
            response_data['response'] = enhanced_response
        response_data['response_time'] = time.time() - start_time
        response_data['intelligent_features_applied'] = True
        return response_data

    if strategy == 'emergency' or processing_info.get('is_emergency', False):
        response_data = rag_engine.generate_emergency_response(processing_info)
        # MEJORAR RESPUESTA DE EMERGENCIA
        if 'response' in response_data:
            enhanced_response = enhance_final_response(response_data['response'], user_message, 'emergency')
            response_data['response'] = enhanced_response
        response_data['response_time'] = time.time() - start_time
        response_data['intelligent_features_applied'] = True
        return response_data

    # ESTRATEGIAS DIFERENCIADAS
    if strategy == 'derivation':
        response_data = rag_engine.generate_derivation_response(processing_info)
        # MEJORAR RESPUESTA DE DERIVACI√ìN
        if 'response' in response_data:
            enhanced_response = enhance_final_response(response_data['response'], user_message, 'derivation')
            response_data['response'] = enhanced_response
        response_data['response_time'] = time.time() - start_time
        response_data['intelligent_features_applied'] = True
        return response_data

    elif strategy == 'multiple_queries':
        response_data = rag_engine.generate_multiple_queries_response(processing_info)
        # MEJORAR RESPUESTA DE M√öLTIPLES CONSULTAS
        if 'response' in response_data:
            enhanced_response = enhance_final_response(response_data['response'], user_message, 'multiple_queries')
            response_data['response'] = enhanced_response
        response_data['response_time'] = time.time() - start_time
        response_data['intelligent_features_applied'] = True
        return response_data

    elif strategy == 'clarification':
        response_data = rag_engine.generate_clarification_response(processing_info)
        # MEJORAR RESPUESTA DE CLARIFICACI√ìN
        if 'response' in response_data:
            enhanced_response = enhance_final_response(response_data['response'], user_message, 'clarification')
            response_data['response'] = enhanced_response
        response_data['response_time'] = time.time() - start_time
        response_data['intelligent_features_applied'] = True
        return response_data

    # ESTRATEGIA EST√ÅNDAR RAG MEJORADA CON CONTEXTO
    normalized_message = rag_engine.enhanced_normalize_text(user_message)
    
    # Generar cache key que incluya contexto conversacional si est√° presente
    cache_components = [user_message]
    if conversational_context:
        # Usar solo una parte del contexto para el cache key (evitar cache key muy largos)
        context_summary = conversational_context[-200:] if len(conversational_context) > 200 else conversational_context
        cache_components.append(context_summary)
    
    cache_key = f"rag_{hashlib.md5('|'.join(cache_components).encode()).hexdigest()}"

    # üî• CACHE DESHABILITADO TEMPORALMENTE - devolv√≠a respuestas malas
    # Necesitamos garantizar que SIEMPRE se ejecute Ollama para generar respuestas
    use_cache = False  # Cambiar a True cuando el sistema funcione correctamente
    
    if use_cache and cache_key in rag_engine.text_cache:
        cached_response = rag_engine.text_cache[cache_key]
        rag_engine.metrics['text_cache_hits'] += 1
        logger.info(f"RAG Text Cache HIT para: '{user_message}'")
        cached_response['response_time'] = time.time() - start_time
        return cached_response

    logger.info(f"üî• RAG Cache DESHABILITADO - generando respuesta fresca para: '{user_message}'")

    try:
        print(f"\nüìå PASO 3: B√öSQUEDA EN CHROMADB")
        print(f"   üìä ChromaDB status: {rag_engine.collection.count()} chunks totales")
        
        # üî• B√öSQUEDA AMPLIADA para mejorar recall de documentos nuevos
        query_lower = user_message.lower()
        if any(word in query_lower for word in ['d√≥nde', 'donde', 'ubicaci√≥n', 'horario']):
            n_results = 7  # Ampliado de 4 a 7
        elif any(word in query_lower for word in ['qu√©', 'que', 'cu√°l', 'cual', 'lista', 'todos']):
            n_results = 10  # Ampliado de 5 a 10
        elif any(word in query_lower for word in ['requisitos', 'c√≥mo', 'como', 'proceso']):
            n_results = 8  # Nuevo caso para consultas procedimentales
        else:
            n_results = 5  # Ampliado de 3 a 5 - mejor cobertura
        
        print(f"   üîé Buscando {n_results} resultados en ChromaDB...")
        sources = rag_engine.hybrid_search(user_message, n_results=n_results)
        
        # üî• FIX: Asegurar que sources siempre sea una lista
        if sources is None:
            sources = []
            logger.warning("‚ö†Ô∏è hybrid_search retorn√≥ None, usando lista vac√≠a")
        
        print(f"   ‚úÖ Fuentes recuperadas: {len(sources)}")
        logger.info(f"üìö Fuentes recuperadas de ChromaDB: {len(sources)}")
        
        final_sources = []
        seen_hashes = set()
        
        for source in sources:
            # üî• FIX: Validar que document no sea None
            if not source.get('document'):
                logger.warning(f"‚ö†Ô∏è Source con documento None detectado, saltando...")
                continue
                
            content_hash = hashlib.md5(source['document'].encode()).hexdigest()
            
            if content_hash in seen_hashes:
                continue
            seen_hashes.add(content_hash)
            
            # M√°ximo 3 fuentes para mantener respuestas concisas
            if len(final_sources) < 3:
                final_sources.append(source)
        
        # FILTRAR FUENTES DE MALA CALIDAD ANTES DE PROCESAR
        quality_sources = []
        for source in final_sources:
            content = source.get('document', '')
            metadata = source.get('metadata', {})
            
            # Detectar fuentes corruptas/malformateadas (m√°s espec√≠fico)
            bad_indicators = [
                'pregunta qu√© es tne respuesta la tarjeta',
                'pregunta que es tne respuesta la tarjeta',
                'pregunta sobre tne respuesta la',
                'pregunta tne respuesta la tarjeta'
            ]
            
            is_corrupt = any(bad in content.lower() for bad in bad_indicators)
            is_too_short = len(content.strip()) < 20  # Reducido de 50 a 20
            has_no_useful_info = content.count(' ') < 3  # Reducido de 5 a 3
            
            # PERMITIR m√°s fuentes v√°lidas
            if not (is_corrupt or is_too_short or has_no_useful_info):
                quality_sources.append(source)
            else:
                logger.warning(f"üóëÔ∏è Fuente de mala calidad filtrada: {content[:100]}...")
        
        # Si el filtrado elimin√≥ todas las fuentes, usar las originales para no quedar sin contexto
        final_sources = quality_sources if quality_sources else final_sources
        logger.info(f"üîç Fuentes de calidad seleccionadas: {len(final_sources)}")
        
        print(f"\nüìå PASO 5: SELECCI√ìN FINAL DE FUENTES")
        print(f"   üìã Fuentes seleccionadas: {len(final_sources)}")
        logger.info(f"üìã Fuentes finales para Ollama: {len(final_sources)}")
        
        if final_sources:
            print(f"   üìÇ ORIGEN DE LAS FUENTES (CHROMADB):")
            for i, src in enumerate(final_sources, 1):
                meta = src.get('metadata', {})
                section = meta.get('section', 'N/A')
                source_file = meta.get('source', meta.get('file_name', 'N/A'))
                chunk_id = meta.get('chunk_id', 'N/A')
                keywords = meta.get('keywords', [])
                if isinstance(keywords, str):
                    keywords = keywords.split(',')[:3]
                else:
                    keywords = keywords[:3]
                score = src.get('relevance_score', 0)
                token_count = meta.get('token_count', 'N/A')
                content_preview = src.get('document', '')[:100].replace('\n', ' ')
                
                print(f"      [{i}] üìÑ Archivo: {source_file}")
                print(f"          üìç Secci√≥n: '{section[:50]}'")
                print(f"          üè∑Ô∏è  Keywords: {', '.join(keywords)}")
                print(f"          üÜî Chunk: {chunk_id}")
                print(f"          ‚≠ê Score: {score:.2f} | üìä Tokens: {token_count}")
                print(f"          üìù Preview: {content_preview}...")
                print(f"          ---")
                logger.info(f"   Fuente {i}: file={source_file}, section={section[:30]}, keywords={keywords}, score={score:.2f}, tokens={token_count}")
        else:
            print(f"\n{'='*80}")
            print(f"‚ùå PASO 5 FALL√ì: NO HAY FUENTES DISPONIBLES")
            print(f"{'='*80}")
            print(f"üîç Query: '{user_message}'")
            print(f"üí° Posibles causas:")
            print(f"   - ChromaDB vac√≠o (verificar auto-reprocesamiento en startup)")
            print(f"   - Query muy espec√≠fica sin documentos relevantes")
            print(f"   - Threshold muy alto filtrando todos los resultados")
            print(f"üîÑ Soluci√≥n: Reiniciar servidor para forzar reprocesamiento")
            print(f"{'='*80}\n")
            logger.error(f"‚ùå NO HAY FUENTES DISPONIBLES - Verificar ChromaDB")

        system_message = (
            "Eres InA, asistente del Punto Estudiantil en DUOC UC Plaza Norte.\n\n"
            "INSTRUCCIONES CR√çTICAS:\n"
            "1. USA LA INFORMACI√ìN proporcionada abajo para responder\n"
            "2. S√© DIRECTO y ESPEC√çFICO - sin saludos ni presentaciones\n"
            "3. Responde en 2-4 l√≠neas m√°ximo\n"
            "4. NO inventes informaci√≥n que no est√© en las fuentes\n"
            "5. Si no tienes info suficiente, di 'Para m√°s informaci√≥n consulta en Punto Estudiantil'\n\n"
        )

        if final_sources:
            system_message += "=== INFORMACI√ìN DE LA BASE DE CONOCIMIENTO ===\n\n"
            for i, source in enumerate(final_sources):
                content = source['document']
                category = source['metadata'].get('category', 'general')
                # Usar m√°s contenido para dar contexto completo
                useful_content = content[:500] + "..." if len(content) > 500 else content
                system_message += f"[{category.upper()}]\n{useful_content}\n\n"
            
            system_message += (
                "RESPONDE usando esta informaci√≥n.\n"
                "Formato: Directo al punto, sin decoraciones ni emojis innecesarios.\n"
                "Si hay pasos o requisitos, enum√©ralos claramente."
            )
        else:
            system_message += "No hay informaci√≥n espec√≠fica disponible.\n"
            logger.warning(f"‚ö†Ô∏è NO HAY FUENTES para '{user_message}' - ChromaDB vac√≠o?")

        # NUEVO: Usar prompt estricto mejorado
        system_message = rag_engine._build_strict_prompt(final_sources, user_message)
        
        # üî• LOGGING CR√çTICO ANTES DE OLLAMA
        print(f"\nüìå PASO 6: GENERACI√ìN CON OLLAMA")
        print(f"   ü§ñ Modelo: {rag_engine.current_model}")
        print(f"   üìö Fuentes para contexto: {len(final_sources)}")
        print(f"   üìù Tama√±o del prompt: {len(system_message)} chars")
        print(f"   ‚öôÔ∏è Par√°metros:")
        print(f"      ‚Ä¢ Temperature: 0.1 (muy determinista)")
        print(f"      ‚Ä¢ Max tokens: 220 (conciso)")
        print(f"      ‚Ä¢ Context window: 4096")
        print(f"   ‚è≥ Generando respuesta...")
        logger.info(f"ü§ñ LLAMANDO A OLLAMA ({rag_engine.current_model}) para: '{user_message}'")
        logger.info(f"üìö Fuentes disponibles: {len(final_sources)}")
        logger.info(f"üìù System message length: {len(system_message)} chars")
        
        try:
            logger.info(f"‚è±Ô∏è Iniciando llamada a Ollama {rag_engine.current_model}...")
            import time as time_module
            ollama_start = time_module.time()
            response = ollama.chat(
                model=rag_engine.current_model,
                messages=[
                    {'role': 'system', 'content': 'Responde estrictamente en espa√±ol (Chile). No uses ingl√©s.'},
                    {'role': 'system', 'content': system_message},
                    {'role': 'user', 'content': user_message}
                ],
                options={
                    'temperature': 0.1,  # Muy determinista para concisi√≥n
                    'num_predict': 220,  # Reducido para respuestas concisas (350‚Üí220)
                    'top_p': 0.85,  # M√°s enfocado (0.9‚Üí0.85)
                    'repeat_penalty': 1.4,  # M√°s penalizaci√≥n a repeticiones (1.3‚Üí1.4)
                    'num_ctx': 4096  # Mayor contexto
                }
            )
            ollama_time = time_module.time() - ollama_start
            
            respuesta = response['message']['content'].strip()
            
            print(f"   ‚úÖ Respuesta generada exitosamente")
            print(f"   ‚è±Ô∏è Tiempo: {ollama_time:.2f}s")
            print(f"   üìù Longitud: {len(respuesta)} caracteres")
            print(f"   üìÑ Preview: {respuesta[:120]}...")
            logger.info(f"‚úÖ Ollama ({rag_engine.current_model}) respondi√≥ en {ollama_time:.2f}s")
            logger.info(f"üìù Respuesta: {len(respuesta)} chars")
            logger.info(f"üìÑ Preview: {respuesta[:150]}")
            
        except Exception as ollama_error:
            print(f"\n{'='*80}")
            print(f"‚ùå ERROR EN PASO 6 (OLLAMA)")
            print(f"{'='*80}")
            print(f"üî¥ Error: {str(ollama_error)[:200]}")
            print(f"üîß Tipo: {type(ollama_error).__name__}")
            print(f"ü§ñ Modelo: {rag_engine.current_model}")
            print(f"üìù Prompt length: {len(system_message)} caracteres")
            print(f"üîÑ Activando sistema de fallback...")
            print(f"{'='*80}\n")
            
            logger.error(f"‚ùå ERROR EN LLAMADA A OLLAMA: {ollama_error}")
            logger.error(f"‚ùå Tipo de error: {type(ollama_error).__name__}")
            logger.error(f"‚ùå Detalles: {str(ollama_error)}")
            
            # Si Ollama falla, construir respuesta estructurada desde las fuentes
            if final_sources:
                print(f"   ‚úÖ Usando {len(final_sources)} fuentes directamente")
                logger.warning(f"‚ö†Ô∏è Ollama fall√≥, usando {len(final_sources)} fuentes directas")
                
                # Construir respuesta estructurada manualmente
                first_source = final_sources[0]['document']
                category = final_sources[0]['metadata'].get('category', 'informaci√≥n')
                
                if 'tne' in user_message.lower():
                    if 'como' in user_message.lower() or 'c√≥mo' in user_message.lower() or 'obten' in user_message.lower():
                        respuesta = "Para solicitar la TNE, accede a www.duoc.cl/sedes/info-tne/. Debes ser alumno regular sin deudas pendientes. El proceso es gestionado por JUNAEB y el retiro se hace en asuntos estudiantiles. Contacto: +56 2 2585 6990. Mall Plaza Norte, horario lunes a viernes 9:00-19:00."
                    else:
                        respuesta = "La TNE es la Tarjeta Nacional Estudiantil que te permite obtener descuentos en el transporte p√∫blico de Santiago. Es gestionada por JUNAEB y Duoc UC Plaza Norte act√∫a como intermediario para validar tu condici√≥n estudiantil. Contacto: +56 2 2585 6990, Mall Plaza Norte."
                elif any(word in user_message.lower() for word in ['beneficio', 'beca', 'ayuda']):
                    respuesta = f"En Duoc UC Plaza Norte tienes acceso a becas JUNAEB, gratuidad estatal, becas internas (Excelencia Acad√©mica, Hermanos DUOC, Deportiva), y financiamiento en cuotas. Para informaci√≥n espec√≠fica, contacta Mesa de Servicios: +56 2 2585 6990, Mall Plaza Norte."
                else:
                    # Construir respuesta gen√©rica estructurada
                    clean_content = first_source[:300].replace('\n', ' ').strip()
                    respuesta = f"Seg√∫n la informaci√≥n de Duoc UC Plaza Norte: {clean_content}. Para m√°s detalles, contacta Mesa de Servicios: +56 2 2585 6990, Mall Plaza Norte."
            else:
                print(f"   ‚ùå Sin fuentes disponibles para fallback")
                logger.error(f"‚ùå Sin fuentes disponibles, retornando mensaje gen√©rico")
                respuesta = "No tengo informaci√≥n espec√≠fica sobre eso en este momento. Para consultas sobre servicios de Duoc UC Plaza Norte, contacta Mesa de Servicios: +56 2 2585 6990, Mall Plaza Norte, horario lunes a viernes 9:00-19:00, s√°bados 9:00-15:00."
        respuesta = _optimize_response(respuesta, user_message)
        logger.info(f"‚úÇÔ∏è Respuesta optimizada: {len(respuesta)} chars")

        # Filtro estacionario desactivado temporalmente para no bloquear respuestas v√°lidas
        # respuesta = rag_engine.stationary_filter.filter_response(respuesta, user_message)
        
        # ‚úÖ VALIDACI√ìN DE INFORMACI√ìN: Verificar que la respuesta tiene contenido √∫til
        bad_indicators = [
            "no encontr", "no dispongo", "no tengo informaci√≥n", "no tengo inform",
            "no puedo", "lo siento", "disculpa", "no cuento", "no dispongo de",
            "consulta en", "dir√≠gete a", "para m√°s informaci√≥n"
        ]
        
        response_lower = respuesta.lower()
        has_bad_indicator = any(ind in response_lower for ind in bad_indicators)
        is_too_short = len(respuesta.strip()) < 30
        is_too_generic = response_lower.count("punto estudiantil") > 1
        
        is_bad_response = is_too_short or (has_bad_indicator and is_too_generic)
        
        if is_bad_response:
            logger.warning(f"‚ö†Ô∏è RESPUESTA MALA DETECTADA: '{respuesta[:150]}'")
            logger.warning(f"  - Too short: {is_too_short} ({len(respuesta)} chars)")
            logger.warning(f"  - Bad indicator: {has_bad_indicator}")
            logger.warning(f"  - Too generic: {is_too_generic}")
            
            if final_sources:
                logger.info(f"üîß RECONSTRUYENDO desde {len(final_sources)} fuentes")
                
                # Construir respuesta directa estructurada
                direct_parts = []
                for i, src in enumerate(final_sources[:2], 1):
                    doc = src['document'].strip()
                    category = src.get('metadata', {}).get('category', 'informaci√≥n')
                    
                    # Limpiar el documento
                    if len(doc) > 600:
                        doc = doc[:600] + "..."
                    
                    # Agregar con formato
                    direct_parts.append(f"{doc}")
                
                respuesta = "\n\n".join(direct_parts)
                logger.info(f"‚úÖ Respuesta RECONSTRUIDA: {len(respuesta)} chars")
            else:
                logger.error(f"‚ùå No hay fuentes para reconstruir respuesta")
                respuesta = "No tengo informaci√≥n espec√≠fica sobre eso. Consulta en Punto Estudiantil, Piso 2, Sede Plaza Norte."
        
        # Validaci√≥n de apropiabilidad desactivada temporalmente
        # is_appropriate, validation_message = rag_engine.stationary_filter.validate_response_appropriateness(respuesta)
        # if not is_appropriate:
        #     logger.warning(f"Respuesta inapropiada detectada: {validation_message}")
        #     respuesta += "\n\nüìç Para esta consulta espec√≠fica, te recomiendo dirigirte al personal del Punto Estudiantil."

        # Derivaci√≥n solo si la respuesta es muy pobre
        if len(respuesta.strip()) < 50 and hasattr(rag_engine, 'derivation_manager') and rag_engine.derivation_manager:
            derivation_analysis = rag_engine.derivation_manager.analyze_query(user_message)
            if derivation_analysis["requires_derivation"]:
                derivation_response = rag_engine.derivation_manager.generate_derivation_response(
                    derivation_analysis["derivation_area"], 
                    user_message
                )
                # Solo agregar derivaci√≥n si tenemos algo de informaci√≥n base
                if respuesta and len(respuesta) > 20:
                    respuesta += f"\n\n{derivation_response['response']}"
                # Si no hay respuesta √∫til, usar derivaci√≥n como fallback
                elif len(respuesta.strip()) < 20:
                    respuesta = derivation_response['response']

        formatted_sources = []
        for source in final_sources:
            formatted_sources.append({
                'content': source['document'][:80] + '...',
                'category': source['metadata'].get('category', 'general'),
                'similarity': round(source.get('similarity', 0.5), 3)
            })

        # üîç DIAGN√ìSTICO COMPLETO: Verificar calidad de informaci√≥n recuperada
        logger.info(f"")
        logger.info(f"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        logger.info(f"üìä DIAGN√ìSTICO COMPLETO RAG")
        logger.info(f"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        logger.info(f"üìù Query: '{user_message}'")
        logger.info(f"üîç Fuentes encontradas: {len(final_sources)}")
        logger.info(f"üìè Longitud respuesta: {len(respuesta)} caracteres")
        
        if final_sources:
            avg_similarity = sum(s.get('similarity', 0) for s in final_sources) / len(final_sources)
            logger.info(f"üìä Similitud promedio: {avg_similarity:.3f}")
            
            for i, src in enumerate(final_sources, 1):
                category = src.get('metadata', {}).get('category', 'unknown')
                similarity = src.get('similarity', 0)
                preview = src['document'][:100].replace('\n', ' ')
                logger.info(f"  üìÑ Fuente {i}: [{category}] sim={similarity:.3f}")
                logger.info(f"     '{preview}...'")
        else:
            logger.warning(f"‚ö†Ô∏è NO SE ENCONTRARON FUENTES en ChromaDB")
        
        logger.info(f"üí¨ Respuesta preview: '{respuesta[:200]}...'")
        logger.info(f"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        logger.info(f"")
        
        # AGREGAR GENERACI√ìN DE QR CODES PARA RESPUESTAS RAG (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(respuesta, user_message)

        # APLICAR MEJORAS A LA RESPUESTA ANTES DE RETORNAR
        category = processing_info.get('topic_classification', {}).get('category', 'general')
        
        # ‚úÖ MEJORA CR√çTICA: Aplicar enhancer correctamente
        if RESPONSE_ENHANCER_AVAILABLE and respuesta and len(respuesta.strip()) > 10:
            try:
                enhanced_respuesta = enhance_final_response(respuesta, user_message, category)
                logger.info(f"‚úÖ Response enhanced: {len(respuesta)} -> {len(enhanced_respuesta)} chars")
            except Exception as e:
                logger.error(f"‚ùå Error enhancing response: {e}")
                enhanced_respuesta = respuesta
        else:
            enhanced_respuesta = respuesta
            if not RESPONSE_ENHANCER_AVAILABLE:
                logger.warning("‚ö†Ô∏è Response enhancer not available")

        response_data = {
            'response': enhanced_respuesta,
            'sources': formatted_sources,
            'category': category,
            'timestamp': time.time(),
            'response_time': time.time() - start_time,
            'cache_type': 'ollama_generated',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],
            'has_qr': qr_processed_response['has_qr']
        }

        # üî• NO CACHEAR hasta que el sistema funcione correctamente
        # rag_engine.text_cache[cache_key] = response_data
        rag_engine.metrics['successful_responses'] += 1
        
        # üìä RESUMEN FINAL
        print(f"\n{'='*80}")
        print(f"‚úÖ CONSULTA COMPLETADA EXITOSAMENTE")
        print(f"{'='*80}")
        print(f"üìä RESUMEN:")
        print(f"   ‚Ä¢ Query: '{user_message}'")
        print(f"   ‚Ä¢ Estrategia: {strategy.upper()}")
        print(f"   ‚Ä¢ Fuentes usadas: {len(final_sources)}")
        print(f"   ‚Ä¢ Modelo: {rag_engine.current_model}")
        print(f"   ‚Ä¢ Tiempo total: {response_data['response_time']:.2f}s")
        print(f"   ‚Ä¢ Longitud respuesta: {len(enhanced_respuesta)} chars")
        if keyword_analysis.get('primary_keyword'):
            print(f"   ‚Ä¢ Keyword detectada: {keyword_analysis.get('primary_keyword')}")
        print(f"{'='*80}\n")
        
        logger.info(f"‚úÖ Respuesta generada exitosamente: {len(enhanced_respuesta)} chars")
        logger.info(f"‚è±Ô∏è Tiempo total: {response_data['response_time']:.2f}s")

        return response_data

    except Exception as e:
        print(f"\n{'='*80}")
        print(f"‚ùå ERROR GENERAL EN PROCESAMIENTO")
        print(f"{'='*80}")
        print(f"üî¥ Error: {str(e)[:200]}")
        print(f"üìù Query: '{user_message}'")
        print(f"üìö Fuentes disponibles: {len(final_sources) if 'final_sources' in locals() else 0}")
        print(f"{'='*80}\n")
        
        logger.error(f"‚ùå ERROR EN RAG EST√ÅNDAR: {str(e)}")
        logger.error(f"   Query: '{user_message[:100]}...'")
        logger.error(f"   Sources available: {len(final_sources) if 'final_sources' in locals() else 0}")
        import traceback
        logger.error(f"   Stack trace: {traceback.format_exc()[:500]}")
        
        # Fallback: si tenemos fuentes recuperadas, devolver su contenido bruto como respuesta
        try:
            if 'final_sources' in locals() and final_sources:
                fallback_texts = []
                formatted_sources = []
                for src in final_sources:
                    doc = src.get('document', '')
                    meta = src.get('metadata', {})
                    fallback_texts.append(doc[:800] + ('...' if len(doc) > 800 else ''))
                    formatted_sources.append({
                        'content': doc[:200] + ('...' if len(doc) > 200 else ''),
                        'category': meta.get('category', 'general'),
                        'source': meta.get('source', 'unknown'),
                        'similarity': round(src.get('similarity', 0.0), 3)
                    })

                fallback_response = '\n\n'.join(fallback_texts[:3])
                return {
                    'response': fallback_response or "Consulta en Punto Estudiantil para informaci√≥n espec√≠fica.",
                    'sources': formatted_sources,
                    'category': processing_info['topic_classification'].get('category', 'general'),
                    'timestamp': time.time(),
                    'response_time': time.time() - start_time,
                    'cache_type': 'fallback_documents',
                    'processing_info': processing_info
                }
        except Exception:
            # If fallback fails, return generic error
            logger.error('Fallback de documentos fall√≥ al generar respuesta')

        return {
            "response": "Error t√©cnico. Intenta nuevamente.",
            "sources": [],
            "category": "error",
            "response_time": time.time() - start_time,
            "processing_info": processing_info
        }


def _optimize_response(respuesta: str, pregunta: str) -> str:
    """OPTIMIZACI√ìN DE RESPUESTA MEJORADA"""
    if respuesta.startswith(("¬°Hola! Soy InA", "Hola, soy el asistente", "Hola, soy InA")):
        respuesta = re.sub(r'^¬°?Hola!?\s*(soy|me llamo)\s*(InA|el asistente)[^.!?]*[.!?]\s*', '', respuesta)
    
    optimizations = {
        "soy el asistente virtual del Punto Estudiantil": "",
        "estoy aqu√≠ para ayudarte con": "Puedo informarte sobre",
        "te recomiendo que te dirijas": "recomiendo dirigirte",
        "debes saber que el proceso": "el proceso",
        "es importante mencionar que": "",
        "en relaci√≥n a tu consulta sobre": "Sobre",
        "respecto a tu pregunta acerca de": "Acerca de",
        "quiero informarte que": "",
        "me complace decirte que": "",
        "como asistente virtual": "",
        "puedo proporcionarte informaci√≥n": "Informaci√≥n:",
        "hola, soy ina, el asistente virtual": "",
        "soy ina, el asistente virtual": "",
        "duoc uc": "Duoc UC",
    }

    for largo, corto in optimizations.items():
        respuesta = respuesta.replace(largo, corto)

    respuesta = re.sub(r'\s+', ' ', respuesta)
    respuesta = respuesta.strip()
    
    if len(respuesta) > 500:
        sentences = respuesta.split('.')
        if len(sentences) > 2:
            respuesta = '. '.join(sentences[:2]) + '.'
    
    return respuesta


class ResponseCache:
    def __init__(self):
        self.cache = {}
        self.ttl = timedelta(hours=1)

    def get_key(self, query: str) -> str:
        return hashlib.md5(query.encode()).hexdigest()

    def get(self, query: str):
        key = self.get_key(query)
        if key in self.cache:
            timestamp, response = self.cache[key]
            if datetime.now() - timestamp < self.ttl:
                return response
        return None

    def set(self, query: str, response: dict):
        key = self.get_key(query)
        self.cache[key] = (datetime.now(), response)


response_cache = ResponseCache()


def get_rag_cache_stats() -> Dict:
    """ESTAD√çSTICAS COMPLETAS"""
    return rag_engine.get_cache_stats()


def clear_caches():
    """LIMPIAR CACHES"""
    rag_engine.text_cache.clear()
    rag_engine.semantic_cache.cache.clear()
    logger.info("Todos los caches limpiados")
    
def get_standard_rag_response(self, question: str, context: List[str]) -> Dict:
    try:
        normalized_question = self.enhanced_normalize_text(question)
        sources = self.hybrid_search(normalized_question)
        return self._process_with_ollama_optimized(question, sources)
    except Exception as e:
        logger.error(f"Error RAG para '{question}': {e}")
        
        # FALLBACK INTELIGENTE POR CATEGOR√çA
        if "deportes" in question.lower():
            return self.templates.get("informacion_general_deportes", 
                                   "Informaci√≥n sobre deportes no disponible temporalmente")
        elif "desarrollo laboral" in question.lower():
            return self.templates.get("que_es_desarrollo_laboral",
                                   "Informaci√≥n sobre desarrollo laboral no disponible")
        else:
            return "Error t√©cnico. Intenta nuevamente o consulta informaci√≥n espec√≠fica."