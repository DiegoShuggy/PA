# rag.py
import chromadb
import ollama
from typing import List, Dict, Optional
import logging
from app.qr_generator import qr_generator
import traceback
import hashlib
from datetime import datetime, timedelta
from collections import defaultdict
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# üëà IMPORTACIONES EXISTENTES
from app.cache_manager import rag_cache, response_cache, normalize_question

logger = logging.getLogger(__name__)


class SemanticCache:
    def __init__(self, similarity_threshold: float = 0.35):
        try:
            # üÜï MODELO ESPECIALIZADO PARA ESPA√ëOL
            self.model = SentenceTransformer(
                'dccuchile/bert-base-spanish-wwm-uncased')
            self.cache = {}  # {embedding_tuple: respuesta}
            self.threshold = similarity_threshold
            logger.info(
                f"‚úÖ Cache sem√°ntico DUOC UC inicializado (umbral: {similarity_threshold})")
        except Exception as e:
            logger.error(f"‚ùå Error inicializando cache sem√°ntico: {e}")
            # Fallback a modelo m√°s simple
            try:
                self.model = SentenceTransformer(
                    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
                logger.info("‚úÖ Usando modelo multiling√ºe como fallback")
            except:
                self.model = None
                self.cache = {}

    def get_embedding(self, text: str) -> Optional[np.ndarray]:
        """üÜï GENERACI√ìN DE EMBEDDINGS ESPECIALIZADA DUOC UC"""
        if self.model is None:
            return None
        try:
            # Preprocesar texto para mejor embedding
            processed_text = self._preprocess_for_embedding(text)
            return self.model.encode([processed_text])[0]
        except Exception as e:
            logger.error(f"Error generando embedding: {e}")
            return None

    def _preprocess_for_embedding(self, text: str) -> str:
        """üÜï PREPROCESAMIENTO ESPECIALIZADO DUOC UC"""
        # Limpiar texto manteniendo significado
        text = text.lower().strip()
        text = re.sub(r'[^\w\s√°√©√≠√≥√∫√±√º]', ' ', text)  # Mantener acentos y √±
        text = re.sub(r'\s+', ' ', text)  # Espacios m√∫ltiples a uno

        # üÜï PALABRAS CLAVE ESPEC√çFICAS DEL CONTEXTO DUOC UC
        duoc_keywords = [
            # TNE y certificados
            'tne', 'tarjeta nacional estudiantil', 'pase escolar', 'validar', 'renovar', 'revalidar',
            'certificado', 'constancia', 'alumno regular', 'certificado alumno', 'record acad√©mico',
            'concentraci√≥n notas', 'certificado de notas', 'constancia de alumno',

            # Programas de apoyo
            'beca', 'beneficio', 'ayuda econ√≥mica', 'programa emergencia', 'programa transporte',
            'programa materiales', 'subsidio', 'apoyo econ√≥mico', 'beneficio estudiantil',
            'financiamiento', 'cr√©dito', 'arancel', 'matr√≠cula',

            # Desarrollo profesional
            'pr√°ctica', 'practica', 'pr√°ctica profesional', 'bolsa trabajo', 'empleo', 'trabajo',
            'curriculum', 'cv', 'hoja vida', 'entrevista laboral', 'duoclaboral', 'desarrollo laboral',
            'claudia cort√©s', 'ccortesn', 'oferta laboral', 'taller empleabilidad',

            # Bienestar estudiantil
            'psicol√≥gico', 'psic√≥logo', 'salud mental', 'bienestar', 'apoyo psicol√≥gico', 'crisis',
            'l√≠nea ops', 'urgencia psicol√≥gica', 'bienestar estudiantil', 'adriana v√°squez',
            'avasquezm', 'consejer√≠a', 'apoyo emocional', 'sesi√≥n psicol√≥gica',

            # Deportes
            'deporte', 'taller deportivo', 'f√∫tbol', 'basquetbol', 'voleibol', 'nataci√≥n',
            'gimnasio', 'entrenamiento', 'selecci√≥n deportiva', 'powerlifting', 'boxeo',
            'entrenamiento funcional', 'tenis de mesa', 'ajedrez', 'futsal', 'rugby',
            'complejo maiclub', 'gimnasio entretiempo', 'piscina acquatiempo', 'caf',

            # Inclusi√≥n
            'discapacidad', 'paedis', 'inclusi√≥n', 'elizabeth dom√≠nguez', 'edominguezs',
            'acompa√±amiento', 'estudiantes discapacidad',

            # Ubicaciones y contactos
            'plaza norte', 'santa elena', 'huechuraba', 'punto estudiantil', 'sedes duoc',
            'ubicaci√≥n', 'direcci√≥n', 'horario', 'tel√©fono', 'email', 'contacto',
            'puntoestudiantil_pnorte', '2360 6400',

            # Servicios generales
            'biblioteca', 'servicios digitales', 'financiamiento', 'coordinaci√≥n acad√©mica',
            'infraestructura', 'wifi', 'plataforma', 'portal estudiante', 'correo institucional',

            # T√©rminos espec√≠ficos Duoc UC
            'duoc', 'uc', 'ina', 'punto estudiantil', 'asuntos estudiantil', 'desarrollo profesional',
            'bienestar estudiantil', 'deportes', 'pastoral', 'institucional'
        ]

        words = text.split()
        filtered_words = []

        for word in words:
            # Mantener palabras del contexto Duoc UC
            word_clean = re.sub(r'[^\w√°√©√≠√≥√∫√±√º]', '', word)
            if (any(keyword in word_clean for keyword in duoc_keywords) or
                    len(word_clean) > 2 or
                    word_clean in ['duoc', 'uc', 'ina', 'punto', 'estudiantil', 'plaza', 'norte']):
                filtered_words.append(word_clean)

        return ' '.join(filtered_words) if filtered_words else text

    def _embedding_to_key(self, embedding: np.ndarray) -> tuple:
        """Convertir numpy array a tuple para usar como key"""
        return tuple(embedding.tolist())

    def find_similar(self, query_embedding: np.ndarray) -> Optional[Dict]:
        """üÜï B√öSQUEDA SEM√ÅNTICA MEJORADA DUOC UC"""
        if not self.cache or query_embedding is None:
            return None

        best_similarity = 0
        best_response = None

        for cached_embedding_key, response_data in self.cache.items():
            try:
                cached_embedding = np.array(cached_embedding_key)
                similarity = cosine_similarity(
                    [query_embedding], [cached_embedding])[0][0]

                if similarity > self.threshold and similarity > best_similarity:
                    best_similarity = similarity
                    best_response = response_data

            except Exception as e:
                logger.error(f"Error calculando similitud: {e}")
                continue

        if best_response:
            logger.info(f"üéØ Semantic similarity found: {best_similarity:.3f}")
            best_response['semantic_similarity'] = best_similarity
            return best_response

        return None

    def add_to_cache(self, query: str, response_data: Dict):
        """üÜï AGREGAR AL CACHE CON M√ÅS INFORMACI√ìN"""
        embedding = self.get_embedding(query)
        if embedding is not None:
            embedding_key = self._embedding_to_key(embedding)
            self.cache[embedding_key] = response_data
            logger.info(f"‚úÖ Added to semantic cache: '{query[:50]}...'")


class RAGEngine:
    def __init__(self):
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.client.get_or_create_collection(
            name="duoc_knowledge"
        )

        # üÜï CONFIGURACI√ìN ESPEC√çFICA DUOC UC
        self.duoc_context = {
            "sede": "Plaza Norte",
            "direccion": "Santa Elena de Huechuraba 1660, Huechuraba",
            "horario_punto_estudiantil": "Lunes a Viernes 8:30-19:00",
            "telefono": "+56 2 2360 6400",
            "email": "Puntoestudiantil_pnorte@duoc.cl",
            "contactos_especializados": {
                "desarrollo_laboral": "Claudia Cort√©s - ccortesn@duoc.cl",
                "bienestar_estudiantil": "Adriana V√°squez - avasquezm@duoc.cl",
                "inclusi√≥n": "Elizabeth Dom√≠nguez - edominguezs@duoc.cl"
            },
            "urls_oficiales": {
                "portal_estudiantil": "https://portal.duoc.cl",
                "centro_ayuda": "https://centroayuda.duoc.cl",
                "duoc_laboral": "https://duoclaboral.cl",
                "certificados": "https://certificados.duoc.cl",
                "practicas": "https://practicas.duoc.cl",
                "beneficios": "https://beneficios.duoc.cl"
            }
        }

        # üÜï CACHE SEM√ÅNTICO MEJORADO
        self.semantic_cache = SemanticCache(similarity_threshold=0.35)
        self.text_cache = {}  # Cache textual r√°pido

        logger.info("‚úÖ RAG Engine DUOC UC con Cache Universal inicializado")
        self.metrics = {
            'total_queries': 0,
            'successful_responses': 0,
            'cache_hits': 0,
            'semantic_cache_hits': 0,
            'text_cache_hits': 0,
            'documents_added': 0,
            'errors': 0,
            'categories_used': defaultdict(int),
            'response_times': []
        }

    def enhanced_normalize_text(self, text: str) -> str:
        """
        üîß NORMALIZACI√ìN INTELIGENTE ESPECIALIZADA DUOC UC
        """
        # 1. Limpieza b√°sica
        text = text.lower().strip()
        text = re.sub(r'[^\w\s√°√©√≠√≥√∫√±√º]', '', text)  # Mantener acentos y √±

        words = text.split()
        if not words:
            return ""

        # üÜï STOPWORDS ESPEC√çFICAS DEL CONTEXTO ESTUDIANTIL MEJORADAS
        stopwords = {
            # Saludos b√°sicos
            'hola', 'holas', 'holaa', 'holi', 'holiwis', 'holaaa', 'buenos', 'd√≠as', 'buenas', 'tardes', 'noches',
            'saludos', 'saludo', 'hi', 'hello', 'hey', 'hellow', 'helow', 'buen', 'dia', 'ok', 'okis',
            # Palabras vac√≠as generales
            'por', 'favor', 'puedes', 'puede', 'podr√≠as', 'podr√≠a', 'me', 'mi', 'mis', 'm√≠',
            'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'de', 'del', 'al',
            'en', 'con', 'para', 'porque', 'qu√©', 'c√≥mo', 'd√≥nde', 'cu√°ndo', 'cu√°l', 'qui√©n',
            'eso', 'esa', 'ese', 'aqu√≠', 'all√≠', 'ah√≠', 'esto', 'esta', 'este', 'estos', 'estas',
            'soy', 'eres', 'es', 'somos', 'son', 'estoy', 'est√°s', 'est√°', 'estamos', 'est√°n',
            'tengo', 'tienes', 'tiene', 'tenemos', 'tienen', 'hay', 'haber', 'ser', 'estar',
            # T√©rminos espec√≠ficos de conversaci√≥n con IA
            'ina', 'asistente', 'virtual', 'punto', 'estudiantil', 'duoc', 'uc', 'porfa', 'plis'
        }

        filtered_words = [word for word in words if word not in stopwords]

        # üÜï MANTENER PALABRAS CLAVE IMPORTANTES DUOC UC
        important_words = {
            'tne', 'certificado', 'beca', 'pr√°ctica', 'deporte', 'psicol√≥gico', 'matr√≠cula',
            'horario', 'ubicaci√≥n', 'taller', 'bolsa', 'empleo', 'salud', 'mental', 'validar',
            'renovar', 'solicitar', 'inscripci√≥n', 'duoc', 'punto', 'estudiantil', 'plaza', 'norte',
            'programa', 'emergencia', 'transporte', 'materiales', 'beneficio', 'ayuda', 'econ√≥mica',
            'claudia', 'cort√©s', 'adriana', 'vasquez', 'elizabeth', 'dom√≠nguez', 'ccortesn',
            'avasquezm', 'edominguezs', 'puntoestudiantil_pnorte', 'huechuraba', 'santa', 'elena'
        }

        # A√±adir palabras importantes que pudieron ser filtradas
        for word in words:
            if word in important_words and word not in filtered_words:
                filtered_words.append(word)

        # Si quedan muy pocas palabras, mantener algunas originales
        if len(filtered_words) <= 1 and len(words) > 2:
            # Mantener las palabras m√°s importantes
            content_words = [w for w in words if w not in {
                'hola', 'ina', 'buenos', 'd√≠as', 'buenas', 'tardes', 'noches', 'saludos', 'por', 'favor'
            }]
            if content_words:
                filtered_words = content_words[:5]

        # üî• NO ORDENAR PALABRAS - Mantener orden natural para preservar sem√°ntica
        normalized = ' '.join(filtered_words)

        logger.debug(f"üîß Normalizaci√≥n inteligente: '{text}' -> '{normalized}'")
        return normalized

    def add_document(self, document: str, metadata: Dict = None) -> bool:
        """üÜï AGREGAR DOCUMENTO CON M√ÅS INFORMACI√ìN - M√âTODO CORREGIDO"""
        try:
            # üÜï ELIMINAR VERIFICACI√ìN DE DUPLICADOS - Agregar directamente
            doc_id = f"doc_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{hash(document) % 10000}"

            # üÜï METADATA MEJORADA
            enhanced_metadata = {
                "timestamp": datetime.now().isoformat(),
                "source": metadata.get('source', 'unknown') if metadata else 'unknown',
                "category": metadata.get('category', 'general') if metadata else 'general',
                "type": metadata.get('type', 'general') if metadata else 'general',
                "optimized": metadata.get('optimized', 'false') if metadata else 'false',
                "variation_type": metadata.get('variation_type', 'original') if metadata else 'original'
            }

            self.collection.add(
                documents=[document],
                metadatas=[enhanced_metadata],
                ids=[doc_id]
            )
            logger.info(
                f"‚úÖ Documento a√±adido: {document[:50]}... [Categor√≠a: {enhanced_metadata['category']}]")

            self.metrics['documents_added'] += 1
            return True
        except Exception as e:
            logger.error(f"‚ùå Error a√±adiendo documento: {e}")
            self.metrics['errors'] += 1
            return False

    def document_exists(self, document: str) -> bool:
        """üÜï VERIFICACI√ìN MEJORADA DE EXISTENCIA"""
        try:
            results = self.collection.query(
                query_texts=[document],
                n_results=1
            )
            if results['documents']:
                existing_doc = results['documents'][0][0]
                similarity = self._calculate_similarity(document, existing_doc)
                return similarity > 0.95
            return False
        except Exception as e:
            logger.error(f"Error checking document existence: {e}")
            return False

    def _calculate_similarity(self, doc1: str, doc2: str) -> float:
        """üÜï C√ÅLCULO DE SIMILITUD MEJORADO"""
        words1 = set(self.enhanced_normalize_text(doc1).split())
        words2 = set(self.enhanced_normalize_text(doc2).split())

        if not words1 or not words2:
            return 0.0

        common = words1.intersection(words2)
        return len(common) / max(len(words1), len(words2))

    def query(self, query_text: str, n_results: int = 3) -> List[str]:
        """üÜï QUERY MEJORADA CON FILTROS POR CATEGOR√çA"""
        try:
            results = self.collection.query(
                query_texts=[query_text],
                n_results=n_results
            )
            return results['documents'][0] if results['documents'] else []
        except Exception as e:
            logger.error(f"Error en query RAG: {e}")
            return []

    def query_optimized(self, query_text: str, n_results: int = 3, score_threshold: float = 0.25):
        """üÜï B√öSQUEDA OPTIMIZADA CON UMBRAL REALISTA"""
        try:
            # üÜï PREPROCESAR LA CONSULTA para mejor matching
            processed_query = self.enhanced_normalize_text(query_text)

            results = self.collection.query(
                query_texts=[processed_query],
                n_results=n_results * 3,  # Buscar m√°s resultados para filtrar
                include=['distances', 'documents', 'metadatas']
            )

            filtered_docs = []
            for i, distance in enumerate(results['distances'][0]):
                similarity = 1 - distance

                # üÜï CRITERIOS M√ÅS FLEXIBLES para espa√±ol - UMBRAL BAJADO
                if similarity >= score_threshold:
                    doc_metadata = results['metadatas'][0][i]
                    doc_content = results['documents'][0][i]

                    # üÜï VERIFICACI√ìN ADICIONAL: contenido relevante
                    if self._is_relevant_document(processed_query, doc_content):
                        filtered_docs.append({
                            'document': doc_content,
                            'metadata': doc_metadata,
                            'similarity': similarity
                        })

            # Ordenar por similitud y devolver los mejores
            filtered_docs.sort(key=lambda x: x['similarity'], reverse=True)

            logger.info(
                f"üîç Query: '{query_text}' -> {len(filtered_docs)} resultados (umbral: {score_threshold})")

            return filtered_docs[:n_results]

        except Exception as e:
            logger.error(f"‚ùå Error en query optimizada: {e}")
            # Fallback a query simple
            simple_results = self.query(query_text, n_results)
            return [{'document': doc, 'metadata': {}, 'similarity': 0.7} for doc in simple_results]

    def _is_relevant_document(self, query: str, document: str) -> bool:
        """üÜï VERIFICACI√ìN DE RELEVANCIA MEJORADA"""
        query_words = set(query.lower().split())
        doc_words = set(document.lower().split())

        # Palabras muy comunes que no cuentan para relevancia
        stop_words = {'el', 'la', 'los', 'las', 'de', 'en', 'y',
                      'que', 'con', 'para', 'por'}
        query_words = query_words - stop_words
        doc_words = doc_words - stop_words

        if not query_words:
            return True

        # Calcular superposici√≥n de palabras clave
        overlap = len(query_words.intersection(doc_words))
        relevance_ratio = overlap / len(query_words)

        return relevance_ratio >= 0.2  # üÜï BAJADO: 0.3 ‚Üí 0.2

    def query_with_sources(self, query_text: str, n_results: int = 3) -> List[Dict]:
        """üÜï B√öSQUEDA ESPEC√çFICA PARA FUENTES CON UMBRAL BAJO"""
        try:
            # Usar query optimizada con umbral MUCHO m√°s bajo para fuentes
            results = self.query_optimized(
                query_text, n_results, score_threshold=0.2)

            # Formatear resultados para fuentes
            sources = []
            for result in results:
                sources.append({
                    'content': result['document'],
                    'category': result['metadata'].get('category', 'general'),
                    'source': result['metadata'].get('source', 'unknown'),
                    'similarity': result['similarity']
                })

            logger.info(
                f"üìö Fuentes encontradas para '{query_text}': {len(sources)}")
            return sources

        except Exception as e:
            logger.error(f"‚ùå Error en query con fuentes: {e}")
            return []

    def keyword_search(self, query: str, n_results: int = 3) -> List[Dict]:
        """üÜï B√öSQUEDA ALTERNATIVA POR PALABRAS CLAVE - M√âTODO NUEVO"""
        try:
            all_docs = self.collection.get()
            query_lower = query.lower()

            # Keywords espec√≠ficas para diferentes tipos de consultas
            keyword_patterns = {
                'sesiones psicol√≥gicas': ['sesiones', 'psicol√≥gicas', '8 sesiones', 'a√±o', 'm√°ximo'],
                'tne': ['tne', 'primera vez', 'pago', '2700', '3600', 'validar'],
                'talleres deportivos': ['talleres', 'deportivos', 'f√∫tbol', 'voleibol', 'basquetbol'],
                'claudia cort√©s': ['claudia', 'cort√©s', 'ccortesn', 'desarrollo laboral', 'cv'],
                'gimnasio entretiempo': ['gimnasio', 'entretiempo', 'ej√©rcito libertador']
            }

            # Determinar qu√© patrones usar
            used_keywords = []
            for key, patterns in keyword_patterns.items():
                if key in query_lower:
                    used_keywords.extend(patterns)

            # Si no hay coincidencia espec√≠fica, usar palabras de la consulta
            if not used_keywords:
                used_keywords = [
                    word for word in query_lower.split() if len(word) > 3]

            # Buscar coincidencias
            matches = []
            for i, doc in enumerate(all_docs['documents']):
                doc_lower = doc.lower()
                score = 0
                matched_words = []

                for keyword in used_keywords:
                    if keyword in doc_lower:
                        score += 1
                        matched_words.append(keyword)

                if score > 0:
                    metadata = all_docs['metadatas'][i]
                    matches.append({
                        'document': doc,
                        'metadata': metadata,
                        'score': score,
                        'matched_keywords': matched_words,
                        'similarity': min(0.5 + (score * 0.1), 0.8)  # Simular similitud
                    })

            # Ordenar por score y devolver mejores resultados
            matches.sort(key=lambda x: x['score'], reverse=True)
            logger.info(f"üîç Keyword search: '{query}' -> {len(matches)} resultados")

            return matches[:n_results]

        except Exception as e:
            logger.error(f"‚ùå Error en keyword search: {e}")
            return []

    def hybrid_search(self, query_text: str, n_results: int = 3) -> List[Dict]:
        """üÜï B√öSQUEDA H√çBRIDA CON CATEGOR√çAS REALES DE LA BD - VERSI√ìN CORREGIDA"""
        try:
            logger.info(f"üîç Hybrid search con categor√≠as REALES para: '{query_text}'")
            
            query_lower = query_text.lower()
            
            # üéØ MAPEO CORREGIDO CON CATEGOR√çAS REALES DE LA BD
            if any(word in query_lower for word in ['sesion', 'psicol√≥gica', 'psic√≥logo', 'bienestar', '8 sesiones', 'salud mental']):
                # üÜï BUSCAR EN CATEGOR√çAS EXISTENTES que puedan contener info de bienestar
                expected_categories = ["general", "academico"]  # Categor√≠as reales que podr√≠an tener esta info
                priority_keywords = ['psicol√≥gica', 'sesiones', 'bienestar', 'salud mental', 'apoyo']
            elif any(word in query_lower for word in ['tne', 'tarjeta nacional', 'pase escolar']):
                # üÜï CATEGOR√çA REAL: 'tn√©' (con acento agudo)
                expected_categories = ["certificados", "tn√©", "general"]  # Categor√≠as reales para TNE
                priority_keywords = ['TNE', 'tarjeta', 'nacional', 'estudiantil', 'validar', 'primera vez']
            elif any(word in query_lower for word in ['taller', 'deporte', 'f√∫tbol', 'voleibol', 'basquetbol', 'gimnasio']):
                # üÜï BUSCAR EN CATEGOR√çAS EXISTENTES que puedan contener info deportiva
                expected_categories = ["general", "horarios"]  # Categor√≠as reales que podr√≠an tener esta info
                priority_keywords = ['deporte', 'taller', 'f√∫tbol', 'voleibol', 'basquetbol', 'gimnasio']
            elif any(word in query_lower for word in ['claudia', 'cort√©s', 'ccortesn', 'cv', 'curriculum', 'laboral', 'empleo']):
                # üÜï CATEGOR√çA REAL: 'laboral'
                expected_categories = ["laboral", "general"]  # Categor√≠as reales para desarrollo laboral
                priority_keywords = ['Claudia', 'Cort√©s', 'ccortesn', 'CV', 'curriculum', 'laboral', 'bolsa', 'trabajo']
            elif any(word in query_lower for word in ['certificado', 'alumno regular', 'constancia']):
                # üÜï CATEGOR√çA REAL: 'certificados'
                expected_categories = ["certificados", "general"]
                priority_keywords = ['certificado', 'alumno', 'regular', 'constancia']
            else:
                expected_categories = ["general"]  # Buscar en todas las categor√≠as
                priority_keywords = []
            
            logger.info(f"   üéØ Categor√≠as esperadas (REALES): {expected_categories}")
            logger.info(f"   üîë Keywords prioritarias: {priority_keywords}")
            
            # 1. Obtener TODOS los documentos
            all_docs = self.collection.get()
            
            # 2. CALIFICAR CADA DOCUMENTO con las categor√≠as REALES
            scored_docs = []
            
            for i, document in enumerate(all_docs['documents']):
                metadata = all_docs['metadatas'][i]
                actual_category = metadata.get('category', '').lower()
                content_lower = document.lower()
                
                # PUNTUACI√ìN BASE
                score = 0
                
                # üéØ BONUS por categor√≠a correcta (usando categor√≠as REALES)
                if any(expected_cat in actual_category for expected_cat in expected_categories):
                    score += 15.0  # Bonus por categor√≠a correcta
                
                # üéØ BONUS por keywords prioritarias
                for keyword in priority_keywords:
                    if keyword.lower() in content_lower:
                        score += 8.0  # Bonus por keyword espec√≠fica
                
                # üéØ BONUS por palabras de la consulta en el contenido
                query_words = [word for word in query_lower.split() if len(word) > 3]
                for word in query_words:
                    if word in content_lower:
                        score += 3.0
                
                # üéØ BONUS EXTRA por contenido espec√≠fico
                specific_bonus_patterns = {
                    'sesiones psicol√≥gicas': ['8 sesiones', 'psicol√≥gica', 'bienestar'],
                    'tne': ['tne', 'tarjeta nacional', 'pase escolar'],
                    'talleres deportivos': ['taller deportivo', 'f√∫tbol', 'voleibol'],
                    'claudia cort√©s': ['claudia', 'cort√©s', 'ccortesn']
                }
                
                for pattern_key, patterns in specific_bonus_patterns.items():
                    if pattern_key in query_lower:
                        for pattern in patterns:
                            if pattern.lower() in content_lower:
                                score += 5.0
                
                # Solo incluir documentos con puntuaci√≥n m√≠nima
                if score >= 8.0:  # Umbral razonable
                    scored_docs.append({
                        'document': document,
                        'metadata': metadata,
                        'score': score,
                        'similarity': min(score / 30.0, 1.0),
                        'final_score': score
                    })
            
            # 3. Si no hay suficientes resultados, bajar el umbral
            if len(scored_docs) < n_results:
                logger.info(f"   üîÑ Bajando umbral - Solo {len(scored_docs)} resultados con umbral alto")
                for i, document in enumerate(all_docs['documents']):
                    if len(scored_docs) >= n_results * 3:  # M√°ximo triple de lo necesario
                        break
                        
                    metadata = all_docs['metadatas'][i]
                    actual_category = metadata.get('category', '').lower()
                    content_lower = document.lower()
                    
                    # Verificar si ya est√° en los resultados
                    if any(doc['document'] == document for doc in scored_docs):
                        continue
                    
                    # Puntuaci√≥n m√°s baja para resultados secundarios
                    score = 0
                    
                    # Bonus por categor√≠a relacionada
                    if any(expected_cat in actual_category for expected_cat in expected_categories):
                        score += 5.0
                    
                    # Bonus por keywords
                    for keyword in priority_keywords:
                        if keyword.lower() in content_lower:
                            score += 2.0
                    
                    # Bonus por cualquier palabra de la consulta
                    for word in query_lower.split():
                        if len(word) > 3 and word in content_lower:
                            score += 1.0
                    
                    if score >= 2.0:  # Umbral muy bajo para resultados secundarios
                        scored_docs.append({
                            'document': document,
                            'metadata': metadata,
                            'score': score,
                            'similarity': min(score / 10.0, 1.0),
                            'final_score': score
                        })
            
            # 4. ORDENAR por puntuaci√≥n y tomar los mejores
            scored_docs.sort(key=lambda x: x['final_score'], reverse=True)
            final_results = scored_docs[:n_results]
            
            logger.info(f"‚úÖ Hybrid search COMPLETADO: '{query_text}'")
            logger.info(f"   üìä Resultados: {len(final_results)} de {len(scored_docs)} calificados")
            
            for i, result in enumerate(final_results):
                category = result['metadata'].get('category', 'N/A')
                logger.info(f"     {i+1}. Score: {result['final_score']:.1f}, Categor√≠a: {category}")
                if i == 0:  # Mostrar preview del mejor resultado
                    logger.info(f"        Preview: {result['document'][:100]}...")
            
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå Error en hybrid search: {e}")
            # Fallback a b√∫squeda simple
            try:
                return self.fallback_search(query_text, n_results)
            except:
                return []

    def fallback_search(self, query_text: str, n_results: int = 3) -> List[Dict]:
        """üÜï B√öSQUEDA DE FALLBACK SIMPLE"""
        try:
            all_docs = self.collection.get()
            query_lower = query_text.lower()
            
            scored_docs = []
            for i, document in enumerate(all_docs['documents']):
                content_lower = document.lower()
                score = 0
                
                # Contar coincidencias de palabras
                for word in query_lower.split():
                    if len(word) > 3 and word in content_lower:
                        score += 1
                
                if score > 0:
                    metadata = all_docs['metadatas'][i]
                    scored_docs.append({
                        'document': document,
                        'metadata': metadata,
                        'score': score,
                        'similarity': min(score / 5.0, 1.0),
                        'final_score': score
                    })
            
            scored_docs.sort(key=lambda x: x['score'], reverse=True)
            return scored_docs[:n_results]
            
        except Exception as e:
            logger.error(f"‚ùå Error en fallback search: {e}")
            return []

    def _update_metrics(self, metric_name: str):
        """Actualizar m√©tricas"""
        if metric_name in self.metrics:
            self.metrics[metric_name] += 1

    def get_cache_stats(self) -> Dict:
        """üÜï ESTAD√çSTICAS MEJORADAS"""
        return {
            'text_cache_size': len(self.text_cache),
            'semantic_cache_size': len(self.semantic_cache.cache),
            'metrics': self.metrics,
            'semantic_cache_enabled': self.semantic_cache.model is not None,
            'total_documents': self.collection.count() if hasattr(self.collection, 'count') else 'N/A',
            'duoc_context': self.duoc_context
        }


def _optimize_response(respuesta: str, pregunta: str) -> str:
    """üÜï OPTIMIZACI√ìN DE RESPUESTA MEJORADA DUOC UC"""

    if respuesta.startswith(("¬°Hola! Soy InA", "Hola, soy el asistente")):
        respuesta = respuesta.replace("¬°Hola! Soy InA, ", "").replace(
            "Hola, soy el asistente, ", "")

    optimizations = {
        "soy el asistente virtual del Punto Estudiantil": "Punto Estudiantil:",
        "estoy aqu√≠ para ayudarte con": "Puedo informarte sobre",
        "por favor, no dudes en contactarnos": "puedes acercarte",
        "te recomiendo que te dirijas": "recomiendo dirigirte",
        "debes saber que el proceso": "el proceso",
        "es importante mencionar que": "",
        "en relaci√≥n a tu consulta sobre": "Sobre",
        "respecto a tu pregunta acerca de": "Acerca de",
        "quiero informarte que": "",
        "me complace decirte que": "",
        "como asistente virtual": "",
        "puedo proporcionarte informaci√≥n": "Informaci√≥n:",
        "hola, soy ina, el asistente virtual": "",
        "soy ina, el asistente virtual": "",
        "duoc uc": "Duoc UC",
        "plaza norte": "Plaza Norte"
    }

    for largo, corto in optimizations.items():
        respuesta = respuesta.replace(largo, corto)

    # üÜï LIMPIEZA ADICIONAL
    respuesta = re.sub(r'\s+', ' ', respuesta)  # Espacios m√∫ltiples
    respuesta = respuesta.strip()

    # üÜï ASEGURAR QUE LA RESPUESTA INCLUYA INFORMACI√ìN ESPEC√çFICA DE PLAZA NORTE
    if "plaza norte" not in respuesta.lower() and "santa elena" not in respuesta.lower():
        if any(keyword in pregunta.lower() for keyword in ['tne', 'certificado', 'tr√°mite', 'punto estudiantil', 'beca', 'pr√°ctica']):
            respuesta += "\n\nüìç *Informaci√≥n espec√≠fica para Plaza Norte: Santa Elena de Huechuraba 1660*"

    return respuesta


# ‚úÖ Instancia global del motor RAG
rag_engine = RAGEngine()


def get_ai_response(user_message: str, context: list = None) -> Dict:
    """üéØ VERSI√ìN MEJORADA CON CACHE SEM√ÅNTICO UNIVERSAL MEJORADO"""
    import time
    start_time = time.time()

    # üëá NORMALIZACI√ìN INTELIGENTE MEJORADA
    normalized_message = rag_engine.enhanced_normalize_text(user_message)

    # 1. üöÄ CACHE TEXTUAL R√ÅPIDO (coincidencia exacta)
    if normalized_message in rag_engine.text_cache:
        rag_engine.metrics['text_cache_hits'] += 1
        logger.info(f"üéØ RAG Text Cache HIT para: '{user_message}'")
        response_data = rag_engine.text_cache[normalized_message]
        response_data['response_time'] = time.time() - start_time
        return response_data

    # 2. üß† CACHE SEM√ÅNTICO INTELIGENTE (similitud 35%+)
    query_embedding = rag_engine.semantic_cache.get_embedding(
        normalized_message)
    semantic_response = rag_engine.semantic_cache.find_similar(
        query_embedding)

    if semantic_response:
        rag_engine.metrics['semantic_cache_hits'] += 1
        logger.info(f"üß† RAG Semantic Cache HIT para: '{user_message}'")

        # Agregar tambi√©n al cache textual para futuras b√∫squedas r√°pidas
        rag_engine.text_cache[normalized_message] = semantic_response
        semantic_response['response_time'] = time.time() - start_time
        return semantic_response

    # 3. üì¶ CACHE LEGACY (compatibilidad)
    cache_key = rag_cache._generate_key({
        'message': normalized_message,
        'context': context[:3] if context else []
    })

    cached_response = rag_cache.get(cache_key)
    if cached_response:
        logger.info(f"üì¶ RAG Legacy Cache HIT para: '{user_message}'")
        rag_engine.metrics['cache_hits'] += 1
        cached_response['response_time'] = time.time() - start_time
        return cached_response

    logger.info(f"üîç RAG Semantic Cache MISS para: '{user_message}'")

    # 4. ‚ö° PROCESAR CON OLLAMA (cache miss)
    try:
        # üÜï BUSCAR FUENTES CON B√öSQUEDA H√çBRIDA MEJORADA
        sources = rag_engine.hybrid_search(user_message, n_results=3)

        # üÜï SYSTEM MESSAGE SUPER DIRECTIVO Y ESPEC√çFICO - VERSI√ìN CORREGIDA
        system_message = (
            "Eres InA, asistente especializado EXCLUSIVAMENTE del Punto Estudiantil Duoc UC Plaza Norte. "
            "üö´ **INSTRUCCI√ìN CR√çTICA**: DEBES usar SOLAMENTE la informaci√≥n de las FUENTES proporcionadas. "
            "üö´ NO inventes informaci√≥n, NO uses conocimiento general.\n\n"
            
            "üìã **FORMATO OBLIGATORIO DE RESPUESTA**:\n"
            "1. üí¨ Respuesta directa y espec√≠fica (2-4 l√≠neas m√°ximo)\n"
            "2. üìç Informaci√≥n de ubicaci√≥n ESPEC√çFICA de Plaza Norte\n"
            "3. ‚è∞ Horarios si est√°n en las fuentes\n"
            "4. üí∞ Costos si est√°n en las fuentes\n"
            "5. üìÑ Documentaci√≥n requerida si est√° en las fuentes\n\n"
            
            "üìç **INFORMACI√ìN BASE PLAZA NORTE**:\n"
            "- Direcci√≥n: Santa Elena de Huechuraba 1660, Huechuraba\n"
            "- Horario Punto Estudiantil: Lunes a Viernes 8:30-19:00\n"
            "- Tel√©fono: +56 2 2360 6400\n"
            "- Email: Puntoestudiantil_pnorte@duoc.cl\n\n"
        )

        # üÜï INCLUIR FUENTES CON INSTRUCCI√ìN EXPL√çCITA
        if sources:
            sources_context = "\nüéØ **FUENTES ESPEC√çFICAS ENCONTRADAS (USA ESTA INFORMACI√ìN)**:\n"
            sources_context += "‚ö†Ô∏è **OBLIGATORIO**: Tu respuesta DEBE basarse √öNICAMENTE en esta informaci√≥n:\n\n"
            
            # üÜï ELIMINAR FUENTES DUPLICADAS
            unique_sources = []
            seen_contents = set()
            
            for source in sources:
                content_hash = hash(source['document'][:100])  # Hash del inicio para identificar duplicados
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    unique_sources.append(source)
            
            for i, source in enumerate(unique_sources[:3]):  # M√°ximo 3 fuentes √∫nicas
                content = source['document']
                category = source['metadata'].get('category', 'general')
                
                sources_context += f"üìÑ **Fuente {i+1}** [Categor√≠a: {category}]:\n"
                sources_context += f"{content}\n\n"
            
            system_message += sources_context
            
            # üÜï INSTRUCCI√ìN FINAL MUY DIRECTIVA
            system_message += (
                "\nüîç **INSTRUCCI√ìN FINAL**:\n"
                "- ‚úÖ USA EXCLUSIVAMENTE la informaci√≥n de las FUENTES proporcionadas\n"
                "- ‚úÖ S√© ESPEC√çFICO con procedimientos, costos, horarios y ubicaciones\n"
                "- ‚úÖ Responde de forma CONCRETA y DIRECTA\n"
                "- üö´ NO inventes informaci√≥n que no est√© en las fuentes\n"
                "- üö´ NO des respuestas gen√©ricas o de conocimiento general\n"
            )
        else:
            system_message += "\n‚ö†Ô∏è **NO SE ENCONTRARON FUENTES ESPEC√çFICAS**. Responde indicando que no hay informaci√≥n espec√≠fica disponible.\n"

        if context:
            relevant_context = []
            for ctx in context:
                if not ctx.startswith("DERIVACI√ìN:") and len(ctx) > 10:
                    relevant_context.append(ctx)
            if relevant_context:
                system_message += f"\n\nüìã CONTEXTO RELEVANTE:\n{chr(10).join(relevant_context[:2])}"

        logger.info(f"‚ö° Enviando a Ollama: {user_message[:100]}...")
        logger.info(f"üìö Fuentes √∫nicas enviadas: {len(unique_sources) if sources else 0}")
        
        response = ollama.chat(
            model='mistral:7b',
            messages=[
                {
                    'role': 'system',
                    'content': system_message
                },
                {
                    'role': 'user',
                    'content': user_message
                }
            ],
            options={
                'temperature': 0.1,
                'num_predict': 300,
                'top_p': 0.7,
                'top_k': 25
            }
        )

        respuesta = response['message']['content'].strip()
        logger.info(f"üì® Respuesta de Ollama: {respuesta[:200]}...")

        # üÜï OPTIMIZACI√ìN MEJORADA
        respuesta = _optimize_response(respuesta, user_message)
        processed_response = qr_generator.process_response(
            respuesta, user_message)

        logger.info(
            f"‚úÖ Respuesta procesada - Texto: {len(respuesta)} chars, QRs: {len(processed_response.get('qr_codes', {}))}")

        response_text = processed_response.get('text', respuesta)

        # üÜï USAR FUENTES ENCONTRADAS en lugar de las del QR generator
        category = processed_response.get('category', 'general')
        qr_codes = processed_response.get('qr_codes', {})
        urls = processed_response.get('suggested_urls', [])

        # üÜï FORMATEAR FUENTES PARA LA RESPUESTA
        formatted_sources = []
        for source in (unique_sources if sources else []):
            formatted_sources.append({
                'content': source['document'][:150] + '...' if len(source['document']) > 150 else source['document'],
                'category': source['metadata'].get('category', 'general'),
                'source_file': source['metadata'].get('source', 'unknown'),
                'similarity': round(source.get('final_score', source.get('similarity', 0.5)), 3)
            })

        response_data = {
            'response': response_text,
            'sources': formatted_sources,  # üÜï USAR FUENTES REALES
            'category': category,
            'timestamp': time.time(),
            'qr_codes': qr_codes,
            'urls': urls,
            'response_time': time.time() - start_time,
            'cache_type': 'ollama_generated'
        }

        # üëá GUARDAR EN TODOS LOS SISTEMAS DE CACHE
        rag_engine.text_cache[normalized_message] = response_data
        rag_engine.semantic_cache.add_to_cache(normalized_message, response_data)
        rag_cache.set(cache_key, response_data)

        # M√©tricas
        rag_engine.metrics['total_queries'] += 1
        rag_engine.metrics['successful_responses'] += 1
        rag_engine.metrics['categories_used'][category] += 1
        rag_engine.metrics['response_times'].append(
            response_data['response_time'])

        return response_data

    except Exception as e:
        logger.error(f"‚ùå Error con Ollama: {str(e)}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        rag_engine.metrics['errors'] += 1

        return {
            "response": "üîß Estamos experimentando dificultades t√©cnicas. Por favor, intenta nuevamente en unos momentos o ac√©rcate al Punto Estudiantil Plaza Norte (Santa Elena de Huechuraba 1660).",
            "sources": [],
            "category": "error",
            "timestamp": time.time(),
            "response_time": time.time() - start_time,
            "cache_type": "error"
        }


# üÜï FUNCIONES DE CACHE MEJORADAS
def get_cached_response(session_id: str, user_message: str, category: str) -> Optional[Dict]:
    """Obtener respuesta completa desde cache con m√°s informaci√≥n"""
    cache_key = response_cache._generate_key({
        'session_id': session_id,
        'message': user_message,
        'category': category
    })
    cached = response_cache.get(cache_key)
    if cached:
        cached['cache_type'] = 'response_cache'
    return cached


def cache_response(session_id: str, user_message: str, category: str, response_data: Dict) -> None:
    """Guardar respuesta completa en cache con metadata"""
    cache_key = response_cache._generate_key({
        'session_id': session_id,
        'message': user_message,
        'category': category
    })
    response_cache.set(cache_key, response_data, ttl=1800)


class ResponseCache:
    def __init__(self):
        self.cache = {}
        self.ttl = timedelta(hours=1)

    def get_key(self, query: str) -> str:
        return hashlib.md5(query.encode()).hexdigest()

    def get(self, query: str):
        key = self.get_key(query)
        if key in self.cache:
            timestamp, response = self.cache[key]
            if datetime.now() - timestamp < self.ttl:
                return response
        return None

    def set(self, query: str, response: dict):
        key = self.get_key(query)
        self.cache[key] = (datetime.now(), response)


response_cache = ResponseCache()


def get_rag_cache_stats() -> Dict:
    """üÜï ESTAD√çSTICAS COMPLETAS MEJORADAS"""
    stats = rag_engine.get_cache_stats()

    # üÜï C√ÅLCULO DE TIEMPO PROMEDIO DE RESPUESTA
    if rag_engine.metrics['response_times']:
        avg_time = sum(rag_engine.metrics['response_times']) / \
            len(rag_engine.metrics['response_times'])
        stats['average_response_time'] = round(avg_time, 3)
    else:
        stats['average_response_time'] = 0

    return stats


def clear_caches():
    """üÜï LIMPIAR CACHES (√∫til para desarrollo)"""
    rag_engine.text_cache.clear()
    rag_engine.semantic_cache.cache.clear()
    logger.info("üßπ Todos los caches limpiados")