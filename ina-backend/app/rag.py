# rag.py - VERSIÃ“N COMPLETA ACTUALIZADA CON QR CORREGIDO
import chromadb
import ollama
from typing import List, Dict, Optional
import logging
from app.qr_generator import qr_generator
import traceback
import hashlib
from datetime import datetime, timedelta
from collections import defaultdict
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# ğŸ‘ˆ IMPORTACIONES EXISTENTES
from app.cache_manager import rag_cache, response_cache, normalize_question
from app.topic_classifier import TopicClassifier
from app.classifier import classifier  # ğŸ†• IMPORTAR CLASIFICADOR

logger = logging.getLogger(__name__)


class SemanticCache:
    def __init__(self, similarity_threshold: float = 0.65):
        try:
            self.model = SentenceTransformer(
                'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            self.cache = {}
            self.threshold = similarity_threshold
            logger.info(f"âœ… Cache semÃ¡ntico inicializado (umbral: {similarity_threshold})")
        except Exception as e:
            logger.error(f"âŒ Error inicializando cache semÃ¡ntico: {e}")
            self.model = None
            self.cache = {}

    def get_embedding(self, text: str) -> Optional[np.ndarray]:
        if self.model is None:
            return None
        try:
            return self.model.encode([text])[0]
        except Exception as e:
            logger.error(f"Error generando embedding: {e}")
            return None

    def _embedding_to_key(self, embedding: np.ndarray) -> tuple:
        return tuple(embedding.tolist())

    def find_similar(self, query_embedding: np.ndarray) -> Optional[Dict]:
        if not self.cache or query_embedding is None:
            return None

        best_similarity = 0
        best_response = None

        for cached_embedding_key, response_data in self.cache.items():
            try:
                cached_embedding = np.array(cached_embedding_key)
                similarity = cosine_similarity(
                    [query_embedding], [cached_embedding])[0][0]

                if similarity > self.threshold and similarity > best_similarity:
                    best_similarity = similarity
                    best_response = response_data

            except Exception as e:
                logger.error(f"Error calculando similitud: {e}")
                continue

        if best_response:
            logger.info(f"ğŸ¯ Semantic similarity found: {best_similarity:.3f}")
            best_response['semantic_similarity'] = best_similarity
            return best_response

        return None

    def add_to_cache(self, query: str, response_data: Dict):
        embedding = self.get_embedding(query)
        if embedding is not None:
            embedding_key = self._embedding_to_key(embedding)
            self.cache[embedding_key] = response_data
            logger.info(f"âœ… Added to semantic cache: '{query[:50]}...'")


class EnhancedTopicClassifier:
    """ğŸ†• CLASIFICADOR MEJORADO CON DETECCIÃ“N INTELIGENTE"""
    
    def __init__(self):
        self.topic_classifier = TopicClassifier()
        
        # ğŸ¯ PALABRAS CLAVE CRÃTICAS PARA DETECCIÃ“N MEJORADA
        self.critical_keywords = {
            'tne': ['tne', 'tarjeta nacional estudiantil', 'pase escolar', 'validar tne', 'renovar tne'],
            'deporte': ['deporte', 'taller deportivo', 'gimnasio', 'entrenamiento', 'fÃºtbol', 'basquetbol'],
            'certificado': ['certificado', 'alumno regular', 'constancia', 'record acadÃ©mico'],
            'bienestar': ['psicolÃ³gico', 'salud mental', 'bienestar', 'crisis', 'urgencia'],
            'practicas': ['prÃ¡ctica', 'empleo', 'curriculum', 'entrevista', 'duoclaboral'],
            'contraseÃ±a': ['contraseÃ±a', 'password', 'mi duoc', 'plataforma', 'correo institucional']
        }

    def classify_topic(self, query: str) -> Dict:
        """ğŸ†• CLASIFICACIÃ“N MEJORADA"""
        return self.topic_classifier.classify_topic(query)

    def should_derive(self, query: str) -> bool:
        """ğŸ†• DETECCIÃ“N MEJORADA DE CONSULTAS PARA DERIVAR"""
        topic_info = self.classify_topic(query)
        
        # Consultas que SIEMPRE deben derivarse
        derivation_keywords = [
            'contraseÃ±a', 'password', 'mi duoc', 'plataforma', 'correo institucional',
            'wifi', 'acceso denegado', 'bloqueado', 'login', 'portal', 'olvidÃ© contraseÃ±a',
            'recuperar contraseÃ±a', 'no puedo entrar', 'error acceso'
        ]
        
        query_lower = query.lower()
        if any(keyword in query_lower for keyword in derivation_keywords):
            return True
        
        return not topic_info.get('is_institutional', True)

    def detect_multiple_queries(self, query: str) -> List[str]:
        """ğŸ†• DETECCIÃ“N INTELIGENTE MEJORADA DE CONSULTAS MÃšLTIPLES"""
        query_lower = query.lower().strip()
        
        # ğŸ¯ EVITAR DIVIDIR CONSULTAS DE DERIVACIÃ“N
        if self.should_derive(query):
            return [query]
        
        # ğŸ¯ PATRONES MÃS INTELIGENTES PARA DIVISIÃ“N
        split_patterns = [
            r'\s+y\s+',          # " y "
            r'\s+tambiÃ©n\s+',    # " tambiÃ©n "
            r'\s+ademÃ¡s\s+',     # " ademÃ¡s "
            r'\s+por otro lado\s+', # " por otro lado "
            r'\s+asimismo\s+',   # " asimismo "
            r',\s*',             # Comas
            r';\s*',             # Puntos y coma
        ]
        
        # Intentar dividir por patrones
        for pattern in split_patterns:
            parts = re.split(pattern, query_lower)
            if len(parts) > 1:
                # ğŸ¯ VERIFICAR QUE LAS PARTES TIENEN SENTIDO
                valid_parts = []
                for part in parts:
                    part_clean = part.strip()
                    # ğŸ¯ CRITERIOS MÃS FLEXIBLES
                    words = part_clean.split()
                    if (len(words) >= 2 or 
                        any(keyword in part_clean for keyword in ['tne', 'deporte', 'taller', 'gimnasio', 'certificado', 'psicolÃ³gico', 'prÃ¡ctica'])):
                        valid_parts.append(part_clean)
                
                if len(valid_parts) > 1:
                    logger.info(f"ğŸ¯ Consulta mÃºltiple detectada por patrÃ³n: {valid_parts}")
                    return valid_parts
        
        # ğŸ¯ DETECCIÃ“N POR PALABRAS CLAVE CONJUNTAS
        topic_combinations = [
            ('tne', 'deporte'), ('tne', 'taller'), ('certificado', 'deporte'),
            ('beca', 'deporte'), ('psicolÃ³gico', 'deporte'), ('tne', 'certificado'),
            ('deporte', 'taller'), ('prÃ¡ctica', 'deporte')
        ]
        
        for combo in topic_combinations:
            if combo[0] in query_lower and combo[1] in query_lower:
                # Extraer partes basadas en palabras clave con contexto
                parts = []
                for keyword in combo:
                    if keyword in query_lower:
                        # Buscar contexto alrededor de la palabra clave
                        keyword_pos = query_lower.find(keyword)
                        # Contexto mÃ¡s amplio para mejor comprensiÃ³n
                        start = max(0, keyword_pos - 30)
                        end = min(len(query_lower), keyword_pos + len(keyword) + 40)
                        context = query_lower[start:end].strip()
                        # Limpiar y validar
                        context = re.sub(r'^\W+', '', context)  # Remover puntuaciÃ³n inicial
                        if len(context.split()) >= 2:
                            parts.append(context)
                
                if len(parts) > 1:
                    logger.info(f"ğŸ¯ Combo detectado: {parts}")
                    return parts
        
        return [query]
    
    def get_derivation_suggestion(self, topic_type: str) -> str:
        """ğŸ†• SUGERENCIAS ESPECÃFICAS PARA DERIVACIÃ“N"""
        return self.topic_classifier.get_redirection_message(topic_type)


class RAGEngine:
    def __init__(self):
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.client.get_or_create_collection(
            name="duoc_knowledge"
        )

        # ğŸ†• CLASIFICADOR DE TEMAS MEJORADO
        self.topic_classifier = EnhancedTopicClassifier()

        # ğŸ†• CONFIGURACIÃ“N ESPECÃFICA DUOC UC
        self.duoc_context = {
            "sede": "Plaza Norte",
            "direccion": "Santa Elena de Huechuraba 1660, Huechuraba",
            "horario_punto_estudiantil": "Lunes a Viernes 8:30-19:00",
            "telefono": "+56 2 2360 6400",
            "email": "Puntoestudiantil_pnorte@duoc.cl"
        }

        # ğŸ†• CACHE SEMÃNTICO MEJORADO
        self.semantic_cache = SemanticCache(similarity_threshold=0.65)
        self.text_cache = {}

        logger.info("âœ… RAG Engine DUOC UC inicializado")
        self.metrics = {
            'total_queries': 0,
            'successful_responses': 0,
            'cache_hits': 0,
            'semantic_cache_hits': 0,
            'text_cache_hits': 0,
            'documents_added': 0,
            'errors': 0,
            'categories_used': defaultdict(int),
            'response_times': [],
            'derivations': 0,
            'multiple_queries': 0,
            'ambiguous_queries': 0,
            'greetings': 0,
            'emergencies': 0,
            'template_responses': 0  # ğŸ†• MÃ‰TRICA PARA TEMPLATES
        }

    def enhanced_normalize_text(self, text: str) -> str:
        """ğŸ†• NORMALIZACIÃ“N SUPER MEJORADA PARA DUOC UC"""
        text = text.lower().strip()
        
        # ğŸ¯ EXPANDIR SINÃ“NIMOS Y VARIANTES ESPECÃFICAS DUOC
        synonym_expansions = {
            'tne': ['tarjeta nacional estudiantil', 'pase escolar', 'tne duoc', 'beneficio tne'],
            'deporte': ['deportes', 'actividad fÃ­sica', 'entrenamiento', 'ejercicio', 'taller deportivo'],
            'taller': ['talleres', 'clase', 'actividad deportiva', 'entrenamiento grupal'],
            'gimnasio': ['gimnasio duoc', 'complejo deportivo', 'instalaciones deportivas', 'maiclub'],
            'certificado': ['certificados', 'constancia', 'documento oficial', 'record acadÃ©mico'],
            'psicolÃ³gico': ['psicÃ³logo', 'salud mental', 'bienestar', 'apoyo emocional', 'consejerÃ­a'],
            'beca': ['becas', 'ayuda econÃ³mica', 'beneficio estudiantil', 'subsidio'],
            'prÃ¡ctica': ['practica profesional', 'empleo', 'trabajo', 'duoclaboral', 'bolsa trabajo'],
            'contraseÃ±a': ['password', 'acceso', 'login', 'plataforma', 'mi duoc'],
        }
        
        # Aplicar expansiones
        expanded_terms = []
        for base, variants in synonym_expansions.items():
            if base in text:
                expanded_terms.extend(variants)
        
        if expanded_terms:
            text += " " + " ".join(expanded_terms)
    
        # ğŸ¯ PATRONES ESPECÃFICOS DUOC
        duoc_patterns = {
            r'plaza norte': 'sede plaza norte ubicaciÃ³n',
            r'mi duoc': 'plataforma mi duoc portal duoc acceso digital',
            r'punto estudiantil': 'punto estudiantil duoc uc atenciÃ³n estudiante',
            r'claudia cortÃ©s': 'desarrollo laboral claudia cortes empleabilidad',
            r'elizabeth domÃ­nguez': 'inclusiÃ³n paedis elizabeth dominguez discapacidad',
            r'adriana vÃ¡squez': 'bienestar estudiantil adriana vasquez salud mental',
            r'complejo maiclub': 'complejo deportivo maiclub gimnasio instalaciones',
            r'gimnasio entretiempo': 'gimnasio entretiempo centro acondicionamiento fÃ­sico',
        }
        
        for pattern, replacement in duoc_patterns.items():
            text = re.sub(pattern, replacement, text)
        
        # Limpieza final
        text = re.sub(r'[^\w\sÃ¡Ã©Ã­Ã³ÃºÃ±Ã¼]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()

    def process_user_query(self, user_message: str) -> Dict:
        """ğŸ†• PROCESAMIENTO INTELIGENTE MEJORADO CON TEMPLATES"""
        self.metrics['total_queries'] += 1
        
        query_lower = user_message.lower().strip()
        
        # ğŸ†• 1. PRIMERO VERIFICAR TEMPLATES (MÃS RÃPIDO)
        template_match = classifier.detect_template_match(user_message)
        if template_match:
            logger.info(f"ğŸš€ TEMPLATE DETECTADO: '{user_message}' -> {template_match}")
            return {
                'processing_strategy': 'template',
                'original_query': user_message,
                'template_id': template_match,
                'query_parts': [user_message]
            }
        
        # ğŸ†• 2. DETECCIÃ“N PRIORITARIA DE SALUDOS
        greeting_keywords = [
            'hola', 'holi', 'holis', 'holaa', 'buenos dÃ­as', 'buenas tardes', 
            'buenas noches', 'saludos', 'quiÃ©n eres', 'presentate', 'presentaciÃ³n',
            'quÃ© eres', 'tu nombre', 'hola ina', 'hola inÃ¡', 'ina hola'
        ]
        
        if any(greeting in query_lower for greeting in greeting_keywords):
            logger.info(f"ğŸ‘‹ SALUDO DETECTADO: {user_message}")
            self.metrics['greetings'] += 1
            return {
                'processing_strategy': 'greeting',
                'original_query': user_message,
                'topic_classification': {'topic': 'greeting', 'type': 'allowed', 'confidence': 0.95},
                'is_greeting': True,
                'query_parts': [user_message]
            }
        
        # ğŸ†• 3. DETECCIÃ“N PRIORITARIA DE URGENCIAS/CRISIS
        emergency_keywords = [
            'crisis', 'urgencia', 'emergencia', 'lÃ­nea ops', 
            'me siento mal', 'ayuda urgente', 'necesito ayuda ahora',
            'estoy desesperado', 'no puedo mÃ¡s', 'pensamientos suicidas',
            'ataque de pÃ¡nico', 'ansiedad extrema', 'angustia severa'
        ]
        
        if any(keyword in query_lower for keyword in emergency_keywords):
            logger.warning(f"ğŸš¨ URGENCIA DETECTADA: {user_message}")
            self.metrics['emergencies'] += 1
            return {
                'processing_strategy': 'emergency',
                'original_query': user_message,
                'topic_classification': {
                    'topic': 'bienestar_estudiantil', 
                    'type': 'allowed',
                    'confidence': 0.95
                },
                'is_emergency': True,
                'query_parts': [user_message]
            }
        
        # ğŸ†• 4. VERIFICAR SI ES DERIVACIÃ“N
        if self.topic_classifier.should_derive(user_message):
            topic_info = self.topic_classifier.classify_topic(user_message)
            logger.info(f"ğŸ¯ DERIVACIÃ“N DETECTADA: {user_message} -> {topic_info.get('category', 'unknown')}")
            self.metrics['derivations'] += 1
            return {
                'processing_strategy': 'derivation',
                'original_query': user_message,
                'topic_classification': topic_info,
                'derivation_suggestion': self.topic_classifier.get_derivation_suggestion(topic_info.get('category', 'unknown')),
                'multiple_queries_detected': False,
                'query_parts': [user_message]
            }
        
        # 5. Clasificar tema
        topic_info = self.topic_classifier.classify_topic(user_message)
        
        # 6. Detectar consultas mÃºltiples SOLO para temas institucionales
        query_parts = self.topic_classifier.detect_multiple_queries(user_message)
        
        response_info = {
            'original_query': user_message,
            'topic_classification': topic_info,
            'multiple_queries_detected': len(query_parts) > 1,
            'query_parts': query_parts,
            'processing_strategy': 'standard'
        }
        
        # ğŸ¯ ESTRATEGIAS DIFERENCIADAS MEJORADAS
        if topic_info.get('category') == 'unknown':
            response_info['processing_strategy'] = 'clarification'
            self.metrics['ambiguous_queries'] += 1
            
        elif len(query_parts) > 1:
            response_info['processing_strategy'] = 'multiple_queries'
            self.metrics['multiple_queries'] += 1
            
        else:
            response_info['processing_strategy'] = 'standard_rag'
            
        logger.info(f"ğŸ¯ Procesamiento: '{user_message}' -> Estrategia: {response_info['processing_strategy']}")
        
        return response_info

    def generate_template_response(self, processing_info: Dict) -> Dict:
        """ğŸ†• GENERAR RESPUESTA DESDE TEMPLATE CON QR CODES CORREGIDO"""
        import time
        start_time = time.time()
        
        template_id = processing_info['template_id']
        
        # ğŸ¯ CARGAR TEMPLATES
        try:
            from app.templates import TEMPLATES
            
            # Buscar template en todas las categorÃ­as
            template_response = None
            template_category = None
            
            for category, templates in TEMPLATES.items():
                if template_id in templates:
                    template_response = templates[template_id]
                    template_category = category
                    break
            
            if template_response:
                # ğŸ”¥ AGREGAR GENERACIÃ“N DE QR CODES PARA TEMPLATES (ESTRUCTURA CORREGIDA)
                original_query = processing_info['original_query']
                qr_processed_response = qr_generator.process_response(template_response, original_query)
                
                response_time = time.time() - start_time
                self.metrics['template_responses'] += 1
                self.metrics['categories_used'][template_category] += 1
                
                logger.info(f"âš¡ TEMPLATE RESPONSE: {template_id} en {response_time:.3f}s")
                if qr_processed_response['has_qr']:
                    logger.info(f"ğŸ“± QR generados desde template: {qr_processed_response['total_qr_generated']} cÃ³digos")
                
                # ğŸ‘ˆ ESTRUCTURA CORREGIDA - qr_codes como dict simple
                return {
                    'response': template_response.strip(),
                    'sources': [],
                    'category': template_category,
                    'response_time': response_time,
                    'cache_type': 'template',
                    'processing_info': processing_info,
                    'template_used': template_id,
                    'qr_codes': qr_processed_response['qr_codes'],  # ğŸ‘ˆ Dict simple {url: qr_image}
                    'has_qr': qr_processed_response['has_qr']       # ğŸ‘ˆ Boolean
                }
            else:
                logger.warning(f"âš ï¸ Template no encontrado: {template_id}")
                
        except ImportError:
            logger.error("âŒ No se pudo importar templates.py")
        except Exception as e:
            logger.error(f"âŒ Error cargando template: {e}")
        
        # Fallback si no se encuentra el template
        return self.generate_clarification_response(processing_info)

    def generate_greeting_response(self, processing_info: Dict) -> Dict:
        """ğŸ†• RESPUESTA CORTA Y AMIGABLE PARA SALUDOS CON QR"""
        import random
        import time
        start_time = time.time()
        
        greeting_options = [
            "Â¡Hola! ğŸ‘‹ Soy InA, tu asistente del Punto Estudiantil Duoc UC. Â¿En quÃ© puedo ayudarte hoy?",
            "Â¡Hola! ğŸ˜Š Soy InA, estoy aquÃ­ para ayudarte con informaciÃ³n del Punto Estudiantil.",
            "Â¡Hola! ğŸ“ Soy InA, tu asistente de Duoc UC. Â¿QuÃ© necesitas saber?",
            "Â¡Hola! ğŸ’« Soy InA, del Punto Estudiantil. Â¿En quÃ© te puedo ayudar?",
        ]
        
        greeting = random.choice(greeting_options)
        
        # ğŸ†• SUGERENCIAS DE CONSULTAS COMUNES
        suggestions = """
        
ğŸ’¡ *Puedo ayudarte con:*
â€¢ ğŸ“ TNE, certificados, programas de apoyo
â€¢ ğŸ§  Salud mental, bienestar estudiantil  
â€¢ ğŸ€ Deportes, talleres, gimnasio
â€¢ ğŸ’¼ CV, prÃ¡cticas, empleabilidad

*Â¿QuÃ© necesitas?* ğŸ™‚
"""
        
        response = greeting + suggestions
        
        # ğŸ”¥ AGREGAR QR CODES PARA GREETING (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(response, processing_info['original_query'])
        
        # ğŸ‘ˆ ESTRUCTURA CORREGIDA
        return {
            'response': response.strip(),
            'sources': [],
            'category': 'greeting',
            'response_time': time.time() - start_time,
            'cache_type': 'greeting',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # ğŸ‘ˆ Dict simple
            'has_qr': qr_processed_response['has_qr']       # ğŸ‘ˆ Boolean
        }

    def generate_emergency_response(self, processing_info: Dict) -> Dict:
        """ğŸ†• RESPUESTA DE EMERGENCIA PRIORITARIA CON QR"""
        import time
        start_time = time.time()
        
        response = """
ğŸš¨ **URGENCIA - APOYO INMEDIATO DISPONIBLE**

ğŸ“ *LÃ­neas de ayuda 24/7:*
â€¢ **LÃ­nea OPS Duoc UC**: +56 2 2820 3450
â€¢ **Salud Responde**: 600 360 7777
â€¢ **Fono Mayor**: 800 4000 35

ğŸ¥ *AtenciÃ³n en sede:*
â€¢ **Sala primeros auxilios**: Primer piso, junto a caja
â€¢ **TelÃ©fono interno**: +56 2 2999 3005

ğŸ’™ *Recuerda: No estÃ¡s solo/a - hay ayuda disponible*

âš ï¸ *Si es emergencia mÃ©dica vital, llama al 131*
"""
        
        # ğŸ”¥ AGREGAR QR CODES PARA EMERGENCIA (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(response, processing_info['original_query'])
        
        # ğŸ‘ˆ ESTRUCTURA CORREGIDA
        return {
            'response': response.strip(),
            'sources': [],
            'category': 'emergency',
            'response_time': time.time() - start_time,
            'cache_type': 'emergency',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # ğŸ‘ˆ Dict simple
            'has_qr': qr_processed_response['has_qr']       # ğŸ‘ˆ Boolean
        }

    def generate_derivation_response(self, processing_info: Dict) -> Dict:
        """ğŸ†• DERIVACIÃ“N MEJORADA CON INFORMACIÃ“N ESPECÃFICA Y QR"""
        import time
        start_time = time.time()
        
        suggestion = processing_info.get('derivation_suggestion', 
            "ğŸ” **Consulta especializada**\n\n"
            "Te recomiendo acercarte a Punto Estudiantil para derivaciÃ³n al Ã¡rea correspondiente.\n\n"
            "ğŸ“ Santa Elena de Huechuraba 1660\n"
            "ğŸ“ +56 2 2360 6400\n"
            "â° L-V 8:30-19:00"
        )
        
        response = f"""
{suggestion}

ğŸ’¡ *Â¿Puedo ayudarte con TNE, bienestar, deportes o desarrollo laboral?*
"""
        
        # ğŸ”¥ AGREGAR QR CODES PARA DERIVACIÃ“N (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(response, processing_info['original_query'])
        
        # ğŸ‘ˆ ESTRUCTURA CORREGIDA
        return {
            'response': response.strip(),
            'sources': [],
            'category': 'derivation',
            'response_time': time.time() - start_time,
            'cache_type': 'derivation',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # ğŸ‘ˆ Dict simple
            'has_qr': qr_processed_response['has_qr']       # ğŸ‘ˆ Boolean
        }

    def generate_multiple_queries_response(self, processing_info: Dict) -> Dict:
        """ğŸ†• RESPUESTA OPTIMIZADA PARA CONSULTAS MÃšLTIPLES CON QR"""
        import time
        start_time = time.time()
        
        query_parts = processing_info['query_parts']
        original_query = processing_info['original_query']
        
        logger.info(f"ğŸ” Procesando {len(query_parts)} consultas mÃºltiples: {query_parts}")
        
        # ğŸ¯ ESTRATEGIA MEJORADA
        detailed_responses = []
        all_sources = []
        
        for i, part in enumerate(query_parts):
            logger.info(f"  ğŸ“ Procesando parte {i+1}: '{part}'")
            
            # ğŸ†• BUSCAR CON TÃ‰RMINOS EXPANDIDOS
            expanded_query = self._expand_query_with_context(part, original_query)
            sources = self.hybrid_search(expanded_query, n_results=2)
            
            if sources:
                part_response = self._process_with_ollama_optimized(expanded_query, sources)
                response_text = part_response['response']
                
                # ğŸ¯ MEJORAR CALIDAD DE RESPUESTA
                if "no hay informaciÃ³n" in response_text.lower() or "consulta en punto estudiantil" in response_text.lower():
                    # Intentar con bÃºsqueda mÃ¡s amplia
                    broader_sources = self.hybrid_search(part, n_results=3)
                    if broader_sources:
                        part_response = self._process_with_ollama_optimized(part, broader_sources)
                
                detailed_responses.append(f"**{i+1}. {part}:**\n{part_response['response']}")
                all_sources.extend(part_response['sources'])
            else:
                # ğŸ†• RESPUESTA MÃS ÃšTIL CON INFORMACIÃ“N GENÃ‰RICA
                generic_info = self._get_generic_topic_info(part)
                detailed_responses.append(f"**{i+1}. {part}:**\n{generic_info}")
        
        # ğŸ¯ CONSTRUIR RESPUESTA MÃS COHERENTE
        if detailed_responses:
            response = "ğŸ“‹ **Varias consultas detectadas:**\n\n" + "\n\n".join(detailed_responses)
            response += "\n\nğŸ’¡ *Â¿Necesitas mÃ¡s detalles de alguna consulta?*"
        else:
            response = "ğŸ¤” No pude procesar todas las consultas. Â¿PodrÃ­as reformularlas por separado?"
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… Consultas mÃºltiples procesadas en {processing_time:.2f}s")
        
        # ğŸ”¥ AGREGAR QR CODES PARA MÃšLTIPLES CONSULTAS (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(response, original_query)
        
        # ğŸ‘ˆ ESTRUCTURA CORREGIDA
        return {
            'response': response,
            'sources': all_sources[:3],
            'category': 'multiple_queries',
            'response_time': processing_time,
            'cache_type': 'multiple_queries',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # ğŸ‘ˆ Dict simple
            'has_qr': qr_processed_response['has_qr']       # ğŸ‘ˆ Boolean
        }

    def _expand_query_with_context(self, partial_query: str, full_query: str) -> str:
        """ğŸ†• EXPANDIR CONSULTA PARCIAL CON CONTEXTO COMPLETO"""
        important_keywords = ['tne', 'deporte', 'taller', 'certificado', 'beca', 'psicolÃ³gico', 'prÃ¡ctica']
        
        expanded = partial_query
        
        for keyword in important_keywords:
            if keyword in full_query and keyword not in partial_query:
                expanded += f" {keyword}"
        
        return expanded

    def _get_generic_topic_info(self, query: str) -> str:
        """ğŸ†• INFORMACIÃ“N GENÃ‰RICA POR TEMA CUANDO NO HAY FUENTES"""
        query_lower = query.lower()
        
        generic_responses = {
            'tne': "â„¹ï¸ **TNE**: Para trÃ¡mites de Tarjeta Nacional Estudiantil, acude a Punto Estudiantil con tu cÃ©dula de identidad. Horario: L-V 8:30-19:00",
            'deporte': "ğŸ€ **Deportes**: Duoc UC ofrece talleres deportivos, gimnasio y selecciones. InformaciÃ³n en Complejo Deportivo Maiclub.",
            'taller': "ğŸ¯ **Talleres**: Hay talleres deportivos, culturales y de desarrollo. Consulta programaciÃ³n en Punto Estudiantil.",
            'certificado': "ğŸ“„ **Certificados**: Solicita certificados de alumno regular en Punto Estudiantil o portal Mi Duoc.",
            'gimnasio': "ğŸ’ª **Gimnasio**: El Complejo Deportivo Maiclub tiene gimnasio, piscina y canchas. Horario: L-V 8:00-21:00.",
            'psicolÃ³gico': "ğŸ§  **Apoyo PsicolÃ³gico**: Sesiones de apoyo psicolÃ³gico disponibles. Contacta a Bienestar Estudiantil.",
            'prÃ¡ctica': "ğŸ’¼ **PrÃ¡cticas**: AsesorÃ­a para prÃ¡cticas profesionales con Claudia CortÃ©s. Desarrollo Laboral, edificio central.",
        }
        
        for topic, response in generic_responses.items():
            if topic in query_lower:
                return response
        
        return "â„¹ï¸ Consulta en Punto Estudiantil para informaciÃ³n especÃ­fica sobre este tema."

    def _process_with_ollama_optimized(self, query: str, sources: List[Dict]) -> Dict:
        """ğŸ†• VERSIÃ“N OPTIMIZADA PARA EQUIPO FINAL"""
        try:
            limited_sources = sources[:2]
            
            if not limited_sources:
                return {
                    'response': "Consulta en Punto Estudiantil para mÃ¡s informaciÃ³n.",
                    'sources': []
                }
            
            context_parts = []
            for i, source in enumerate(limited_sources):
                content = source['document']
                short_content = content[:150] + "..." if len(content) > 150 else content
                context_parts.append(f"Fuente {i+1}: {short_content}")
            
            context = "\n".join(context_parts)
            
            system_message = (
                "Eres InA, asistente del Punto Estudiantil Duoc UC. "
                f"Responde BREVE y ÃšTIL con esta informaciÃ³n: {context}\n\n"
                "INSTRUCCIONES:\n- MÃ¡ximo 3 lÃ­neas\n- SÃ© especÃ­fico\n- Si no hay info suficiente, di 'Consulta en Punto Estudiantil'"
            )
            
            response = ollama.chat(
                model='mistral:7b',
                messages=[
                    {'role': 'system', 'content': system_message},
                    {'role': 'user', 'content': query}
                ],
                options={'temperature': 0.1, 'num_predict': 80}
            )
            
            return {
                'response': response['message']['content'].strip(),
                'sources': [{
                    'content': source['document'][:80] + '...',
                    'category': source['metadata'].get('category', 'general'),
                    'similarity': round(source.get('similarity', 0.5), 3)
                } for source in limited_sources]
            }
            
        except Exception as e:
            logger.error(f"âŒ Error procesando con Ollama: {e}")
            if sources:
                short_response = sources[0]['document'][:100] + "..." if len(sources[0]['document']) > 100 else sources[0]['document']
                return {
                    'response': short_response,
                    'sources': []
                }
            else:
                return {
                    'response': "Consulta en Punto Estudiantil para informaciÃ³n especÃ­fica.",
                    'sources': []
                }

    def generate_clarification_response(self, processing_info: Dict) -> Dict:
        """GENERAR RESPUESTA PARA CONSULTAS AMBIGUAS CON QR"""
        import time
        start_time = time.time()
        
        original_query = processing_info['original_query']
        
        response = f"""
ğŸ¤” No entiendo completamente '{original_query}'.

ğŸ’¡ *Â¿Te refieres a alguno de estos temas?*

â€¢ TNE y certificados
â€¢ Programas de apoyo econÃ³mico  
â€¢ Salud mental y bienestar
â€¢ Deportes y actividades
â€¢ Desarrollo laboral y CV

*Ejemplo: "Â¿CÃ³mo saco mi TNE?"*
"""
        
        # ğŸ”¥ AGREGAR QR CODES PARA CLARIFICATION (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(response, original_query)
        
        # ğŸ‘ˆ ESTRUCTURA CORREGIDA
        return {
            'response': response.strip(),
            'sources': [],
            'category': 'clarification',
            'response_time': time.time() - start_time,
            'cache_type': 'clarification',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # ğŸ‘ˆ Dict simple
            'has_qr': qr_processed_response['has_qr']       # ğŸ‘ˆ Boolean
        }

    def add_document(self, document: str, metadata: Dict = None) -> bool:
        """AGREGAR DOCUMENTO AL RAG"""
        try:
            doc_id = f"doc_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}_{hash(document) % 10000}"

            enhanced_metadata = {
                "timestamp": datetime.now().isoformat(),
                "source": metadata.get('source', 'unknown') if metadata else 'unknown',
                "category": metadata.get('category', 'general') if metadata else 'general',
                "type": metadata.get('type', 'general') if metadata else 'general'
            }

            self.collection.add(
                documents=[document],
                metadatas=[enhanced_metadata],
                ids=[doc_id]
            )
            
            self.metrics['documents_added'] += 1
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error aÃ±adiendo documento: {e}")
            self.metrics['errors'] += 1
            return False

    def query(self, query_text: str, n_results: int = 3) -> List[str]:
        """QUERY BÃSICA"""
        try:
            results = self.collection.query(
                query_texts=[query_text],
                n_results=n_results
            )
            return results['documents'][0] if results['documents'] else []
        except Exception as e:
            logger.error(f"Error en query RAG: {e}")
            return []

    def query_optimized(self, query_text: str, n_results: int = 3, score_threshold: float = 0.35):
        """ğŸ†• BÃšSQUEDA OPTIMIZADA CON UMBRALES FLEXIBLES"""
        try:
            processed_query = self.enhanced_normalize_text(query_text)

            results = self.collection.query(
                query_texts=[processed_query],
                n_results=n_results * 4,
                include=['distances', 'documents', 'metadatas']
            )

            filtered_docs = []
            for i, distance in enumerate(results['distances'][0]):
                similarity = 1 - distance
                
                current_threshold = score_threshold
                if 'dÃ³nde' in query_text.lower() or 'ubicaciÃ³n' in query_text.lower():
                    current_threshold = 0.25
                
                if similarity >= current_threshold:
                    doc_content = results['documents'][0][i]
                    doc_metadata = results['metadatas'][0][i]
                    
                    if self._is_relevant_document_improved(processed_query, doc_content):
                        filtered_docs.append({
                            'document': doc_content,
                            'metadata': doc_metadata,
                            'similarity': similarity
                        })

            filtered_docs.sort(key=lambda x: x['similarity'], reverse=True)
            
            if not filtered_docs:
                logger.info(f"ğŸ” No se encontraron documentos para: {query_text}")
                return []
            
            return filtered_docs[:n_results]

        except Exception as e:
            logger.error(f"âŒ Error en query optimizada: {e}")
            # En caso de error, retornar resultados simples sin recursiÃ³n
            simple_results = self.query(query_text, n_results)
            return [{'document': doc, 'metadata': {}, 'similarity': 0.7} for doc in simple_results]

    def _is_relevant_document_improved(self, query: str, document: str) -> bool:
        """ğŸ†• VERIFICACIÃ“N DE RELEVANCIA MEJORADA"""
        query_words = set(query.lower().split())
        doc_words = set(document.lower().split())

        critical_keywords = {
            'tne', 'deporte', 'taller', 'gimnasio', 'certificado', 'beca', 
            'psicolÃ³gico', 'claudia', 'elizabeth', 'adriana', 'duoc', 'estudiantil',
            'prÃ¡ctica', 'empleo', 'curriculum', 'entrevista'
        }
        
        critical_matches = critical_keywords.intersection(query_words)
        if critical_matches:
            doc_has_critical = any(keyword in document.lower() for keyword in critical_matches)
            if doc_has_critical:
                return True

        stop_words = {'el', 'la', 'los', 'las', 'de', 'en', 'y', 'que', 'con', 'para', 'por'}
        query_words = query_words - stop_words
        doc_words = doc_words - stop_words

        if not query_words:
            return True

        overlap = len(query_words.intersection(doc_words))
        relevance_ratio = overlap / len(query_words)

        return relevance_ratio >= 0.15

    def query_with_sources(self, query_text: str, n_results: int = 3) -> List[Dict]:
        """BÃšSQUEDA CON FUENTES"""
        try:
            results = self.query_optimized(query_text, n_results, score_threshold=0.35)

            sources = []
            for result in results:
                sources.append({
                    'content': result['document'],
                    'category': result['metadata'].get('category', 'general'),
                    'source': result['metadata'].get('source', 'unknown'),
                    'similarity': result['similarity']
                })

            return sources

        except Exception as e:
            logger.error(f"âŒ Error en query con fuentes: {e}")
            return []

    def hybrid_search(self, query_text: str, n_results: int = 3) -> List[Dict]:
        """ğŸ†• BÃšSQUEDA HÃBRIDA MEJORADA"""
        try:
            results = self.query_optimized(query_text, n_results * 2, score_threshold=0.35)

            filtered_docs = []
            for result in results:
                if result['similarity'] >= 0.35:
                    filtered_docs.append(result)

            filtered_docs.sort(key=lambda x: x['similarity'], reverse=True)
            return filtered_docs[:n_results]

        except Exception as e:
            logger.error(f"âŒ Error en hybrid search: {e}")
            return []

    def get_cache_stats(self) -> Dict:
        """ESTADÃSTICAS MEJORADAS"""
        stats = {
            'text_cache_size': len(self.text_cache),
            'semantic_cache_size': len(self.semantic_cache.cache),
            'metrics': self.metrics,
            'semantic_cache_enabled': self.semantic_cache.model is not None,
            'total_documents': self.collection.count() if hasattr(self.collection, 'count') else 'N/A',
            'duoc_context': self.duoc_context,
            'processing_stats': {
                'total_derivations': self.metrics['derivations'],
                'total_multiple_queries': self.metrics['multiple_queries'],
                'total_ambiguous': self.metrics['ambiguous_queries'],
                'total_greetings': self.metrics['greetings'],
                'total_emergencies': self.metrics['emergencies'],
                'total_templates': self.metrics['template_responses']  # ğŸ†•
            }
        }

        if self.metrics['response_times']:
            avg_time = sum(self.metrics['response_times']) / len(self.metrics['response_times'])
            stats['average_response_time'] = round(avg_time, 3)
        else:
            stats['average_response_time'] = 0

        return stats


# âœ… Instancia global del motor RAG
rag_engine = RAGEngine()


def get_ai_response(user_message: str, context: list = None) -> Dict:
    """ğŸ¯ VERSIÃ“N MEJORADA - PROCESAMIENTO INTELIGENTE CON TEMPLATES Y QR CORREGIDO"""
    import time
    start_time = time.time()

    processing_info = rag_engine.process_user_query(user_message)
    strategy = processing_info['processing_strategy']

    # ğŸ†• ESTRATEGIAS PRIORITARIAS - TEMPLATES PRIMERO
    if strategy == 'template':
        response_data = rag_engine.generate_template_response(processing_info)
        response_data['response_time'] = time.time() - start_time
        return response_data

    if strategy == 'greeting' or processing_info.get('is_greeting', False):
        response_data = rag_engine.generate_greeting_response(processing_info)
        response_data['response_time'] = time.time() - start_time
        return response_data

    if strategy == 'emergency' or processing_info.get('is_emergency', False):
        response_data = rag_engine.generate_emergency_response(processing_info)
        response_data['response_time'] = time.time() - start_time
        return response_data

    # ğŸ†• ESTRATEGIAS DIFERENCIADAS
    if strategy == 'derivation':
        response_data = rag_engine.generate_derivation_response(processing_info)
        response_data['response_time'] = time.time() - start_time
        return response_data

    elif strategy == 'multiple_queries':
        response_data = rag_engine.generate_multiple_queries_response(processing_info)
        response_data['response_time'] = time.time() - start_time
        return response_data

    elif strategy == 'clarification':
        response_data = rag_engine.generate_clarification_response(processing_info)
        response_data['response_time'] = time.time() - start_time
        return response_data

    # ğŸ†• ESTRATEGIA ESTÃNDAR RAG MEJORADA
    normalized_message = rag_engine.enhanced_normalize_text(user_message)
    cache_key = f"rag_{hashlib.md5(user_message.encode()).hexdigest()}"

    if cache_key in rag_engine.text_cache:
        cached_response = rag_engine.text_cache[cache_key]
        rag_engine.metrics['text_cache_hits'] += 1
        logger.info(f"ğŸ¯ RAG Text Cache HIT para: '{user_message}'")
        cached_response['response_time'] = time.time() - start_time
        return cached_response

    logger.info(f"ğŸ” RAG Cache MISS para: '{user_message}'")

    try:
        sources = rag_engine.hybrid_search(user_message, n_results=3)
        
        final_sources = []
        seen_hashes = set()
        
        for source in sources:
            content_hash = hashlib.md5(source['document'].encode()).hexdigest()
            
            if content_hash in seen_hashes:
                continue
            seen_hashes.add(content_hash)
            
            if len(final_sources) < 2:
                final_sources.append(source)

        system_message = (
            "Eres InA, asistente del Punto Estudiantil Duoc UC Plaza Norte. "
            "Responde SOLO con la informaciÃ³n proporcionada.\n\n"
        )

        if final_sources:
            system_message += "ğŸ“š INFORMACIÃ“N DISPONIBLE:\n\n"
            for i, source in enumerate(final_sources):
                content = source['document']
                category = source['metadata'].get('category', 'general')
                short_content = content[:200] + "..." if len(content) > 200 else content
                system_message += f"--- Fuente {i+1} ({category}) ---\n{short_content}\n\n"
            
            system_message += (
                "ğŸ’¡ Responde ÃšNICAMENTE con la informaciÃ³n de arriba.\n"
                "ğŸ“ SÃ© especÃ­fico y breve (mÃ¡ximo 3 lÃ­neas).\n"
                "âŒ NO inventes informaciÃ³n.\n"
                "âœ… Si la informaciÃ³n no es suficiente, di 'Consulta en Punto Estudiantil'."
            )
        else:
            system_message += "âš ï¸ No hay informaciÃ³n especÃ­fica disponible.\n"

        response = ollama.chat(
            model='mistral:7b',
            messages=[
                {'role': 'system', 'content': system_message},
                {'role': 'user', 'content': user_message}
            ],
            options={'temperature': 0.1, 'num_predict': 100}
        )

        respuesta = response['message']['content'].strip()
        respuesta = _optimize_response(respuesta, user_message)

        formatted_sources = []
        for source in final_sources:
            formatted_sources.append({
                'content': source['document'][:80] + '...',
                'category': source['metadata'].get('category', 'general'),
                'similarity': round(source.get('similarity', 0.5), 3)
            })

        # ğŸ”¥ AGREGAR GENERACIÃ“N DE QR CODES PARA RESPUESTAS RAG (ESTRUCTURA CORREGIDA)
        qr_processed_response = qr_generator.process_response(respuesta, user_message)

        response_data = {
            'response': respuesta,
            'sources': formatted_sources,
            'category': processing_info['topic_classification'].get('category', 'general'),
            'timestamp': time.time(),
            'response_time': time.time() - start_time,
            'cache_type': 'ollama_generated',
            'processing_info': processing_info,
            'qr_codes': qr_processed_response['qr_codes'],  # ğŸ‘ˆ Dict simple {url: qr_image}
            'has_qr': qr_processed_response['has_qr']       # ğŸ‘ˆ Boolean
        }

        rag_engine.text_cache[cache_key] = response_data
        rag_engine.metrics['successful_responses'] += 1

        return response_data

    except Exception as e:
        logger.error(f"âŒ Error en RAG estÃ¡ndar: {str(e)}")
        return {
            "response": "ğŸ”§ Error tÃ©cnico. Intenta nuevamente.",
            "sources": [],
            "category": "error",
            "response_time": time.time() - start_time,
            "processing_info": processing_info
        }


def _optimize_response(respuesta: str, pregunta: str) -> str:
    """ğŸ†• OPTIMIZACIÃ“N DE RESPUESTA MEJORADA"""
    if respuesta.startswith(("Â¡Hola! Soy InA", "Hola, soy el asistente", "Hola, soy InA")):
        respuesta = re.sub(r'^Â¡?Hola!?\s*(soy|me llamo)\s*(InA|el asistente)[^.!?]*[.!?]\s*', '', respuesta)
    
    optimizations = {
        "soy el asistente virtual del Punto Estudiantil": "",
        "estoy aquÃ­ para ayudarte con": "Puedo informarte sobre",
        "te recomiendo que te dirijas": "recomiendo dirigirte",
        "debes saber que el proceso": "el proceso",
        "es importante mencionar que": "",
        "en relaciÃ³n a tu consulta sobre": "Sobre",
        "respecto a tu pregunta acerca de": "Acerca de",
        "quiero informarte que": "",
        "me complace decirte que": "",
        "como asistente virtual": "",
        "puedo proporcionarte informaciÃ³n": "InformaciÃ³n:",
        "hola, soy ina, el asistente virtual": "",
        "soy ina, el asistente virtual": "",
        "duoc uc": "Duoc UC",
    }

    for largo, corto in optimizations.items():
        respuesta = respuesta.replace(largo, corto)

    respuesta = re.sub(r'\s+', ' ', respuesta)
    respuesta = respuesta.strip()
    
    if len(respuesta) > 500:
        sentences = respuesta.split('.')
        if len(sentences) > 2:
            respuesta = '. '.join(sentences[:2]) + '.'
    
    return respuesta


class ResponseCache:
    def __init__(self):
        self.cache = {}
        self.ttl = timedelta(hours=1)

    def get_key(self, query: str) -> str:
        return hashlib.md5(query.encode()).hexdigest()

    def get(self, query: str):
        key = self.get_key(query)
        if key in self.cache:
            timestamp, response = self.cache[key]
            if datetime.now() - timestamp < self.ttl:
                return response
        return None

    def set(self, query: str, response: dict):
        key = self.get_key(query)
        self.cache[key] = (datetime.now(), response)


response_cache = ResponseCache()


def get_rag_cache_stats() -> Dict:
    """ESTADÃSTICAS COMPLETAS"""
    return rag_engine.get_cache_stats()


def clear_caches():
    """LIMPIAR CACHES"""
    rag_engine.text_cache.clear()
    rag_engine.semantic_cache.cache.clear()
    logger.info("ğŸ§¹ Todos los caches limpiados")