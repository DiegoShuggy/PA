# reprocess_documents.py - Script para reprocesar documentos con chunking inteligente
"""
Script para reprocesar todos los documentos existentes usando el nuevo sistema
de chunking semÃ¡ntico inteligente.

IMPORTANTE: Ejecutar este script eliminarÃ¡ los chunks antiguos y los reemplazarÃ¡
con chunks semÃ¡nticos optimizados con metadatos enriquecidos.
"""

import sys
import os
import logging
from pathlib import Path

# Agregar el directorio raÃ­z al path (2 niveles arriba desde scripts/utilities/)
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Importar configuraciones
from app import chroma_config  # Desactivar telemetrÃ­a ANTES de importar chromadb

# Ahora importar el resto
from app.rag import rag_engine
from app.training_data_loader import training_loader

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def clear_chromadb():
    """Limpiar ChromaDB antes de reprocesar"""
    try:
        # Obtener colecciÃ³n
        collection = rag_engine.client.get_collection("duoc_knowledge")
        count = collection.count()
        
        logger.warning(f"âš ï¸ Se eliminarÃ¡n {count} documentos existentes")
        response = input("Â¿Continuar? (yes/no): ")
        
        if response.lower() != 'yes':
            logger.info("OperaciÃ³n cancelada")
            return False
        
        # Eliminar colecciÃ³n
        rag_engine.client.delete_collection("duoc_knowledge")
        logger.info("âœ… ChromaDB limpiada")
        
        # Recrear colecciÃ³n vacÃ­a
        rag_engine.collection = rag_engine.client.get_or_create_collection(
            name="duoc_knowledge"
        )
        logger.info("âœ… ColecciÃ³n recreada")
        return True
        
    except Exception as e:
        logger.error(f"Error limpiando ChromaDB: {e}")
        return False


def reprocess_all_documents():
    """Reprocesar todos los documentos con chunking inteligente"""
    print("\n" + "="*80)
    print("ğŸ”„ INICIANDO REPROCESAMIENTO CON CHUNKING INTELIGENTE")
    print("="*80)
    logger.info("="*80)
    logger.info("ğŸ”„ INICIANDO REPROCESAMIENTO CON CHUNKING INTELIGENTE")
    logger.info("="*80)
    
    # Verificar que el chunker estÃ¡ disponible
    from app.intelligent_chunker import semantic_chunker
    print(f"âœ… Chunker inteligente disponible (chunk_size={semantic_chunker.chunk_size})")
    logger.info(f"âœ… Chunker inteligente disponible (chunk_size={semantic_chunker.chunk_size})")
    
    # Paso 1: Limpiar ChromaDB
    print("\nğŸ“‹ PASO 1: Limpiando ChromaDB...")
    logger.info("\nğŸ“‹ PASO 1: Limpiando ChromaDB...")
    if not clear_chromadb():
        print("âŒ FALLÃ“ la limpieza de ChromaDB")
        return False
    print("âœ… ChromaDB limpiado correctamente")
    
    # Paso 2: Reprocesar documentos
    print("\nğŸ“‹ PASO 2: Reprocesando documentos...")
    logger.info("\nğŸ“‹ PASO 2: Reprocesando documentos...")
    
    # Forzar recarga
    training_loader.data_loaded = False
    training_loader.base_knowledge_loaded = False
    training_loader.word_documents_loaded = False
    
    # Cargar con nuevo chunking
    print("â³ Cargando documentos con chunking inteligente...")
    print("   (Esto puede tomar 1-2 minutos)")
    success = training_loader.load_all_training_data()
    
    if success:
        # Verificar resultados
        collection = rag_engine.collection
        new_count = collection.count()
        
        print("\n" + "="*80)
        print("âœ… REPROCESAMIENTO COMPLETADO")
        print(f"ğŸ“Š Chunks en ChromaDB: {new_count}")
        print("="*80)
        logger.info("\n" + "="*80)
        logger.info("âœ… REPROCESAMIENTO COMPLETADO")
        logger.info(f"ğŸ“Š Documentos en ChromaDB: {new_count}")
        logger.info("="*80)
        
        # Mostrar estadÃ­sticas del chunker
        stats = semantic_chunker.get_stats()
        print(f"\nğŸ“ˆ ESTADÃSTICAS DEL CHUNKER:")
        print(f"  - TamaÃ±o de chunk: {stats['chunk_size']} tokens")
        print(f"  - Overlap: {stats['overlap']} tokens")
        print(f"  - Chunk mÃ­nimo: {stats['min_chunk_size']} tokens")
        print(f"  - Keywords institucionales: {stats['institutional_keywords_count']}")
        logger.info(f"\nğŸ“ˆ ESTADÃSTICAS DEL CHUNKER:")
        logger.info(f"  - TamaÃ±o de chunk: {stats['chunk_size']} tokens")
        logger.info(f"  - Overlap: {stats['overlap']} tokens")
        logger.info(f"  - Chunk mÃ­nimo: {stats['min_chunk_size']} tokens")
        logger.info(f"  - Keywords institucionales: {stats['institutional_keywords_count']}")
        
        return True
    else:
        print("âŒ Error reprocesando documentos")
        logger.error("âŒ Error reprocesando documentos")
        return False


def test_new_chunks():
    """Probar que los nuevos chunks tienen metadatos enriquecidos"""
    print("\nğŸ“‹ PASO 3: Verificando calidad de chunks...")
    logger.info("\nğŸ“‹ PASO 3: Verificando calidad de chunks...")
    
    try:
        # Buscar un chunk de prueba
        results = rag_engine.collection.query(
            query_texts=["tne tarjeta nacional estudiantil"],
            n_results=5
        )
        
        if not results['documents'][0]:
            print("âš ï¸ No se encontraron documentos sobre TNE")
            logger.warning("âš ï¸ No se encontraron documentos sobre TNE")
            return
        
        print(f"\nâœ… ENCONTRADOS {len(results['documents'][0])} CHUNKS")
        print("\nğŸ“‹ EJEMPLO DE METADATOS ENRIQUECIDOS:")
        logger.info("\nâœ… EJEMPLO DE CHUNK CON METADATOS ENRIQUECIDOS:")
        
        for i, (doc, metadata) in enumerate(zip(results['documents'][0][:3], results['metadatas'][0][:3])):
            print(f"\n--- CHUNK {i+1} ---")
            print(f"ğŸ“„ Fuente: {metadata.get('source', 'N/A')}")
            print(f"ğŸ“‚ CategorÃ­a: {metadata.get('category', 'N/A')}")
            print(f"ğŸ“Œ SecciÃ³n: {metadata.get('section', 'N/A')[:50]}...")
            print(f"ğŸ·ï¸  Keywords: {metadata.get('keywords', 'N/A')}")
            print(f"ğŸ”¢ Tokens: {metadata.get('token_count', 'N/A')}")
            print(f"ğŸ†” Chunk ID: {metadata.get('chunk_id', 'N/A')[:16]}...")
            print(f"ğŸ’¬ Preview: {doc[:150]}...")
            
            logger.info(f"\n--- CHUNK {i+1} ---")
            logger.info(f"ğŸ“„ Fuente: {metadata.get('source', 'N/A')}")
            logger.info(f"ğŸ“‚ CategorÃ­a: {metadata.get('category', 'N/A')}")
            logger.info(f"ğŸ“Œ SecciÃ³n: {metadata.get('section', 'N/A')}")
            logger.info(f"ğŸ·ï¸  Keywords: {metadata.get('keywords', 'N/A')}")
            logger.info(f"ğŸ”¢ Tokens: {metadata.get('token_count', 'N/A')}")
            logger.info(f"ğŸ†” Chunk ID: {metadata.get('chunk_id', 'N/A')}")
            logger.info(f"ğŸ“ TÃ­tulo: {metadata.get('title', 'N/A')}")
            logger.info(f"ğŸ“… Fecha: {metadata.get('fecha_procesamiento', 'N/A')}")
            logger.info(f"ğŸ”— Overlap: {metadata.get('has_overlap', 'N/A')}")
            logger.info(f"ğŸ’¬ Preview: {doc[:200]}...")
        
        # Verificar que tiene metadata enriquecida
        has_section = any(m.get('section') for m in results['metadatas'][0])
        has_keywords = any(m.get('keywords') for m in results['metadatas'][0])
        has_tokens = any(m.get('token_count') for m in results['metadatas'][0])
        
        print(f"\nâœ… VERIFICACIÃ“N:")
        print(f"   Secciones: {'âœ“' if has_section else 'âœ—'}")
        print(f"   Keywords: {'âœ“' if has_keywords else 'âœ—'}")
        print(f"   Token count: {'âœ“' if has_tokens else 'âœ—'}")
        
        if has_section and has_keywords and has_tokens:
            print(f"\nğŸ‰ Metadatos enriquecidos verificados correctamente")
            logger.info("\nâœ… Metadatos enriquecidos verificados correctamente")
        else:
            print(f"\nâš ï¸ Algunos metadatos faltan - verificar chunker")
            logger.warning("\nâš ï¸ Algunos metadatos faltan")
        
    except Exception as e:
        print(f"âŒ Error verificando chunks: {e}")
        logger.error(f"âŒ Error verificando chunks: {e}")


if __name__ == "__main__":
    print("="*80)
    print("ğŸš€ SCRIPT DE REPROCESAMIENTO DE DOCUMENTOS")
    print("="*80)
    print("\nEste script:")
    print("1. EliminarÃ¡ todos los documentos existentes en ChromaDB")
    print("2. ReprocesarÃ¡ documentos con CHUNKING SEMÃNTICO INTELIGENTE")
    print("3. AgregarÃ¡ METADATOS ENRIQUECIDOS a cada chunk")
    print("4. MejorarÃ¡ la precisiÃ³n del RAG significativamente")
    print("\nâš ï¸  ADVERTENCIA: Esta operaciÃ³n es irreversible")
    print("="*80)
    
    logger.info("="*80)
    logger.info("ğŸš€ SCRIPT DE REPROCESAMIENTO DE DOCUMENTOS")
    logger.info("="*80)
    
    proceed = input("\nÂ¿Deseas continuar? (yes/no): ")
    
    if proceed.lower() == 'yes':
        print("\nğŸš€ INICIANDO PROCESO...\n")
        success = reprocess_all_documents()
        if success:
            test_new_chunks()
            print("\n" + "="*80)
            print("ğŸ‰ Â¡REPROCESAMIENTO EXITOSO!")
            print("="*80)
            print("\nğŸ“‹ PRÃ“XIMOS PASOS:")
            print("   1. Reinicia el servidor:")
            print("      uvicorn app.main:app --reload --port 8000")
            print("   2. Prueba consultas:")
            print("      - 'tne' â†’ Debe dar pasos especÃ­ficos")
            print("      - 'beneficios' â†’ Debe listar 4-5 beneficios")
            print("      - 'marte' â†’ Debe rechazar correctamente")
            print("="*80)
            logger.info("\nğŸ‰ Â¡Reprocesamiento exitoso! El sistema RAG estÃ¡ optimizado.")
            logger.info("ğŸ’¡ Reinicia el servidor para usar los nuevos chunks.")
        else:
            print("\n" + "="*80)
            print("âŒ REPROCESAMIENTO FALLÃ“")
            print("="*80)
            print("Revisa los mensajes de error arriba")
            logger.error("\nâŒ Reprocesamiento fallÃ³. Verifica los logs.")
            sys.exit(1)
    else:
        print("âŒ OperaciÃ³n cancelada")
        logger.info("OperaciÃ³n cancelada")
        sys.exit(0)
