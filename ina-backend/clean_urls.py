import re
import requests
from urllib.parse import urlparse
import time

def clean_urls_file():
    """
    Limpia el archivo urls.txt eliminando comentarios y verificando URLs
    """
    input_file = "urls.txt"
    output_file = "urls_clean.txt"
    
    valid_urls = []
    invalid_count = 0
    
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except FileNotFoundError:
        print(f"‚ùå No se encontr√≥ el archivo {input_file}")
        return
    
    print(f"üìã Procesando {len(lines)} l√≠neas del archivo {input_file}...")
    
    for line_num, line in enumerate(lines, 1):
        line = line.strip()
        
        # Saltar l√≠neas vac√≠as y comentarios
        if not line or line.startswith('#') or line.startswith('//'):
            continue
            
        # Verificar que la l√≠nea sea una URL v√°lida
        if not (line.startswith('http://') or line.startswith('https://')):
            print(f"‚ö†Ô∏è  L√≠nea {line_num}: No es una URL v√°lida - {line}")
            invalid_count += 1
            continue
            
        # Verificar formato de URL
        try:
            parsed = urlparse(line)
            if not parsed.netloc:
                print(f"‚ö†Ô∏è  L√≠nea {line_num}: URL mal formada - {line}")
                invalid_count += 1
                continue
        except Exception as e:
            print(f"‚ö†Ô∏è  L√≠nea {line_num}: Error al parsear URL - {line}")
            invalid_count += 1
            continue
        
        valid_urls.append(line)
        
    print(f"\nüìä RESUMEN:")
    print(f"‚úÖ URLs v√°lidas encontradas: {len(valid_urls)}")
    print(f"‚ùå L√≠neas inv√°lidas eliminadas: {invalid_count}")
    
    # Guardar URLs limpias
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# URLs DUOC UC - VERIFICADAS\n")
        f.write("# Archivo limpio generado autom√°ticamente\n\n")
        for url in valid_urls:
            f.write(f"{url}\n")
    
    print(f"‚úÖ Archivo limpio guardado como: {output_file}")
    return output_file

def verify_urls(filename="urls_clean.txt", max_check=20):
    """
    Verifica la accesibilidad de las primeras URLs
    """
    print(f"\nüîç Verificando accesibilidad de URLs...")
    
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except FileNotFoundError:
        print(f"‚ùå No se encontr√≥ el archivo {filename}")
        return
    
    urls = [line.strip() for line in lines if line.strip() and not line.startswith('#')]
    check_count = min(len(urls), max_check)
    
    print(f"üìã Verificando las primeras {check_count} URLs...\n")
    
    accessible = 0
    for i, url in enumerate(urls[:check_count]):
        try:
            response = requests.head(url, timeout=10, allow_redirects=True)
            if response.status_code == 200:
                print(f"‚úÖ {url} - OK")
                accessible += 1
            else:
                print(f"‚ö†Ô∏è  {url} - {response.status_code}")
        except Exception as e:
            print(f"‚ùå {url} - Error: {str(e)[:50]}")
        
        # Peque√±a pausa para no sobrecargar
        time.sleep(0.5)
    
    print(f"\nüìä VERIFICACI√ìN COMPLETADA:")
    print(f"‚úÖ URLs accesibles: {accessible}/{check_count}")
    print(f"‚ö†Ô∏è  URLs con problemas: {check_count - accessible}/{check_count}")

if __name__ == "__main__":
    print("üßπ LIMPIADOR DE URLs DUOC UC")
    print("=" * 50)
    
    # Limpiar archivo
    clean_file = clean_urls_file()
    
    if clean_file:
        # Verificar algunas URLs
        verify_urls(clean_file)
        
        print(f"\nüéØ SIGUIENTE PASO:")
        print(f"   1. Revisar el archivo {clean_file}")
        print(f"   2. Reemplazar urls.txt con el archivo limpio")
        print(f"   3. Reiniciar el sistema para aplicar cambios")